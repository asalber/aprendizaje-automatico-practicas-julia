---
title: Árboles de decisión
lang: es
---

Los árboles de decisión son modelos de aprendizaje simples e intuitivos que pueden utilizarse para tanto para predecir variables cuantitativas (regresión) como categóricas (clasificación). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje basados en árboles de decisión con Julia.

## Ejercicios Resueltos

Para la realización de esta práctica se requieren los siguientes paquetes:

```julia
using CSV  # Para la lectura de archivos CSV.
using DataFrames  # Para el manejo de datos tabulares.
using Tidier # Para el preprocesamiento de datos.
using PrettyTables  # Para mostrar tablas formateadas.
using Plots  # Para el dibujo de gráficas.
using GLMakie  # Para obtener gráficos interactivos.
using AlgebraOfGraphics # Para generar gráficos mediante la gramática de gráficos.
using DecisionTree # Para construir árboles de decisión.
using GraphMakie # Para la visualización de árboles de decisión.
```

:::{#exr-arboles-decision-1}
El conjunto de datos [`tenis.csv`](/datos/tenis.csv) contiene información sobre las condiciones meteorológicas de varios días y si se pudo jugar al tenis o no.

a.  Cargar los datos del archivo `tenis.csv` en un data frame.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using CSV, DataFrames
    df = CSV.read("datos/tenis.csv", DataFrame)
    ```
    :::

a.  Crear un diagrama de barras que muestre la distribución de frecuencias de cada variable meteorológica según si se pudo jugar al tenis o no. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using GLMakie, AlgebraOfGraphics

    function frecuencias(df::DataFrame, var::Symbol)
        # Calculamos el número de días de cada clase que se juega al tenis.
        frec = combine(groupby(df, [var, :Tenis]), nrow => :Días)
        # Dibujamos el diagrama de barras.
        plt = data(frec) * 
        mapping(var, :Días, stack = :Tenis, color = :Tenis, ) * 
        visual(BarPlot) 
        # Devolvemos el gráfico.
        return plt
    end

    fig = Figure()
    draw!(fig[1, 1], frecuencias(df, :Cielo))
    draw!(fig[1, 2], frecuencias(df, :Temperatura))
    draw!(fig[1, 3], frecuencias(df, :Humedad))
    draw!(fig[1, 4], frecuencias(df, :Viento))
    fig
    ```

    A la vista de las frecuencias de cada variable, las variable `Cielo` y `Humedad` parecen ser las que más influye en la decisión de jugar al tenis.
    :::

a.  Calcular la impureza del conjunto de datos utilizando el índice de Gini. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?

    :::{.callout-note collapse="true"}
    ## Ayuda
    El [índice de Gini](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) se calcula mediante la fórmula
    
    $$ GI = 1 - \sum_{i=1}^{n} p_i^2 $$

    donde $p_i$ es la proporción de cada clase en el conjunto de datos y $n$ es el número de clases.
    
    El índice de Gini toma valores entre $0$ y $1-\frac{1}{n}$ ($0.5$ en el caso de clasificación binaria), donde $0$ indica que todas las instancias pertenecen a una sola clase (mínima impureza) y $1-\frac{1}{n}$ indica que las instancias están distribuidas uniformemente entre todas las clases (máxima impureza).
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    function gini(df::DataFrame, var::Symbol)
        # Calculamos el número de ejemplos.
        n = nrow(df)
        # Calculamos las frecuencias absolutas de cada clase.
        frec = combine(groupby(df, var), nrow => :ni)
        # Calculamos la proporción de cada clase.
        frec.p = frec.ni ./ n
        # Calculamos el índice de Gini.
        gini = 1 - sum(frec.p .^ 2)
        return gini
    end

    g0 = gini(df, :Tenis)
    ```
    :::

a.  ¿Qué reducción del índice Gini se obtiene si dividimos el conjunto de ejemplos según la variable `Humedad`? ¿Y si dividimos el conjunto con respecto a la variable `Viento`?

    :::{.callout-note collapse="true"}
    ## Ayuda
    La reducción del índice de Gini se calcula como la diferencia entre el índice de Gini del conjunto original y el índice de Gini del conjunto dividido.
    
    $$ \Delta GI = GI_{original} - GI_{dividido} $$

    donde el índice de Gini del conjunto dividido es la media ponderada de los índices de Gini de los subconjuntos resultantes de la división.
    :::
    
    :::{.callout-tip collapse="true"}
    ## Solución

    Calculamos primero la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Humedad`.

    ```{julia}
    using Tidier
    # Dividimos el conjunto de ejemplos según la variable Humedad.
    df_humedad_alta = @filter(df, Humedad == "Alta")
    df_humedad_normal = @filter(df, Humedad == "Normal")
    # Calculamos los tamaños de los subconjuntos de ejemplos.
    n = nrow(df_humedad_alta), nrow(df_humedad_normal)
    # Calculamos el índice de Gini de cada subconjunto.
    gis = gini(df_humedad_alta, :Tenis), gini(df_humedad_normal, :Tenis)
    # Calculamos media ponderada de los índices de Gini de los subconjuntos 
    g_humedad = sum(gis .* n) / sum(n)
    # Calculamos la reducción del índice de Gini.
    g0 - g_humedad
    ```

    Calculamos ahora la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Viento`.
    
    ```{julia}
    # Dividimos el conjunto de ejemplos según la variable `Viento`
    df_viento_fuerte = @filter(df, Viento == "Fuerte")
    df_viento_suave = @filter(df, Viento == "Suave")
    # Calculamos los tamaños de los subconjuntos de ejemplos
    n = nrow(df_viento_fuerte), nrow(df_viento_suave)
    # Calculamos el índice de Gini de cada subconjunto
    gis = gini(df_viento_fuerte, :Tenis), gini(df_viento_suave, :Tenis)
    # Calculamos media ponderada de los índices de Gini de los subconjuntos
    g_viento = sum(gis .* n) / sum(n)
    # Calculamos la reducción del índice de Gini
    g0 - g_viento
    ```

    Como se puede observar, la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Humedad` es mayor que la reducción del índice de Gini al dividir el conjunto con respecto a la variable `Viento`. Por lo tanto, la variable `Humedad` parece tener más influencia en la decisión de jugar al tenis y sería la variable que se debería elegir para dividir el conjunto de ejemplos.
    :::

a.  Construir un árbol de decisión que explique si se puede jugar al tenis en función de las variables meteorológicas.
    
    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función `DecisionTreeClassifier` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/). 

    Los parámetros más importantes de esta función son:

    - `max_depth`: Profundidad máxima del árbol. Si no se indica, el árbol crecerá hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de `min_samples_split` ejemplos.
    - `min_samples_leaf`: Número mínimo de ejemplos en una hoja (1 por defecto).
    - `min_samples_split`: Número mínimo de ejemplos para dividir un nodo (2 por defecto).
    - `min_impurity_decrease`: Reducción mínima de la impureza para dividir un nodo (0 por defecto).
    - `post-prune`: Si se indica `true`, se poda el árbol después de que se ha construido. La poda reduce el tamaño del árbol eliminando nodos que no aportan información útil.
    - `merge_purity_threshold`: Umbral de pureza para fusionar nodos. Si se indica, se fusionan los nodos que tienen una pureza menor que este umbral.
    - `feature_importance`: Indica la medida para calcular la importancia de las variables a la hora de dividir el conjunto de datos. Puede ser `:impurity` o `:split`. Si no se indica, se utiliza la impureza de Gini.
    - `rng`: Indica la semilla para la generación de números aleatorios. Si no se indica, se utiliza el generador de números aleatorios por defecto.
    :::
    
    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using DecisionTree, CategoricalArrays
    # Variables predictoras.
    X = Matrix(select(df, Not(:Tenis)))
    # Variable objetivo.
    y = df.Tenis
    # Convertir las variables categóricas a enteros.
    X = hcat([levelcode.(categorical(X[:, j])) for j in 1:size(X, 2)]...)
    # Convertir la variable objetivo a enteros.
    y = levelcode.(categorical(y))
    tree = DecisionTreeClassifier(max_depth=3)
    fit!(tree, X, y)
    ```
    :::

a.  Visualizar el árbol de decisión construido.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función `plot_tree` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/).
    :::
    
    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    print_tree(tree, feature_names=names(df)[1:end-1])
    ```
    :::
:::

:::{#exr-arboles-decision-2}
El conjunto de datos [pingüinos.csv]() contiene un conjunto de datos sobre tres eEspecie de pingüinos con las siguientes variables:

- Especie: Especie de pingüino, comúnmente Adelie, Chinstrap o Gentoo.
- Isla: Isla del archipiélago Palmer donde se realizó la observación.
- Longitud_pico: Longitud del pico en mm.
- Profundidad_pico: Profundidad del pico en mm
- Longitud_ala: Longitud de la aleta en mm.
- Peso: Masa corporal en gramos.
- Sexo: Sexo

a.  Cargar los datos del archivo `pinguïnos.csv` en un data frame.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using CSV, DataFrames
    df = CSV.read("datos/pingüinos.csv", DataFrame, missingstring="NA")
    ```
    :::

a.  Hacer un análisis de los datos perdidos en el data frame. 

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    describe(df, :nmissing)
    ```
    :::

a.  Eliminar del data frame los casos con valores perdidos.

    :::{.callout-tip collapse="true"}
    ## Solución
    ```{julia}
    dropmissing!(df)
    ```
    :::

a.  Crear diagramas que muestren la distribución de frecuencias de cada variable según la especie de pingüino. ¿Qué variable parece tener más influencia en la especie de pingüino?

    :::{.callout-tip collapse="true"}
    ## Solución

    Para las variables cualitativas dibujamos diagramas de barras.

    ```{julia}
    using GLMakie, AlgebraOfGraphics

    frec_isla = combine(groupby(df, [:Isla, :Especie]), nrow => :Frecuencia)
    data(frec_isla) * 
        mapping(:Isla, :Frecuencia, stack = :Especie, color =:Especie) *
        visual(BarPlot) |> draw
    ```
    
    ```{julia}
    frec_sexo = combine(groupby(df, [:Sexo, :Especie]), nrow => :Frecuencia)
    data(frec_sexo) * 
        mapping(:Sexo, :Frecuencia, stack = :Especie, color =:Especie) *
        visual(BarPlot) |> draw
    ```

    Para las variables cuantitativas dibujamos diagramas de cajas.

    ```{julia}
    function cajas(df, var, clase)
        data(df) *
            mapping(clase, var, color = clase) *
            visual(BoxPlot) |> 
            draw
    end

    cajas(df, :Longitud_pico, :Especie)
    ```
    
    ```{julia}
    cajas(df, :Profundidad_pico, :Especie)
    ```
    
    ```{julia}
    cajas(df, :Longitud_ala, :Especie)
    ```
    
    ```{julia}
    cajas(df, :Peso, :Especie)
    ```
    :::

a.  ¿Cuál es la reducción de la impureza del conjunto de datos si dividimos el conjunto de datos en dos conjuntos según si la longitud del pico es mayor o menor que 44 mm?

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using Tidier
    function gini(df::DataFrame, var::Symbol)
        n = nrow(df)
        frec = combine(groupby(df, var), nrow => :ni)
        frec.p = frec.ni ./ n
        gini = 1 - sum(frec.p .^ 2)
        return gini
    end

    function reduccion_impureza(df::DataFrame, var::Symbol, val::Number)
        # Dividimos el conjunto de ejemplos según la longitud del pico es menor de 44.
        df_menor = @eval @filter($df, $var <= $val)
        df_mayor = @eval @filter($df, $var > $val)
        # Calculamos los tamaños de los subconjuntos de ejemplos.
        n = nrow(df_menor), nrow(df_mayor)
        # Calculamos el índice de Gini de cada subconjunto.
        gis = gini(df_menor, :Especie), gini(df_mayor, :Especie)
        # Calculamos media ponderada de los índices de Gini de los subconjuntos.
        g1 = sum(gis .* n) / sum(n)
        # Calculamos la reducción del índice de Gini.
        gini(df, :Especie) - g1
    end

    reduccion_impureza(df, :Longitud_pico, 44)
    ```
    :::

a.  Determinar el valor óptimo de división del conjunto de datos según la longitud del pico. Para ello, calcular la reducción de la impureza para cada valor de longitud del pico y dibujar el resultado.

    :::{.callout-tip collapse="true"}
    ## Solución

    Dibujamos la reducción de la impureza en función de la longitud del pico.

    ```{julia}
    using Plots
    # Valores únicos de longitud del pico.
    valores = unique(df.Longitud_pico)
    # Reducción de la impureza para cada valor.
    reducciones = [reduccion_impureza(df, :Longitud_pico, val) for val in valores]
    # Graficamos el resultado.
    Plots.scatter(valores, reducciones, xlabel = "Longitud del pico", ylabel = "Reducción de la impureza", legend = false)
    ```

    Y ahora obtenemos el valor óptimo de división del conjunto de datos según la longitud del pico.

    ```{julia}
    val_optimo = valores[argmax(reducciones)]
    ```
    ::: 

a.  Dividir aleatoriamente el dataframe en un conjunto de entrenamiento y un conjunto de test con proporciones $3/4$ y $1/4$ respectivamente.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Utilizar la función [`shuffle`](https://docs.julialang.org/en/v1/stdlib/Random/#Random.shuffle) del paquete [`Random`](https://docs.julialang.org/en/v1/stdlib/Random/) para barajar el dataframe y luego dividirlo en dos subconjuntos.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución    

    ```{julia}
    using Random
    # Establecemos la semilla para la reproducibilidad.
    Random.seed!(1234)
    # Barajamos el dataframe.
    df = shuffle(df)
    # Dividimos el dataframe en un conjunto de entrenamiento y un conjunto de test.
    n = nrow(df)
    df_test = df[1:div(n, 4), :]
    df_train = df[div(n, 4)+1:end, :]
    ```
    :::

a.  Construir un árbol de decisión con el conjunto de entrenamiento sin tener en cuenta la variable `Isla` y visualizarlo.
    
    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using DecisionTree, CategoricalArrays
    # Variables predictivas.
    X_train = Matrix(select(df_train, Not(:Isla, :Especie)))
    # Variable objetivo.
    y_train = df_train.Especie
    # Convertir las variables categóricas a enteros.
    X_train = hcat([levelcode.(categorical(X_train[:, j])) for j in 1:size(X_train, 2)]...)
    # Convertir la variable objetivo a enteros
    y_train = levelcode.(categorical(y_train))

    # Construimos el árbol de decisión con profundidad máxima 3.
    tree = DecisionTreeClassifier(max_depth = 3)
    fit!(tree, X_train, y_train)
    print_tree(tree, feature_names=names(df)[3:end])
    ```
    :::

a.  Predecir la especie de los pingüinos del conjunto de test y calcular la matriz de confusión de las predicciones.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Utilizar la función [`confmat`](https://juliaai.github.io/StatisticalMeasures.jl/stable/confusion_matrices/#StatisticalMeasures.ConfusionMatrices.confmat) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para barajar el dataframe y luego dividirlo en dos subconjuntos.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using StatisticalMeasures
    # Variables predictivas
    X_test = Matrix(select(df_test, Not(:Isla, :Especie)))
    # Variable objetivo
    y_test = df_test.Especie
    # Convertir las variables categóricas a enteros
    X_test = hcat([levelcode.(categorical(X_test[:, j])) for j in 1:size(X_test, 2)]...)
    # Convertir la variable objetivo a enteros
    y_test = levelcode.(categorical(y_test))
    # Predecimos la especie de pingüino del conjunto de test
    y_pred = predict(tree, X_test)
    # Calculamos la precisión del modelo
    confmat(y_pred, y_test)
    ```
    :::

a.  Calcular la precisión del modelo.

    :::{.callout-note collapse="true"}
    ## Ayuda
    La precisión es la proporción de predicciones correctas sobre el total de predicciones.

    Utilizar la función [`accuracy`](https://juliaai.github.io/StatisticalMeasures.jl/stable/auto_generated_list_of_measures/#StatisticalMeasures.Accuracy) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para calcular la precisión del modelo.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    # Calculamos la precisión del modelo
    accuracy(y_pred, y_test)
    ```
    :::

:::

:::{#exr-arboles-decision-3}
El fichero [`vinos.csv`](datos/vinos.csv) contiene información sobre las características de una muestra de vinos portugueses de la denominación "Vinho Verde". Las variables que contiene son:

| Variable             | Descripción                                                           | Tipo (unidades)        |
|----------------------------------------|-----------------------------------------------------------------------|------------------------|
| tipo                 | Tipo de vino                                                          | Categórica (blanco, tinto) |
| meses.barrica        | Mesesde envejecimiento en barrica                               | Numérica(meses)  |
| acided.fija          | Cantidadde ácidotartárico                                 | Numérica(g/dm3)  |
| acided.volatil       | Cantidad de ácido acético                                             | Numérica(g/dm3)  |
| acido.citrico        | Cantidad de ácidocítrico                                        | Numérica(g/dm3)  |
| azucar.residual      | Cantidad de azúcarremanente después de la fermentación          | Numérica(g/dm3)  |
| cloruro.sodico       | Cantidad de clorurosódico                                       | Numérica(g/dm3)  |
| dioxido.azufre.libre | Cantidad de dióxido de azufreen formalibre                | Numérica(mg/dm3) |
| dioxido.azufre.total | Cantidadde dióxido de azufretotal en forma libre o ligada | Numérica(mg/dm3) |
| densidad             | Densidad                                                              | Numérica(g/cm3)  |
| ph                   | pH                                                                    | Numérica(0-14)   |
| sulfatos             | Cantidadde sulfato de potasio                                   | Numérica(g/dm3)  |
| alcohol              | Porcentajede contenidode alcohol                          | Numérica(0-100)  |
| calidad              | Calificación otorgada porun panel de expertos                   | Numérica(0-10)   |

a.  Crear un data frame con los datos de los vinos a partir del fichero [`vinos.csv`](datos/vinos.csv).

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using CSV, DataFrames
    df = CSV.read("datos/vinos.csv", DataFrame)
    ```
    :::

a.  Mostrar los tipos de cada variable del data frame.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función `schema` del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/).
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using MLJ
    schema(df)
    ```
    :::

a.  Hacer un análisis de los datos perdidos en el data frame.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    describe(df, :nmissing)
    ```
    :::

a.  Mostrar la distribución de frecuencias de las variables cuantitativas del data frame mediante histogramas.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using GLMakie
    fig = Figure() 
    ax = [Axis(fig[trunc(Int, i / 3), i % 3], title = names(df)[i+2]) for i in 0:12]
    for i in 1:13
        hist!(ax[i], df[!, i+1], strokewidth = 0.5, strokecolor = (:white, 0.5))
    end
    fig
    ```
    :::

a.  Se considera que un vino es bueno si tiene una puntuación de calidad mayor que $6.5$. Recodificar la variable `calidad` en una variable categórica que tome el valor 1 si la calidad es mayor que $6.5$ y 0 en caso contrario.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    using CategoricalArrays
    # Recodificamos la variable calidad.
    df.calidad = cut(df.calidad, [0, 6.5, 10], labels = [" ☹️ ", " 😊 "])
    ```
    :::

a.  Descomponer el data frame en un data frame con las variables predictivas y un vector con la variable objetivo `bueno`.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    y, X = unpack(df, ==(:calidad), rng = 123)
    ```
    :::

a.  Para poder entrenar un modelo de un arbol de decisión, las variables predictivas deben ser cuantitativas. Transmformar las variables categóricas en variables numéricas.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    # Convertir las variables categóricas a enteros.
    coerce!(X, :tipo => OrderedFactor, :meses_barrica => Continuous)
    schema(X)
    ```
    :::

a.  Definir un modelo de árbol de decisión con profundidad máxima 3.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Cargar el modelo `DecisionTreeClassifier` del paquete [`DecisionTree`](https://docs.juliahub.com/DecisionTree/) con la macros `@iload`.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    # Cargamos el tipo de modelo.
    Tree = @iload DecisionTreeClassifier pkg = "DecisionTree"
    # Instanciamos el modelo con sus parámetros.
    arbol = Tree(max_depth =3, rng = 123)
    ```
    :::

a.  Evaluar el modelo tomando un 70% de ejemplos en el conjunto de entrenamiento y un 30% en el conjunto de test. Utilizar como métrica la precisión.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función [`evaluate`](https://juliaai.github.io/MLJ.jl/stable/evaluating_model_performance/#MLJBase.evaluate!) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para evaluar el modelo. Los parámetros más importantes de esta función son:

    - `resampling`: Indica el método de muestreo para definir los conjuntos de entrenamiento y test. Los métodos más habituales son:
        - `Holdout(fraction_train = p)`: Divide el conjunto de datos tomando una proporción de $p$ ejemplos en el conjunto de entrenamiento y $1-p$ en el conjunto de test.
        - `CV(nfolds = n, shuffle = true|false)`: Utiliza validación cruzada con `n` iteraciones. Si se indica `shuffle = true`, se utiliza validación cruzada aleatoria.
        - `StratifiedCV(nfolds = n, shuffle = true|false)`: Utiliza validación cruzada estratificada con `n` iteraciones. Si se indica `shuffle = true`, se utiliza validación cruzada estratificada aleatoria.
        - `InSample()`: Utiliza el conjunto de entrenamiento como conjunto de test.
  
    - `measures`: Indica las métricas a utilizar para evaluar el modelo. Las métricas más habituales son:
        - `cross_entropy`: Pérdida de entropía cruzada.
        - `confusion_matrix`: Matriz de confusión.
        - `true_positive_rate`: Tasa de verdaderos positivos.
        - `true_negative_rate`: Tasa de verdaderos negativos.
        - `ppv`: Valor predictivo positivo.
        - `npv`: Valor predictivo negativo.
        - `accuracy`: Precisión.
    
        Se puede indicar más de una en un vector.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    evaluate(arbol, X, y, resampling = Holdout(fraction_train = 0.7, rng = 123), measures = accuracy)
    ```
    :::

a.  Evaluar el modelo mediante validación cruzada estratificada usando las métricas de la pérdida de entropía cruzada, la matriz de confusión, la tasa de verdaderos positivos, la tasa de verdaderos negativos, el valor predictivo positivo, el valor predictivo negativo y la precisión. ¿Es un buen modelo?

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    evaluate(arbol, X, y, resampling = StratifiedCV(rng = 123), measures = [cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy])
    ```

    La precisión del modelo es de $0.834$ que no está mal, pero si consdieramos la tasa de verdadero positivos, que es $0.13$ y la tasa de verdaderos negativos, que es prácticamente 1, el modelo tiene un buen rendimiento en la clasificación de los vinos malos, pero un mal rendimiento en la clasificación de los vinos buenos. Por lo tanto, no podemos decir que sea un buen modelo.
    :::

a.  Construir árboles de decisión con profundidades máximas de 2 a 10 y evaluar el modelo con validación cruzada estratificada. ¿Cuál es la profundidad máxima que da mejor resultado?

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función [`TunedModel`](https://juliaai.github.io/MLJ.jl/stable/tuning_models/#MLJTuning.TunedModel) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para ajustar los parámetros del modelo.

    Los parámetros más importantes de esta función son:
    - `model`: Indica el modelo a ajustar.
    - `resampling`: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.
    - `tuning`: Indica el método de ajuste de los parámetros del modelo. Los métodos más habituales son:
        - `Grid(resolution = n)`: Ajusta los parámetros del modelo utilizando una cuadrícula de búsqueda con `n` valores.
        - `RandomSearch(resolution = n)`: Ajusta los parámetros del modelo utilizando una búsqueda aleatoria con `n` valores.
    - range: Indica el rango de valores a utilizar para ajustar los parámetros del modelo. Se puede indicar un rango de valores o un vector de valores.
    - `measure`: Indica la métrica a utilizar para evaluar el modelo.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    # Instanciamos el modelo de árbol de decisión.
    arbol = Tree()
    # Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.
    r = range(arbol, :max_depth, lower=2, upper=10)
    # Ajustamos los parámetros del modelo utilizando una cuadrícula de búsqueda con 9 valores.
    arbol_parametrizado = TunedModel(
        model = arbol,
        resampling = StratifiedCV(rng = 123),
        tuning = Grid(resolution = 9),
        range = r,
        measure = accuracy)
    # Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.
    mach = machine(arbol_parametrizado, X, y)
    # Ajustamos los parámetros del modelo.
    MLJ.fit!(mach)
    # Mostramos los parámetros del mejor modelo.
    fitted_params(mach).best_model
    ```
    :::

a.  Dibujar la curva de aprendizaje del modelo en función de la profundidad del árbol de decisión.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función [`learning_curve`](https://juliaai.github.io/MLJ.jl/stable/learning_curves/#MLJBase.learning_curve) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para dibujar la curva de aprendizaje.
    Los parámetros más importantes de esta función son:
    - `mach`: Indica la máquina de aprendizaje a utilizar.
    - `range`: Indica el rango de valores a utilizar para ajustar los parámetros del modelo.
    - `resampling`: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.
    - `measure`: Indica la métrica a utilizar para evaluar el modelo.
    - `rngs`: Indica la semilla para la generación de números aleatorios. Se pueden indicar varias semillas en un vector y se genera una curva de aprendizaje para cada semilla.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    # Instanciamos el modelo de árbol de decisión.
    arbol = Tree()
    # Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.
    mach = machine(arbol, X, y)
    # Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.
    r = range(arbol, :max_depth, lower=2, upper=10)
    # Dibujamos la curva de aprendizaje.
    curva = learning_curve(mach, range = r, resampling = StratifiedCV(rng = 123), measure = accuracy)
    # Dibujamos la curva de aprendizaje.
    fig = Figure()
    ax = Axis(fig[1, 1], title = "Curva de aprendizaje", xlabel = "Profundidad del árbol", ylabel = "Precisión")
    Makie.scatter!(ax, curva.parameter_values, curva.measurements)
    fig
    ```
    :::

a.  Construir un árbol de decisión con la profundidad máxima que da mejor resultado y visualizarlo.

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    # Instanciamos el modelo de árbol de decisión.
    arbol = Tree(max_depth = 4)
    # Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.
    mach = machine(arbol, X, y)
    # Ajustamos los parámetros del modelo.
    MLJ.fit!(mach)
    # Visualizamos el árbol de decisión.
    fitted_params(mach).tree
    ```
    :::

a.  ¿Cuál es la importancia de cada variable en el modelo?

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función `feature_importances` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para calcular la importancia de cada variable.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    ```{julia}
    # Calculamos la importancia de cada variable.
    feature_importances(mach)
    ```
    :::

a.  Predecir la calidad de los 10 primeros vinos del conjunto de ejemplos.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la función `predict` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para predecir las probabilidades de pertenecer a cada clase un ejemplo o conjunto de ejemplos.

    Usar la función `predict_mode` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para predecir la clase de un ejemplo o conjunto de ejemplos.
    :::

    :::{.callout-tip collapse="true"}
    ## Solución

    Primero calculamos las probabilidades de cada clase.

    ```{julia}
    MLJ.predict(mach, X[1:10, :])
    ```

    Y ahora predecimos la clase.

    ```{julia}
    predict_mode(mach, X[1:10, :])
    ```
    :::
:::