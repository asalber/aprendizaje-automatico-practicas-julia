---
title: 츼rboles de decisi칩n
lang: es
---

Los 치rboles de decisi칩n son modelos de aprendizaje simples e intuitivos que pueden utilizarse para tanto para predecir variables cuantitativas (regresi칩n) como categ칩ricas (clasificaci칩n). Esta pr치ctica contiene ejercicios que muestran como construir modelos de aprendizaje basados en 치rboles de decisi칩n con Julia.

## Ejercicios Resueltos

Para la realizaci칩n de esta pr치ctica se requieren los siguientes paquetes:

```julia
using CSV  # Para la lectura de archivos CSV.
using DataFrames  # Para el manejo de datos tabulares.
using Tidier # Para el preprocesamiento de datos.
using PrettyTables  # Para mostrar tablas formateadas.
using Plots  # Para el dibujo de gr치ficas.
using GLMakie  # Para obtener gr치ficos interactivos.
using AlgebraOfGraphics # Para generar gr치ficos mediante la gram치tica de gr치ficos.
using DecisionTree # Para construir 치rboles de decisi칩n.
using GraphMakie # Para la visualizaci칩n de 치rboles de decisi칩n.
```

:::{#exr-arboles-decision-1}
El conjunto de datos [`tenis.csv`](/datos/tenis.csv) contiene informaci칩n sobre las condiciones meteorol칩gicas de varios d칤as y si se pudo jugar al tenis o no.

a.  Cargar los datos del archivo `tenis.csv` en un data frame.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using CSV, DataFrames
    df = CSV.read("datos/tenis.csv", DataFrame)
    ```
    :::

a.  Crear un diagrama de barras que muestre la distribuci칩n de frecuencias de cada variable meteorol칩gica seg칰n si se pudo jugar al tenis o no. 쯈u칠 variable meteorol칩gica parece tener m치s influencia en la decisi칩n de jugar al tenis?

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using GLMakie, AlgebraOfGraphics

    function frecuencias(df::DataFrame, var::Symbol)
        # Calculamos el n칰mero de d칤as de cada clase que se juega al tenis.
        frec = combine(groupby(df, [var, :Tenis]), nrow => :D칤as)
        # Dibujamos el diagrama de barras.
        plt = data(frec) * 
        mapping(var, :D칤as, stack = :Tenis, color = :Tenis, ) * 
        visual(BarPlot) 
        # Devolvemos el gr치fico.
        return plt
    end

    fig = Figure()
    draw!(fig[1, 1], frecuencias(df, :Cielo))
    draw!(fig[1, 2], frecuencias(df, :Temperatura))
    draw!(fig[1, 3], frecuencias(df, :Humedad))
    draw!(fig[1, 4], frecuencias(df, :Viento))
    fig
    ```

    A la vista de las frecuencias de cada variable, las variable `Cielo` y `Humedad` parecen ser las que m치s influye en la decisi칩n de jugar al tenis.
    :::

a.  Calcular la impureza del conjunto de datos utilizando el 칤ndice de Gini. 쯈u칠 variable meteorol칩gica parece tener m치s influencia en la decisi칩n de jugar al tenis?

    :::{.callout-note collapse="true"}
    ## Ayuda
    El [칤ndice de Gini](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) se calcula mediante la f칩rmula
    
    $$ GI = 1 - \sum_{i=1}^{n} p_i^2 $$

    donde $p_i$ es la proporci칩n de cada clase en el conjunto de datos y $n$ es el n칰mero de clases.
    
    El 칤ndice de Gini toma valores entre $0$ y $1-\frac{1}{n}$ ($0.5$ en el caso de clasificaci칩n binaria), donde $0$ indica que todas las instancias pertenecen a una sola clase (m칤nima impureza) y $1-\frac{1}{n}$ indica que las instancias est치n distribuidas uniformemente entre todas las clases (m치xima impureza).
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    function gini(df::DataFrame, var::Symbol)
        # Calculamos el n칰mero de ejemplos.
        n = nrow(df)
        # Calculamos las frecuencias absolutas de cada clase.
        frec = combine(groupby(df, var), nrow => :ni)
        # Calculamos la proporci칩n de cada clase.
        frec.p = frec.ni ./ n
        # Calculamos el 칤ndice de Gini.
        gini = 1 - sum(frec.p .^ 2)
        return gini
    end

    g0 = gini(df, :Tenis)
    ```
    :::

a.  쯈u칠 reducci칩n del 칤ndice Gini se obtiene si dividimos el conjunto de ejemplos seg칰n la variable `Humedad`? 쯏 si dividimos el conjunto con respecto a la variable `Viento`?

    :::{.callout-note collapse="true"}
    ## Ayuda
    La reducci칩n del 칤ndice de Gini se calcula como la diferencia entre el 칤ndice de Gini del conjunto original y el 칤ndice de Gini del conjunto dividido.
    
    $$ \Delta GI = GI_{original} - GI_{dividido} $$

    donde el 칤ndice de Gini del conjunto dividido es la media ponderada de los 칤ndices de Gini de los subconjuntos resultantes de la divisi칩n.
    :::
    
    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    Calculamos primero la reducci칩n del 칤ndice de Gini al dividir el conjunto de ejemplos seg칰n la variable `Humedad`.

    ```{julia}
    using Tidier
    # Dividimos el conjunto de ejemplos seg칰n la variable Humedad.
    df_humedad_alta = @filter(df, Humedad == "Alta")
    df_humedad_normal = @filter(df, Humedad == "Normal")
    # Calculamos los tama침os de los subconjuntos de ejemplos.
    n = nrow(df_humedad_alta), nrow(df_humedad_normal)
    # Calculamos el 칤ndice de Gini de cada subconjunto.
    gis = gini(df_humedad_alta, :Tenis), gini(df_humedad_normal, :Tenis)
    # Calculamos media ponderada de los 칤ndices de Gini de los subconjuntos 
    g_humedad = sum(gis .* n) / sum(n)
    # Calculamos la reducci칩n del 칤ndice de Gini.
    g0 - g_humedad
    ```

    Calculamos ahora la reducci칩n del 칤ndice de Gini al dividir el conjunto de ejemplos seg칰n la variable `Viento`.
    
    ```{julia}
    # Dividimos el conjunto de ejemplos seg칰n la variable `Viento`
    df_viento_fuerte = @filter(df, Viento == "Fuerte")
    df_viento_suave = @filter(df, Viento == "Suave")
    # Calculamos los tama침os de los subconjuntos de ejemplos
    n = nrow(df_viento_fuerte), nrow(df_viento_suave)
    # Calculamos el 칤ndice de Gini de cada subconjunto
    gis = gini(df_viento_fuerte, :Tenis), gini(df_viento_suave, :Tenis)
    # Calculamos media ponderada de los 칤ndices de Gini de los subconjuntos
    g_viento = sum(gis .* n) / sum(n)
    # Calculamos la reducci칩n del 칤ndice de Gini
    g0 - g_viento
    ```

    Como se puede observar, la reducci칩n del 칤ndice de Gini al dividir el conjunto de ejemplos seg칰n la variable `Humedad` es mayor que la reducci칩n del 칤ndice de Gini al dividir el conjunto con respecto a la variable `Viento`. Por lo tanto, la variable `Humedad` parece tener m치s influencia en la decisi칩n de jugar al tenis y ser칤a la variable que se deber칤a elegir para dividir el conjunto de ejemplos.
    :::

a.  Construir un 치rbol de decisi칩n que explique si se puede jugar al tenis en funci칩n de las variables meteorol칩gicas.
    
    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n `DecisionTreeClassifier` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/). 

    Los par치metros m치s importantes de esta funci칩n son:

    - `max_depth`: Profundidad m치xima del 치rbol. Si no se indica, el 치rbol crecer치 hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de `min_samples_split` ejemplos.
    - `min_samples_leaf`: N칰mero m칤nimo de ejemplos en una hoja (1 por defecto).
    - `min_samples_split`: N칰mero m칤nimo de ejemplos para dividir un nodo (2 por defecto).
    - `min_impurity_decrease`: Reducci칩n m칤nima de la impureza para dividir un nodo (0 por defecto).
    - `post-prune`: Si se indica `true`, se poda el 치rbol despu칠s de que se ha construido. La poda reduce el tama침o del 치rbol eliminando nodos que no aportan informaci칩n 칰til.
    - `merge_purity_threshold`: Umbral de pureza para fusionar nodos. Si se indica, se fusionan los nodos que tienen una pureza menor que este umbral.
    - `feature_importance`: Indica la medida para calcular la importancia de las variables a la hora de dividir el conjunto de datos. Puede ser `:impurity` o `:split`. Si no se indica, se utiliza la impureza de Gini.
    - `rng`: Indica la semilla para la generaci칩n de n칰meros aleatorios. Si no se indica, se utiliza el generador de n칰meros aleatorios por defecto.
    :::
    
    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using DecisionTree, CategoricalArrays
    # Variables predictoras.
    X = Matrix(select(df, Not(:Tenis)))
    # Variable objetivo.
    y = df.Tenis
    # Convertir las variables categ칩ricas a enteros.
    X = hcat([levelcode.(categorical(X[:, j])) for j in 1:size(X, 2)]...)
    # Convertir la variable objetivo a enteros.
    y = levelcode.(categorical(y))
    tree = DecisionTreeClassifier(max_depth=3)
    fit!(tree, X, y)
    ```
    :::

a.  Visualizar el 치rbol de decisi칩n construido.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n `plot_tree` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/).
    :::
    
    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    print_tree(tree, feature_names=names(df)[1:end-1])
    ```
    :::
:::

:::{#exr-arboles-decision-2}
El conjunto de datos [ping칲inos.csv]() contiene un conjunto de datos sobre tres eEspecie de ping칲inos con las siguientes variables:

- Especie: Especie de ping칲ino, com칰nmente Adelie, Chinstrap o Gentoo.
- Isla: Isla del archipi칠lago Palmer donde se realiz칩 la observaci칩n.
- Longitud_pico: Longitud del pico en mm.
- Profundidad_pico: Profundidad del pico en mm
- Longitud_ala: Longitud de la aleta en mm.
- Peso: Masa corporal en gramos.
- Sexo: Sexo

a.  Cargar los datos del archivo `pingu칦nos.csv` en un data frame.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using CSV, DataFrames
    df = CSV.read("datos/ping칲inos.csv", DataFrame, missingstring="NA")
    ```
    :::

a.  Hacer un an치lisis de los datos perdidos en el data frame. 

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    describe(df, :nmissing)
    ```
    :::

a.  Eliminar del data frame los casos con valores perdidos.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n
    ```{julia}
    dropmissing!(df)
    ```
    :::

a.  Crear diagramas que muestren la distribuci칩n de frecuencias de cada variable seg칰n la especie de ping칲ino. 쯈u칠 variable parece tener m치s influencia en la especie de ping칲ino?

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    Para las variables cualitativas dibujamos diagramas de barras.

    ```{julia}
    using GLMakie, AlgebraOfGraphics

    frec_isla = combine(groupby(df, [:Isla, :Especie]), nrow => :Frecuencia)
    data(frec_isla) * 
        mapping(:Isla, :Frecuencia, stack = :Especie, color =:Especie) *
        visual(BarPlot) |> draw
    ```
    
    ```{julia}
    frec_sexo = combine(groupby(df, [:Sexo, :Especie]), nrow => :Frecuencia)
    data(frec_sexo) * 
        mapping(:Sexo, :Frecuencia, stack = :Especie, color =:Especie) *
        visual(BarPlot) |> draw
    ```

    Para las variables cuantitativas dibujamos diagramas de cajas.

    ```{julia}
    function cajas(df, var, clase)
        data(df) *
            mapping(clase, var, color = clase) *
            visual(BoxPlot) |> 
            draw
    end

    cajas(df, :Longitud_pico, :Especie)
    ```
    
    ```{julia}
    cajas(df, :Profundidad_pico, :Especie)
    ```
    
    ```{julia}
    cajas(df, :Longitud_ala, :Especie)
    ```
    
    ```{julia}
    cajas(df, :Peso, :Especie)
    ```
    :::

a.  쮺u치l es la reducci칩n de la impureza del conjunto de datos si dividimos el conjunto de datos en dos conjuntos seg칰n si la longitud del pico es mayor o menor que 44 mm?

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using Tidier
    function gini(df::DataFrame, var::Symbol)
        n = nrow(df)
        frec = combine(groupby(df, var), nrow => :ni)
        frec.p = frec.ni ./ n
        gini = 1 - sum(frec.p .^ 2)
        return gini
    end

    function reduccion_impureza(df::DataFrame, var::Symbol, val::Number)
        # Dividimos el conjunto de ejemplos seg칰n la longitud del pico es menor de 44.
        df_menor = @eval @filter($df, $var <= $val)
        df_mayor = @eval @filter($df, $var > $val)
        # Calculamos los tama침os de los subconjuntos de ejemplos.
        n = nrow(df_menor), nrow(df_mayor)
        # Calculamos el 칤ndice de Gini de cada subconjunto.
        gis = gini(df_menor, :Especie), gini(df_mayor, :Especie)
        # Calculamos media ponderada de los 칤ndices de Gini de los subconjuntos.
        g1 = sum(gis .* n) / sum(n)
        # Calculamos la reducci칩n del 칤ndice de Gini.
        gini(df, :Especie) - g1
    end

    reduccion_impureza(df, :Longitud_pico, 44)
    ```
    :::

a.  Determinar el valor 칩ptimo de divisi칩n del conjunto de datos seg칰n la longitud del pico. Para ello, calcular la reducci칩n de la impureza para cada valor de longitud del pico y dibujar el resultado.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    Dibujamos la reducci칩n de la impureza en funci칩n de la longitud del pico.

    ```{julia}
    using Plots
    # Valores 칰nicos de longitud del pico.
    valores = unique(df.Longitud_pico)
    # Reducci칩n de la impureza para cada valor.
    reducciones = [reduccion_impureza(df, :Longitud_pico, val) for val in valores]
    # Graficamos el resultado.
    Plots.scatter(valores, reducciones, xlabel = "Longitud del pico", ylabel = "Reducci칩n de la impureza", legend = false)
    ```

    Y ahora obtenemos el valor 칩ptimo de divisi칩n del conjunto de datos seg칰n la longitud del pico.

    ```{julia}
    val_optimo = valores[argmax(reducciones)]
    ```
    ::: 

a.  Dividir aleatoriamente el dataframe en un conjunto de entrenamiento y un conjunto de test con proporciones $3/4$ y $1/4$ respectivamente.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Utilizar la funci칩n [`shuffle`](https://docs.julialang.org/en/v1/stdlib/Random/#Random.shuffle) del paquete [`Random`](https://docs.julialang.org/en/v1/stdlib/Random/) para barajar el dataframe y luego dividirlo en dos subconjuntos.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n    

    ```{julia}
    using Random
    # Establecemos la semilla para la reproducibilidad.
    Random.seed!(1234)
    # Barajamos el dataframe.
    df = shuffle(df)
    # Dividimos el dataframe en un conjunto de entrenamiento y un conjunto de test.
    n = nrow(df)
    df_test = df[1:div(n, 4), :]
    df_train = df[div(n, 4)+1:end, :]
    ```
    :::

a.  Construir un 치rbol de decisi칩n con el conjunto de entrenamiento sin tener en cuenta la variable `Isla` y visualizarlo.
    
    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using DecisionTree, CategoricalArrays
    # Variables predictivas.
    X_train = Matrix(select(df_train, Not(:Isla, :Especie)))
    # Variable objetivo.
    y_train = df_train.Especie
    # Convertir las variables categ칩ricas a enteros.
    X_train = hcat([levelcode.(categorical(X_train[:, j])) for j in 1:size(X_train, 2)]...)
    # Convertir la variable objetivo a enteros
    y_train = levelcode.(categorical(y_train))

    # Construimos el 치rbol de decisi칩n con profundidad m치xima 3.
    tree = DecisionTreeClassifier(max_depth = 3)
    fit!(tree, X_train, y_train)
    print_tree(tree, feature_names=names(df)[3:end])
    ```
    :::

a.  Predecir la especie de los ping칲inos del conjunto de test y calcular la matriz de confusi칩n de las predicciones.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Utilizar la funci칩n [`confmat`](https://juliaai.github.io/StatisticalMeasures.jl/stable/confusion_matrices/#StatisticalMeasures.ConfusionMatrices.confmat) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para barajar el dataframe y luego dividirlo en dos subconjuntos.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using StatisticalMeasures
    # Variables predictivas
    X_test = Matrix(select(df_test, Not(:Isla, :Especie)))
    # Variable objetivo
    y_test = df_test.Especie
    # Convertir las variables categ칩ricas a enteros
    X_test = hcat([levelcode.(categorical(X_test[:, j])) for j in 1:size(X_test, 2)]...)
    # Convertir la variable objetivo a enteros
    y_test = levelcode.(categorical(y_test))
    # Predecimos la especie de ping칲ino del conjunto de test
    y_pred = predict(tree, X_test)
    # Calculamos la precisi칩n del modelo
    confmat(y_pred, y_test)
    ```
    :::

a.  Calcular la precisi칩n del modelo.

    :::{.callout-note collapse="true"}
    ## Ayuda
    La precisi칩n es la proporci칩n de predicciones correctas sobre el total de predicciones.

    Utilizar la funci칩n [`accuracy`](https://juliaai.github.io/StatisticalMeasures.jl/stable/auto_generated_list_of_measures/#StatisticalMeasures.Accuracy) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para calcular la precisi칩n del modelo.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    # Calculamos la precisi칩n del modelo
    accuracy(y_pred, y_test)
    ```
    :::

:::

:::{#exr-arboles-decision-3}
El fichero [`vinos.csv`](datos/vinos.csv) contiene informaci칩n sobre las caracter칤sticas de una muestra de vinos portugueses de la denominaci칩n "Vinho Verde". Las variables que contiene son:

| Variable             | Descripci칩n                                                           | Tipo (unidades)        |
|----------------------------------------|-----------------------------------------------------------------------|------------------------|
| tipo                 | Tipo de vino                                                          | Categ칩rica (blanco, tinto) |
| meses.barrica        | Mesesde envejecimiento en barrica                               | Num칠rica(meses)  |
| acided.fija          | Cantidadde 치cidotart치rico                                 | Num칠rica(g/dm3)  |
| acided.volatil       | Cantidad de 치cido ac칠tico                                             | Num칠rica(g/dm3)  |
| acido.citrico        | Cantidad de 치cidoc칤trico                                        | Num칠rica(g/dm3)  |
| azucar.residual      | Cantidad de az칰carremanente despu칠s de la fermentaci칩n          | Num칠rica(g/dm3)  |
| cloruro.sodico       | Cantidad de cloruros칩dico                                       | Num칠rica(g/dm3)  |
| dioxido.azufre.libre | Cantidad de di칩xido de azufreen formalibre                | Num칠rica(mg/dm3) |
| dioxido.azufre.total | Cantidadde di칩xido de azufretotal en forma libre o ligada | Num칠rica(mg/dm3) |
| densidad             | Densidad                                                              | Num칠rica(g/cm3)  |
| ph                   | pH                                                                    | Num칠rica(0-14)   |
| sulfatos             | Cantidadde sulfato de potasio                                   | Num칠rica(g/dm3)  |
| alcohol              | Porcentajede contenidode alcohol                          | Num칠rica(0-100)  |
| calidad              | Calificaci칩n otorgada porun panel de expertos                   | Num칠rica(0-10)   |

a.  Crear un data frame con los datos de los vinos a partir del fichero [`vinos.csv`](datos/vinos.csv).

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using CSV, DataFrames
    df = CSV.read("datos/vinos.csv", DataFrame)
    ```
    :::

a.  Mostrar los tipos de cada variable del data frame.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n `schema` del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/).
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using MLJ
    schema(df)
    ```
    :::

a.  Hacer un an치lisis de los datos perdidos en el data frame.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    describe(df, :nmissing)
    ```
    :::

a.  Mostrar la distribuci칩n de frecuencias de las variables cuantitativas del data frame mediante histogramas.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using GLMakie
    fig = Figure() 
    ax = [Axis(fig[trunc(Int, i / 3), i % 3], title = names(df)[i+2]) for i in 0:12]
    for i in 1:13
        hist!(ax[i], df[!, i+1], strokewidth = 0.5, strokecolor = (:white, 0.5))
    end
    fig
    ```
    :::

a.  Se considera que un vino es bueno si tiene una puntuaci칩n de calidad mayor que $6.5$. Recodificar la variable `calidad` en una variable categ칩rica que tome el valor 1 si la calidad es mayor que $6.5$ y 0 en caso contrario.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    using CategoricalArrays
    # Recodificamos la variable calidad.
    df.calidad = cut(df.calidad, [0, 6.5, 10], labels = [" 驕좶잺 ", " 游땕 "])
    ```
    :::

a.  Descomponer el data frame en un data frame con las variables predictivas y un vector con la variable objetivo `bueno`.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    y, X = unpack(df, ==(:calidad), rng = 123)
    ```
    :::

a.  Para poder entrenar un modelo de un arbol de decisi칩n, las variables predictivas deben ser cuantitativas. Transmformar las variables categ칩ricas en variables num칠ricas.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    # Convertir las variables categ칩ricas a enteros.
    coerce!(X, :tipo => OrderedFactor, :meses_barrica => Continuous)
    schema(X)
    ```
    :::

a.  Definir un modelo de 치rbol de decisi칩n con profundidad m치xima 3.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Cargar el modelo `DecisionTreeClassifier` del paquete [`DecisionTree`](https://docs.juliahub.com/DecisionTree/) con la macros `@iload`.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    # Cargamos el tipo de modelo.
    Tree = @iload DecisionTreeClassifier pkg = "DecisionTree"
    # Instanciamos el modelo con sus par치metros.
    arbol = Tree(max_depth =3, rng = 123)
    ```
    :::

a.  Evaluar el modelo tomando un 70% de ejemplos en el conjunto de entrenamiento y un 30% en el conjunto de test. Utilizar como m칠trica la precisi칩n.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n [`evaluate`](https://juliaai.github.io/MLJ.jl/stable/evaluating_model_performance/#MLJBase.evaluate!) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para evaluar el modelo. Los par치metros m치s importantes de esta funci칩n son:

    - `resampling`: Indica el m칠todo de muestreo para definir los conjuntos de entrenamiento y test. Los m칠todos m치s habituales son:
        - `Holdout(fraction_train = p)`: Divide el conjunto de datos tomando una proporci칩n de $p$ ejemplos en el conjunto de entrenamiento y $1-p$ en el conjunto de test.
        - `CV(nfolds = n, shuffle = true|false)`: Utiliza validaci칩n cruzada con `n` iteraciones. Si se indica `shuffle = true`, se utiliza validaci칩n cruzada aleatoria.
        - `StratifiedCV(nfolds = n, shuffle = true|false)`: Utiliza validaci칩n cruzada estratificada con `n` iteraciones. Si se indica `shuffle = true`, se utiliza validaci칩n cruzada estratificada aleatoria.
        - `InSample()`: Utiliza el conjunto de entrenamiento como conjunto de test.
  
    - `measures`: Indica las m칠tricas a utilizar para evaluar el modelo. Las m칠tricas m치s habituales son:
        - `cross_entropy`: P칠rdida de entrop칤a cruzada.
        - `confusion_matrix`: Matriz de confusi칩n.
        - `true_positive_rate`: Tasa de verdaderos positivos.
        - `true_negative_rate`: Tasa de verdaderos negativos.
        - `ppv`: Valor predictivo positivo.
        - `npv`: Valor predictivo negativo.
        - `accuracy`: Precisi칩n.
    
        Se puede indicar m치s de una en un vector.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    evaluate(arbol, X, y, resampling = Holdout(fraction_train = 0.7, rng = 123), measures = accuracy)
    ```
    :::

a.  Evaluar el modelo mediante validaci칩n cruzada estratificada usando las m칠tricas de la p칠rdida de entrop칤a cruzada, la matriz de confusi칩n, la tasa de verdaderos positivos, la tasa de verdaderos negativos, el valor predictivo positivo, el valor predictivo negativo y la precisi칩n. 쮼s un buen modelo?

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    evaluate(arbol, X, y, resampling = StratifiedCV(rng = 123), measures = [cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy])
    ```

    La precisi칩n del modelo es de $0.834$ que no est치 mal, pero si consdieramos la tasa de verdadero positivos, que es $0.13$ y la tasa de verdaderos negativos, que es pr치cticamente 1, el modelo tiene un buen rendimiento en la clasificaci칩n de los vinos malos, pero un mal rendimiento en la clasificaci칩n de los vinos buenos. Por lo tanto, no podemos decir que sea un buen modelo.
    :::

a.  Construir 치rboles de decisi칩n con profundidades m치ximas de 2 a 10 y evaluar el modelo con validaci칩n cruzada estratificada. 쮺u치l es la profundidad m치xima que da mejor resultado?

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n [`TunedModel`](https://juliaai.github.io/MLJ.jl/stable/tuning_models/#MLJTuning.TunedModel) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para ajustar los par치metros del modelo.

    Los par치metros m치s importantes de esta funci칩n son:
    - `model`: Indica el modelo a ajustar.
    - `resampling`: Indica el m칠todo de muestreo para definir los conjuntos de entrenamiento y test.
    - `tuning`: Indica el m칠todo de ajuste de los par치metros del modelo. Los m칠todos m치s habituales son:
        - `Grid(resolution = n)`: Ajusta los par치metros del modelo utilizando una cuadr칤cula de b칰squeda con `n` valores.
        - `RandomSearch(resolution = n)`: Ajusta los par치metros del modelo utilizando una b칰squeda aleatoria con `n` valores.
    - range: Indica el rango de valores a utilizar para ajustar los par치metros del modelo. Se puede indicar un rango de valores o un vector de valores.
    - `measure`: Indica la m칠trica a utilizar para evaluar el modelo.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    # Instanciamos el modelo de 치rbol de decisi칩n.
    arbol = Tree()
    # Definimos el rango de valores a utilizar para ajustar los par치metros del modelo.
    r = range(arbol, :max_depth, lower=2, upper=10)
    # Ajustamos los par치metros del modelo utilizando una cuadr칤cula de b칰squeda con 9 valores.
    arbol_parametrizado = TunedModel(
        model = arbol,
        resampling = StratifiedCV(rng = 123),
        tuning = Grid(resolution = 9),
        range = r,
        measure = accuracy)
    # Definimos una m치quina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.
    mach = machine(arbol_parametrizado, X, y)
    # Ajustamos los par치metros del modelo.
    MLJ.fit!(mach)
    # Mostramos los par치metros del mejor modelo.
    fitted_params(mach).best_model
    ```
    :::

a.  Dibujar la curva de aprendizaje del modelo en funci칩n de la profundidad del 치rbol de decisi칩n.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n [`learning_curve`](https://juliaai.github.io/MLJ.jl/stable/learning_curves/#MLJBase.learning_curve) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para dibujar la curva de aprendizaje.
    Los par치metros m치s importantes de esta funci칩n son:
    - `mach`: Indica la m치quina de aprendizaje a utilizar.
    - `range`: Indica el rango de valores a utilizar para ajustar los par치metros del modelo.
    - `resampling`: Indica el m칠todo de muestreo para definir los conjuntos de entrenamiento y test.
    - `measure`: Indica la m칠trica a utilizar para evaluar el modelo.
    - `rngs`: Indica la semilla para la generaci칩n de n칰meros aleatorios. Se pueden indicar varias semillas en un vector y se genera una curva de aprendizaje para cada semilla.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    # Instanciamos el modelo de 치rbol de decisi칩n.
    arbol = Tree()
    # Definimos una m치quina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.
    mach = machine(arbol, X, y)
    # Definimos el rango de valores a utilizar para ajustar los par치metros del modelo.
    r = range(arbol, :max_depth, lower=2, upper=10)
    # Dibujamos la curva de aprendizaje.
    curva = learning_curve(mach, range = r, resampling = StratifiedCV(rng = 123), measure = accuracy)
    # Dibujamos la curva de aprendizaje.
    fig = Figure()
    ax = Axis(fig[1, 1], title = "Curva de aprendizaje", xlabel = "Profundidad del 치rbol", ylabel = "Precisi칩n")
    Makie.scatter!(ax, curva.parameter_values, curva.measurements)
    fig
    ```
    :::

a.  Construir un 치rbol de decisi칩n con la profundidad m치xima que da mejor resultado y visualizarlo.

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    # Instanciamos el modelo de 치rbol de decisi칩n.
    arbol = Tree(max_depth = 4)
    # Definimos una m치quina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.
    mach = machine(arbol, X, y)
    # Ajustamos los par치metros del modelo.
    MLJ.fit!(mach)
    # Visualizamos el 치rbol de decisi칩n.
    fitted_params(mach).tree
    ```
    :::

a.  쮺u치l es la importancia de cada variable en el modelo?

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n `feature_importances` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para calcular la importancia de cada variable.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    ```{julia}
    # Calculamos la importancia de cada variable.
    feature_importances(mach)
    ```
    :::

a.  Predecir la calidad de los 10 primeros vinos del conjunto de ejemplos.

    :::{.callout-note collapse="true"}
    ## Ayuda
    Usar la funci칩n `predict` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para predecir las probabilidades de pertenecer a cada clase un ejemplo o conjunto de ejemplos.

    Usar la funci칩n `predict_mode` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para predecir la clase de un ejemplo o conjunto de ejemplos.
    :::

    :::{.callout-tip collapse="true"}
    ## Soluci칩n

    Primero calculamos las probabilidades de cada clase.

    ```{julia}
    MLJ.predict(mach, X[1:10, :])
    ```

    Y ahora predecimos la clase.

    ```{julia}
    predict_mode(mach, X[1:10, :])
    ```
    :::
:::