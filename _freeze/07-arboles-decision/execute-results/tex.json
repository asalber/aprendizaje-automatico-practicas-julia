{
  "hash": "92067a4ad2cd01c543ada044490be865",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Árboles de decisión\nlang: es\n---\n\nLos árboles de decisión son modelos de aprendizaje simples e intuitivos que pueden utilizarse para tanto para predecir variables cuantitativas (regresión) como categóricas (clasificación). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje basados en árboles de decisión con Julia.\n\n## Ejercicios Resueltos\n\nPara la realización de esta práctica se requieren los siguientes paquetes:\n\n```julia\nusing CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing Tidier # Para el preprocesamiento de datos.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de gráficas.\nusing GLMakie  # Para obtener gráficos interactivos.\nusing AlgebraOfGraphics # Para generar gráficos mediante la gramática de gráficos.\nusing DecisionTree # Para construir árboles de decisión.\nusing GraphMakie # Para la visualización de árboles de decisión.\n```\n\n:::{#exr-arboles-decision-1}\nEl conjunto de datos [`tenis.csv`](/datos/tenis.csv) contiene información sobre las condiciones meteorológicas de varios días y si se pudo jugar al tenis o no.\n\na.  Cargar los datos del archivo `tenis.csv` en un data frame.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=1}\n    ``` {.julia .cell-code}\n    using CSV, DataFrames\n    df = CSV.read(\"datos/tenis.csv\", DataFrame)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=2}\n    ```{=tex}\n    \\begin{tabular}{r|ccccc}\n    \t& Cielo & Temperatura & Humedad & Viento & Tenis\\\\\n    \t\\hline\n    \t& String15 & String15 & String7 & String7 & String3\\\\\n    \t\\hline\n    \t1 & Soleado & Caluroso & Alta & Suave & No \\\\\n    \t2 & Soleado & Caluroso & Alta & Fuerte & No \\\\\n    \t3 & Nublado & Caluroso & Alta & Suave & Sí \\\\\n    \t4 & Lluvioso & Moderado & Alta & Suave & Sí \\\\\n    \t5 & Lluvioso & Frío & Normal & Suave & Sí \\\\\n    \t6 & Lluvioso & Frío & Normal & Fuerte & No \\\\\n    \t7 & Nublado & Frío & Normal & Fuerte & Sí \\\\\n    \t8 & Soleado & Moderado & Alta & Suave & No \\\\\n    \t9 & Soleado & Frío & Normal & Suave & Sí \\\\\n    \t10 & Lluvioso & Moderado & Normal & Suave & Sí \\\\\n    \t11 & Soleado & Moderado & Normal & Fuerte & Sí \\\\\n    \t12 & Nublado & Moderado & Alta & Fuerte & Sí \\\\\n    \t13 & Nublado & Caluroso & Normal & Suave & Sí \\\\\n    \t14 & Lluvioso & Moderado & Alta & Fuerte & No \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Crear un diagrama de barras que muestre la distribución de frecuencias de cada variable meteorológica según si se pudo jugar al tenis o no. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=2}\n    ``` {.julia .cell-code}\n    using GLMakie, AlgebraOfGraphics\n    \n    function frecuencias(df::DataFrame, var::Symbol)\n        # Calculamos el número de días de cada clase que se juega al tenis.\n        frec = combine(groupby(df, [var, :Tenis]), nrow => :Días)\n        # Dibujamos el diagrama de barras.\n        plt = data(frec) * \n        mapping(var, :Días, stack = :Tenis, color = :Tenis, ) * \n        visual(BarPlot) \n        # Devolvemos el gráfico.\n        return plt\n    end\n    \n    fig = Figure()\n    draw!(fig[1, 1], frecuencias(df, :Cielo))\n    draw!(fig[1, 2], frecuencias(df, :Temperatura))\n    draw!(fig[1, 3], frecuencias(df, :Humedad))\n    draw!(fig[1, 4], frecuencias(df, :Viento))\n    fig\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=3}\n    ![](07-arboles-decision_files/figure-pdf/cell-3-output-1.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    A la vista de las frecuencias de cada variable, las variable `Cielo` y `Humedad` parecen ser las que más influye en la decisión de jugar al tenis.\n    :::\n\na.  Calcular la impureza del conjunto de datos utilizando el índice de Gini. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    El [índice de Gini](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) se calcula mediante la fórmula\n    \n    $$ GI = 1 - \\sum_{i=1}^{n} p_i^2 $$\n\n    donde $p_i$ es la proporción de cada clase en el conjunto de datos y $n$ es el número de clases.\n    \n    El índice de Gini toma valores entre $0$ y $1-\\frac{1}{n}$ ($0.5$ en el caso de clasificación binaria), donde $0$ indica que todas las instancias pertenecen a una sola clase (mínima impureza) y $1-\\frac{1}{n}$ indica que las instancias están distribuidas uniformemente entre todas las clases (máxima impureza).\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=3}\n    ``` {.julia .cell-code}\n    function gini(df::DataFrame, var::Symbol)\n        # Calculamos el número de ejemplos.\n        n = nrow(df)\n        # Calculamos las frecuencias absolutas de cada clase.\n        frec = combine(groupby(df, var), nrow => :ni)\n        # Calculamos la proporción de cada clase.\n        frec.p = frec.ni ./ n\n        # Calculamos el índice de Gini.\n        gini = 1 - sum(frec.p .^ 2)\n        return gini\n    end\n    \n    g0 = gini(df, :Tenis)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=4}\n    ```\n    0.4591836734693877\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  ¿Qué reducción del índice Gini se obtiene si dividimos el conjunto de ejemplos según la variable `Humedad`? ¿Y si dividimos el conjunto con respecto a la variable `Viento`?\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    La reducción del índice de Gini se calcula como la diferencia entre el índice de Gini del conjunto original y el índice de Gini del conjunto dividido.\n    \n    $$ \\Delta GI = GI_{original} - GI_{dividido} $$\n\n    donde el índice de Gini del conjunto dividido es la media ponderada de los índices de Gini de los subconjuntos resultantes de la división.\n    :::\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n    Calculamos primero la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Humedad`.\n\n\n    ::: {.cell execution_count=4}\n    ``` {.julia .cell-code}\n    using Tidier\n    # Dividimos el conjunto de ejemplos según la variable Humedad.\n    df_humedad_alta = @filter(df, Humedad == \"Alta\")\n    df_humedad_normal = @filter(df, Humedad == \"Normal\")\n    # Calculamos los tamaños de los subconjuntos de ejemplos.\n    n = nrow(df_humedad_alta), nrow(df_humedad_normal)\n    # Calculamos el índice de Gini de cada subconjunto.\n    gis = gini(df_humedad_alta, :Tenis), gini(df_humedad_normal, :Tenis)\n    # Calculamos media ponderada de los índices de Gini de los subconjuntos \n    g_humedad = sum(gis .* n) / sum(n)\n    # Calculamos la reducción del índice de Gini.\n    g0 - g_humedad\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=5}\n    ```\n    0.09183673469387743\n    ```\n    :::\n    :::\n    \n    \n    Calculamos ahora la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Viento`.\n\n\n    ::: {.cell execution_count=5}\n    ``` {.julia .cell-code}\n    # Dividimos el conjunto de ejemplos según la variable `Viento`\n    df_viento_fuerte = @filter(df, Viento == \"Fuerte\")\n    df_viento_suave = @filter(df, Viento == \"Suave\")\n    # Calculamos los tamaños de los subconjuntos de ejemplos\n    n = nrow(df_viento_fuerte), nrow(df_viento_suave)\n    # Calculamos el índice de Gini de cada subconjunto\n    gis = gini(df_viento_fuerte, :Tenis), gini(df_viento_suave, :Tenis)\n    # Calculamos media ponderada de los índices de Gini de los subconjuntos\n    g_viento = sum(gis .* n) / sum(n)\n    # Calculamos la reducción del índice de Gini\n    g0 - g_viento\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=6}\n    ```\n    0.030612244897959162\n    ```\n    :::\n    :::\n    \n    \n    Como se puede observar, la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Humedad` es mayor que la reducción del índice de Gini al dividir el conjunto con respecto a la variable `Viento`. Por lo tanto, la variable `Humedad` parece tener más influencia en la decisión de jugar al tenis y sería la variable que se debería elegir para dividir el conjunto de ejemplos.\n\na.  Construir un árbol de decisión que explique si se puede jugar al tenis en función de las variables meteorológicas.\n    \n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `DecisionTreeClassifier` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/). \n    :::\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=6}\n    ``` {.julia .cell-code}\n    using DecisionTree, CategoricalArrays\n    # Variables predictoras.\n    X = Matrix(select(df, Not(:Tenis)))\n    # Variable objetivo.\n    y = df.Tenis\n    # Convertir las variables categóricas a enteros.\n    X = hcat([levelcode.(categorical(X[:, j])) for j in 1:size(X, 2)]...)\n    # Convertir la variable objetivo a enteros.\n    y = levelcode.(categorical(y))\n    tree = DecisionTreeClassifier(max_depth=3)\n    fit!(tree, X, y)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=7}\n    ```\n    DecisionTreeClassifier\n    max_depth:                3\n    min_samples_leaf:         1\n    min_samples_split:        2\n    min_purity_increase:      0.0\n    pruning_purity_threshold: 1.0\n    n_subfeatures:            0\n    classes:                  [1, 2]\n    root:                     Decision Tree\n    Leaves: 6\n    Depth:  3\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Visualizar el árbol de decisión construido.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `plot_tree` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/).\n    :::\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=7}\n    ``` {.julia .cell-code}\n    print_tree(tree, feature_names=names(df)[1:end-1])\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Feature 3: \"Humedad\" < 2.0 ?\n    ├─ Feature 1: \"Cielo\" < 3.0 ?\n        ├─ Feature 4: \"Viento\" < 2.0 ?\n            ├─ 2 : 1/2\n            └─ 2 : 2/2\n        └─ 1 : 3/3\n    └─ Feature 1: \"Cielo\" < 2.0 ?\n        ├─ Feature 4: \"Viento\" < 2.0 ?\n            ├─ 1 : 1/1\n            └─ 2 : 2/2\n        └─ 2 : 4/4\n    ```\n    :::\n    :::\n    \n    \n:::\n\n\n:::{#exr-arboles-decision-2}\nEl conjunto de datos [pingüinos.csv]() contiene un conjunto de datos sobre tres eEspecie de pingüinos con las siguientes variables:\n\n- Especie: Especie de pingüino, comúnmente Adelie, Chinstrap o Gentoo.\n- Isla: Isla del archipiélago Palmer donde se realizó la observación.\n- Longitud_pico: Longitud del pico en mm.\n- Profundidad_pico: Profundidad del pico en mm\n- Longitud_ala: Longitud de la aleta en mm.\n- Peso: Masa corporal en gramos.\n- Sexo: Sexo\n\na.  Cargar los datos del archivo `pinguïnos.csv` en un data frame.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=8}\n    ``` {.julia .cell-code}\n    using CSV, DataFrames\n    df = CSV.read(\"datos/pingüinos.csv\", DataFrame, missingstring=\"NA\")\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=9}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& Especie & Isla & Longitud\\_pico & Profundidad\\_pico & Longitud\\_ala & Peso & Sexo\\\\\n    \t\\hline\n    \t& String15 & String15 & Float64? & Float64? & Int64? & Int64? & String7?\\\\\n    \t\\hline\n    \t1 & Adelie & Torgersen & 39.1 & 18.7 & 181 & 3750 & macho \\\\\n    \t2 & Adelie & Torgersen & 39.5 & 17.4 & 186 & 3800 & hembra \\\\\n    \t3 & Adelie & Torgersen & 40.3 & 18.0 & 195 & 3250 & hembra \\\\\n    \t4 & Adelie & Torgersen & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} \\\\\n    \t5 & Adelie & Torgersen & 36.7 & 19.3 & 193 & 3450 & hembra \\\\\n    \t6 & Adelie & Torgersen & 39.3 & 20.6 & 190 & 3650 & macho \\\\\n    \t7 & Adelie & Torgersen & 38.9 & 17.8 & 181 & 3625 & hembra \\\\\n    \t8 & Adelie & Torgersen & 39.2 & 19.6 & 195 & 4675 & macho \\\\\n    \t9 & Adelie & Torgersen & 34.1 & 18.1 & 193 & 3475 & \\emph{missing} \\\\\n    \t10 & Adelie & Torgersen & 42.0 & 20.2 & 190 & 4250 & \\emph{missing} \\\\\n    \t11 & Adelie & Torgersen & 37.8 & 17.1 & 186 & 3300 & \\emph{missing} \\\\\n    \t12 & Adelie & Torgersen & 37.8 & 17.3 & 180 & 3700 & \\emph{missing} \\\\\n    \t13 & Adelie & Torgersen & 41.1 & 17.6 & 182 & 3200 & hembra \\\\\n    \t14 & Adelie & Torgersen & 38.6 & 21.2 & 191 & 3800 & macho \\\\\n    \t15 & Adelie & Torgersen & 34.6 & 21.1 & 198 & 4400 & macho \\\\\n    \t16 & Adelie & Torgersen & 36.6 & 17.8 & 185 & 3700 & hembra \\\\\n    \t17 & Adelie & Torgersen & 38.7 & 19.0 & 195 & 3450 & hembra \\\\\n    \t18 & Adelie & Torgersen & 42.5 & 20.7 & 197 & 4500 & macho \\\\\n    \t19 & Adelie & Torgersen & 34.4 & 18.4 & 184 & 3325 & hembra \\\\\n    \t20 & Adelie & Torgersen & 46.0 & 21.5 & 194 & 4200 & macho \\\\\n    \t21 & Adelie & Biscoe & 37.8 & 18.3 & 174 & 3400 & hembra \\\\\n    \t22 & Adelie & Biscoe & 37.7 & 18.7 & 180 & 3600 & macho \\\\\n    \t23 & Adelie & Biscoe & 35.9 & 19.2 & 189 & 3800 & hembra \\\\\n    \t24 & Adelie & Biscoe & 38.2 & 18.1 & 185 & 3950 & macho \\\\\n    \t25 & Adelie & Biscoe & 38.8 & 17.2 & 180 & 3800 & macho \\\\\n    \t26 & Adelie & Biscoe & 35.3 & 18.9 & 187 & 3800 & hembra \\\\\n    \t27 & Adelie & Biscoe & 40.6 & 18.6 & 183 & 3550 & macho \\\\\n    \t28 & Adelie & Biscoe & 40.5 & 17.9 & 187 & 3200 & hembra \\\\\n    \t29 & Adelie & Biscoe & 37.9 & 18.6 & 172 & 3150 & hembra \\\\\n    \t30 & Adelie & Biscoe & 40.5 & 18.9 & 180 & 3950 & macho \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \na.  Hacer un análisis de los datos perdidos en el data frame. \n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=9}\n    ``` {.julia .cell-code}\n    describe(df, :nmissing)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=10}\n    ```{=tex}\n    \\begin{tabular}{r|cc}\n    \t& variable & nmissing\\\\\n    \t\\hline\n    \t& Symbol & Int64\\\\\n    \t\\hline\n    \t1 & Especie & 0 \\\\\n    \t2 & Isla & 0 \\\\\n    \t3 & Longitud\\_pico & 2 \\\\\n    \t4 & Profundidad\\_pico & 2 \\\\\n    \t5 & Longitud\\_ala & 2 \\\\\n    \t6 & Peso & 2 \\\\\n    \t7 & Sexo & 11 \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Eliminar del data frame los casos con valores perdidos.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=10}\n    ``` {.julia .cell-code}\n    dropmissing!(df)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=11}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& Especie & Isla & Longitud\\_pico & Profundidad\\_pico & Longitud\\_ala & Peso & Sexo\\\\\n    \t\\hline\n    \t& String15 & String15 & Float64 & Float64 & Int64 & Int64 & String7\\\\\n    \t\\hline\n    \t1 & Adelie & Torgersen & 39.1 & 18.7 & 181 & 3750 & macho \\\\\n    \t2 & Adelie & Torgersen & 39.5 & 17.4 & 186 & 3800 & hembra \\\\\n    \t3 & Adelie & Torgersen & 40.3 & 18.0 & 195 & 3250 & hembra \\\\\n    \t4 & Adelie & Torgersen & 36.7 & 19.3 & 193 & 3450 & hembra \\\\\n    \t5 & Adelie & Torgersen & 39.3 & 20.6 & 190 & 3650 & macho \\\\\n    \t6 & Adelie & Torgersen & 38.9 & 17.8 & 181 & 3625 & hembra \\\\\n    \t7 & Adelie & Torgersen & 39.2 & 19.6 & 195 & 4675 & macho \\\\\n    \t8 & Adelie & Torgersen & 41.1 & 17.6 & 182 & 3200 & hembra \\\\\n    \t9 & Adelie & Torgersen & 38.6 & 21.2 & 191 & 3800 & macho \\\\\n    \t10 & Adelie & Torgersen & 34.6 & 21.1 & 198 & 4400 & macho \\\\\n    \t11 & Adelie & Torgersen & 36.6 & 17.8 & 185 & 3700 & hembra \\\\\n    \t12 & Adelie & Torgersen & 38.7 & 19.0 & 195 & 3450 & hembra \\\\\n    \t13 & Adelie & Torgersen & 42.5 & 20.7 & 197 & 4500 & macho \\\\\n    \t14 & Adelie & Torgersen & 34.4 & 18.4 & 184 & 3325 & hembra \\\\\n    \t15 & Adelie & Torgersen & 46.0 & 21.5 & 194 & 4200 & macho \\\\\n    \t16 & Adelie & Biscoe & 37.8 & 18.3 & 174 & 3400 & hembra \\\\\n    \t17 & Adelie & Biscoe & 37.7 & 18.7 & 180 & 3600 & macho \\\\\n    \t18 & Adelie & Biscoe & 35.9 & 19.2 & 189 & 3800 & hembra \\\\\n    \t19 & Adelie & Biscoe & 38.2 & 18.1 & 185 & 3950 & macho \\\\\n    \t20 & Adelie & Biscoe & 38.8 & 17.2 & 180 & 3800 & macho \\\\\n    \t21 & Adelie & Biscoe & 35.3 & 18.9 & 187 & 3800 & hembra \\\\\n    \t22 & Adelie & Biscoe & 40.6 & 18.6 & 183 & 3550 & macho \\\\\n    \t23 & Adelie & Biscoe & 40.5 & 17.9 & 187 & 3200 & hembra \\\\\n    \t24 & Adelie & Biscoe & 37.9 & 18.6 & 172 & 3150 & hembra \\\\\n    \t25 & Adelie & Biscoe & 40.5 & 18.9 & 180 & 3950 & macho \\\\\n    \t26 & Adelie & Dream & 39.5 & 16.7 & 178 & 3250 & hembra \\\\\n    \t27 & Adelie & Dream & 37.2 & 18.1 & 178 & 3900 & macho \\\\\n    \t28 & Adelie & Dream & 39.5 & 17.8 & 188 & 3300 & hembra \\\\\n    \t29 & Adelie & Dream & 40.9 & 18.9 & 184 & 3900 & macho \\\\\n    \t30 & Adelie & Dream & 36.4 & 17.0 & 195 & 3325 & hembra \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Crear diagramas que muestren la distribución de frecuencias de cada variable según la especie de pingüino. ¿Qué variable parece tener más influencia en la especie de pingüino?\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n    Para las variables cualitativas dibujamos diagramas de barras.\n\n\n    ::: {.cell execution_count=11}\n    ``` {.julia .cell-code}\n    using GLMakie, AlgebraOfGraphics\n    \n    frec_isla = combine(groupby(df, [:Isla, :Especie]), nrow => :Frecuencia)\n    data(frec_isla) * \n        mapping(:Isla, :Frecuencia, stack = :Especie, color =:Especie) *\n        visual(BarPlot) |> draw\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=12}\n    ![](07-arboles-decision_files/figure-pdf/cell-12-output-1.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=12}\n    ``` {.julia .cell-code}\n    frec_sexo = combine(groupby(df, [:Sexo, :Especie]), nrow => :Frecuencia)\n    data(frec_sexo) * \n        mapping(:Sexo, :Frecuencia, stack = :Especie, color =:Especie) *\n        visual(BarPlot) |> draw\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=13}\n    ![](07-arboles-decision_files/figure-pdf/cell-13-output-1.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    Para las variables cuantitativas dibujamos diagramas de cajas.\n\n\n    ::: {.cell execution_count=13}\n    ``` {.julia .cell-code}\n    function cajas(df, var, clase)\n        data(df) *\n            mapping(clase, var, color = clase) *\n            visual(BoxPlot) |> \n            draw\n    end\n    \n    cajas(df, :Longitud_pico, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=14}\n    ![](07-arboles-decision_files/figure-pdf/cell-14-output-1.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=14}\n    ``` {.julia .cell-code}\n    cajas(df, :Profundidad_pico, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=15}\n    ![](07-arboles-decision_files/figure-pdf/cell-15-output-1.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=15}\n    ``` {.julia .cell-code}\n    cajas(df, :Longitud_ala, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=16}\n    ![](07-arboles-decision_files/figure-pdf/cell-16-output-1.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=16}\n    ``` {.julia .cell-code}\n    cajas(df, :Peso, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=17}\n    ![](07-arboles-decision_files/figure-pdf/cell-17-output-1.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    :::\n\na.  ¿Cuál es la reducción de la impureza del conjunto de datos si dividimos el conjunto de datos en dos conjuntos según si la longitud del pico es mayor o menor que 44 mm?\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=17}\n    ``` {.julia .cell-code}\n    using Tidier\n    function gini(df::DataFrame, var::Symbol)\n        n = nrow(df)\n        frec = combine(groupby(df, var), nrow => :ni)\n        frec.p = frec.ni ./ n\n        gini = 1 - sum(frec.p .^ 2)\n        return gini\n    end\n    \n    function reduccion_impureza(df::DataFrame, var::Symbol, val::Number)\n        # Dividimos el conjunto de ejemplos según la longitud del pico es menor de 44.\n        df_menor = @eval @filter($df, $var <= $val)\n        df_mayor = @eval @filter($df, $var > $val)\n        # Calculamos los tamaños de los subconjuntos de ejemplos.\n        n = nrow(df_menor), nrow(df_mayor)\n        # Calculamos el índice de Gini de cada subconjunto.\n        gis = gini(df_menor, :Especie), gini(df_mayor, :Especie)\n        # Calculamos media ponderada de los índices de Gini de los subconjuntos.\n        g1 = sum(gis .* n) / sum(n)\n        # Calculamos la reducción del índice de Gini.\n        gini(df, :Especie) - g1\n    end\n    \n    reduccion_impureza(df, :Longitud_pico, 44)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=18}\n    ```\n    0.26577182779353914\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Determinar el valor óptimo de división del conjunto de datos según la longitud del pico. Para ello, calcular la reducción de la impureza para cada valor de longitud del pico y dibujar el resultado.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n    Dibujamos la reducción de la impureza en función de la longitud del pico.\n\n\n    ::: {.cell execution_count=18}\n    ``` {.julia .cell-code}\n    using Plots\n    # Valores únicos de longitud del pico.\n    valores = unique(df.Longitud_pico)\n    # Reducción de la impureza para cada valor.\n    reducciones = [reduccion_impureza(df, :Longitud_pico, val) for val in valores]\n    # Graficamos el resultado.\n    Plots.scatter(valores, reducciones, xlabel = \"Longitud del pico\", ylabel = \"Reducción de la impureza\", legend = false)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=19}\n    ![](07-arboles-decision_files/figure-pdf/cell-19-output-1.svg){fig-pos='H'}\n    :::\n    :::\n    \n    \n    Y ahora obtenemos el valor óptimo de división del conjunto de datos según la longitud del pico.\n\n\n    ::: {.cell execution_count=19}\n    ``` {.julia .cell-code}\n    val_optimo = valores[argmax(reducciones)]\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=20}\n    ```\n    42.3\n    ```\n    :::\n    :::\n    \n    \n    ::: \n\na.  Dividir aleatoriamente el dataframe en un conjunto de entrenamiento y un conjunto de test con proporciones $3/4$ y $1/4$ respectivamente.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Utilizar la función [`shuffle`](https://docs.julialang.org/en/v1/stdlib/Random/#Random.shuffle) del paquete [`Random`](https://docs.julialang.org/en/v1/stdlib/Random/) para barajar el dataframe y luego dividirlo en dos subconjuntos.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución    \n\n\n    ::: {.cell execution_count=20}\n    ``` {.julia .cell-code}\n    using Random\n    # Establecemos la semilla para la reproducibilidad.\n    Random.seed!(1234)\n    # Barajamos el dataframe.\n    df = shuffle(df)\n    # Dividimos el dataframe en un conjunto de entrenamiento y un conjunto de test.\n    n = nrow(df)\n    df_test = df[1:div(n, 4), :]\n    df_train = df[div(n, 4)+1:end, :]\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=21}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& Especie & Isla & Longitud\\_pico & Profundidad\\_pico & Longitud\\_ala & Peso & Sexo\\\\\n    \t\\hline\n    \t& String15 & String15 & Float64 & Float64 & Int64 & Int64 & String7\\\\\n    \t\\hline\n    \t1 & Adelie & Dream & 39.0 & 18.7 & 185 & 3650 & macho \\\\\n    \t2 & Chinstrap & Dream & 52.8 & 20.0 & 205 & 4550 & macho \\\\\n    \t3 & Chinstrap & Dream & 55.8 & 19.8 & 207 & 4000 & macho \\\\\n    \t4 & Adelie & Torgersen & 35.1 & 19.4 & 193 & 4200 & macho \\\\\n    \t5 & Adelie & Torgersen & 34.6 & 21.1 & 198 & 4400 & macho \\\\\n    \t6 & Gentoo & Biscoe & 50.0 & 15.2 & 218 & 5700 & macho \\\\\n    \t7 & Chinstrap & Dream & 50.6 & 19.4 & 193 & 3800 & macho \\\\\n    \t8 & Chinstrap & Dream & 43.5 & 18.1 & 202 & 3400 & hembra \\\\\n    \t9 & Adelie & Dream & 36.9 & 18.6 & 189 & 3500 & hembra \\\\\n    \t10 & Adelie & Dream & 36.6 & 18.4 & 184 & 3475 & hembra \\\\\n    \t11 & Chinstrap & Dream & 46.6 & 17.8 & 193 & 3800 & hembra \\\\\n    \t12 & Gentoo & Biscoe & 50.8 & 17.3 & 228 & 5600 & macho \\\\\n    \t13 & Chinstrap & Dream & 52.2 & 18.8 & 197 & 3450 & macho \\\\\n    \t14 & Adelie & Dream & 39.6 & 18.8 & 190 & 4600 & macho \\\\\n    \t15 & Adelie & Torgersen & 42.8 & 18.5 & 195 & 4250 & macho \\\\\n    \t16 & Adelie & Biscoe & 36.5 & 16.6 & 181 & 2850 & hembra \\\\\n    \t17 & Gentoo & Biscoe & 49.1 & 14.8 & 220 & 5150 & hembra \\\\\n    \t18 & Chinstrap & Dream & 43.2 & 16.6 & 187 & 2900 & hembra \\\\\n    \t19 & Gentoo & Biscoe & 43.3 & 13.4 & 209 & 4400 & hembra \\\\\n    \t20 & Gentoo & Biscoe & 49.5 & 16.1 & 224 & 5650 & macho \\\\\n    \t21 & Adelie & Biscoe & 37.8 & 20.0 & 190 & 4250 & macho \\\\\n    \t22 & Gentoo & Biscoe & 50.4 & 15.3 & 224 & 5550 & macho \\\\\n    \t23 & Adelie & Biscoe & 45.6 & 20.3 & 191 & 4600 & macho \\\\\n    \t24 & Chinstrap & Dream & 45.4 & 18.7 & 188 & 3525 & hembra \\\\\n    \t25 & Adelie & Dream & 39.2 & 18.6 & 190 & 4250 & macho \\\\\n    \t26 & Gentoo & Biscoe & 48.4 & 14.4 & 203 & 4625 & hembra \\\\\n    \t27 & Adelie & Torgersen & 35.2 & 15.9 & 186 & 3050 & hembra \\\\\n    \t28 & Gentoo & Biscoe & 48.4 & 16.3 & 220 & 5400 & macho \\\\\n    \t29 & Adelie & Dream & 33.1 & 16.1 & 178 & 2900 & hembra \\\\\n    \t30 & Adelie & Dream & 36.8 & 18.5 & 193 & 3500 & hembra \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Construir un árbol de decisión con el conjunto de entrenamiento sin tener en cuenta la variable `Isla` y visualizarlo.\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=21}\n    ``` {.julia .cell-code}\n    using DecisionTree, CategoricalArrays\n    # Variables predictivas.\n    X_train = Matrix(select(df_train, Not(:Isla, :Especie)))\n    # Variable objetivo.\n    y_train = df_train.Especie\n    # Convertir las variables categóricas a enteros.\n    X_train = hcat([levelcode.(categorical(X_train[:, j])) for j in 1:size(X_train, 2)]...)\n    # Convertir la variable objetivo a enteros\n    y_train = levelcode.(categorical(y_train))\n    \n    # Construimos el árbol de decisión con profundidad máxima 3.\n    tree = DecisionTreeClassifier(max_depth = 3)\n    fit!(tree, X_train, y_train)\n    print_tree(tree, feature_names=names(df)[3:end])\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Feature 3: \"Longitud_ala\" < 29.0 ?\n    ├─ Feature 1: \"Longitud_pico\" < 62.0 ?\n        ├─ 1 : 96/96\n        └─ Feature 1: \"Longitud_pico\" < 87.0 ?\n            ├─ 2 : 10/20\n            └─ 2 : 37/38\n    └─ Feature 2: \"Profundidad_pico\" < 46.0 ?\n        ├─ 3 : 90/90\n        └─ Feature 1: \"Longitud_pico\" < 109.0 ?\n            ├─ 1 : 2/2\n            └─ 2 : 4/4\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Predecir la especie de los pingüinos del conjunto de test y calcular la matriz de confusión de las predicciones.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Utilizar la función [`confmat`](https://juliaai.github.io/StatisticalMeasures.jl/stable/confusion_matrices/#StatisticalMeasures.ConfusionMatrices.confmat) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para barajar el dataframe y luego dividirlo en dos subconjuntos.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=22}\n    ``` {.julia .cell-code}\n    using StatisticalMeasures\n    # Variables predictivas\n    X_test = Matrix(select(df_test, Not(:Isla, :Especie)))\n    # Variable objetivo\n    y_test = df_test.Especie\n    # Convertir las variables categóricas a enteros\n    X_test = hcat([levelcode.(categorical(X_test[:, j])) for j in 1:size(X_test, 2)]...)\n    # Convertir la variable objetivo a enteros\n    y_test = levelcode.(categorical(y_test))\n    # Predecimos la especie de pingüino del conjunto de test\n    y_pred = predict(tree, X_test)\n    # Calculamos la precisión del modelo\n    confmat(y_pred, y_test)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=23}\n    ```\n              ┌──────────────┐\n              │ Ground Truth │\n    ┌─────────┼────┬────┬────┤\n    │Predicted│ 1  │ 2  │ 3  │\n    ├─────────┼────┼────┼────┤\n    │    1    │ 38 │ 11 │ 9  │\n    ├─────────┼────┼────┼────┤\n    │    2    │ 0  │ 6  │ 0  │\n    ├─────────┼────┼────┼────┤\n    │    3    │ 0  │ 0  │ 19 │\n    └─────────┴────┴────┴────┘\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Calcular la precisión del modelo.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    La precisión es la proporción de predicciones correctas sobre el total de predicciones.\n\n    Utilizar la función [`accuracy`](https://juliaai.github.io/StatisticalMeasures.jl/stable/auto_generated_list_of_measures/#StatisticalMeasures.Accuracy) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para calcular la precisión del modelo.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=23}\n    ``` {.julia .cell-code}\n    # Calculamos la precisión del modelo\n    accuracy(y_pred, y_test)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=24}\n    ```\n    0.7590361445783133\n    ```\n    :::\n    :::\n    \n    \n    :::\n\n:::\n\n:::{#exr-arboles-decision-3}\nEl fichero [`vinos.csv`](datos/vinos.csv) contiene información sobre las características de una muestra de vinos portugueses de la denominación \"Vinho Verde\". Las variables que contiene son:\n\n| Variable             | Descripción                                                           | Tipo (unidades)        |\n|----------------------------------------|-----------------------------------------------------------------------|------------------------|\n| tipo                 | Tipo de vino                                                          | Categórica (blanco, tinto) |\n| meses.barrica        | Mesesde envejecimiento en barrica                               | Numérica(meses)  |\n| acided.fija          | Cantidadde ácidotartárico                                 | Numérica(g/dm3)  |\n| acided.volatil       | Cantidad de ácido acético                                             | Numérica(g/dm3)  |\n| acido.citrico        | Cantidad de ácidocítrico                                        | Numérica(g/dm3)  |\n| azucar.residual      | Cantidad de azúcarremanente después de la fermentación          | Numérica(g/dm3)  |\n| cloruro.sodico       | Cantidad de clorurosódico                                       | Numérica(g/dm3)  |\n| dioxido.azufre.libre | Cantidad de dióxido de azufreen formalibre                | Numérica(mg/dm3) |\n| dioxido.azufre.total | Cantidadde dióxido de azufretotal en forma libre o ligada | Numérica(mg/dm3) |\n| densidad             | Densidad                                                              | Numérica(g/cm3)  |\n| ph                   | pH                                                                    | Numérica(0-14)   |\n| sulfatos             | Cantidadde sulfato de potasio                                   | Numérica(g/dm3)  |\n| alcohol              | Porcentajede contenidode alcohol                          | Numérica(0-100)  |\n| calidad              | Calificación otorgada porun panel de expertos                   | Numérica(0-10)   |\n\na.  Crear un data frame con los datos de los vinos a partir del fichero [`vinos.csv`](datos/vinos.csv).\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=24}\n    ``` {.julia .cell-code}\n    using CSV, DataFrames\n    df = CSV.read(\"datos/vinos.csv\", DataFrame)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=25}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& tipo & meses\\_barrica & acided\\_fija & acided\\_volatil & acido\\_citrico & azucar\\_residual & \\\\\n    \t\\hline\n    \t& String7 & Int64 & Float64 & Float64 & Float64 & Float64 & \\\\\n    \t\\hline\n    \t1 & blanco & 0 & 7.0 & 0.27 & 0.36 & 20.7 & $\\dots$ \\\\\n    \t2 & blanco & 0 & 6.3 & 0.3 & 0.34 & 1.6 & $\\dots$ \\\\\n    \t3 & blanco & 0 & 8.1 & 0.28 & 0.4 & 6.9 & $\\dots$ \\\\\n    \t4 & blanco & 0 & 7.2 & 0.23 & 0.32 & 8.5 & $\\dots$ \\\\\n    \t5 & blanco & 0 & 6.2 & 0.32 & 0.16 & 7.0 & $\\dots$ \\\\\n    \t6 & blanco & 0 & 8.1 & 0.22 & 0.43 & 1.5 & $\\dots$ \\\\\n    \t7 & blanco & 0 & 8.1 & 0.27 & 0.41 & 1.45 & $\\dots$ \\\\\n    \t8 & blanco & 0 & 8.6 & 0.23 & 0.4 & 4.2 & $\\dots$ \\\\\n    \t9 & blanco & 0 & 7.9 & 0.18 & 0.37 & 1.2 & $\\dots$ \\\\\n    \t10 & blanco & 0 & 6.6 & 0.16 & 0.4 & 1.5 & $\\dots$ \\\\\n    \t11 & blanco & 0 & 8.3 & 0.42 & 0.62 & 19.25 & $\\dots$ \\\\\n    \t12 & blanco & 0 & 6.6 & 0.17 & 0.38 & 1.5 & $\\dots$ \\\\\n    \t13 & blanco & 0 & 6.3 & 0.48 & 0.04 & 1.1 & $\\dots$ \\\\\n    \t14 & blanco & 0 & 6.2 & 0.66 & 0.48 & 1.2 & $\\dots$ \\\\\n    \t15 & blanco & 0 & 7.4 & 0.34 & 0.42 & 1.1 & $\\dots$ \\\\\n    \t16 & blanco & 0 & 6.5 & 0.31 & 0.14 & 7.5 & $\\dots$ \\\\\n    \t17 & blanco & 0 & 6.4 & 0.31 & 0.38 & 2.9 & $\\dots$ \\\\\n    \t18 & blanco & 0 & 6.8 & 0.26 & 0.42 & 1.7 & $\\dots$ \\\\\n    \t19 & blanco & 0 & 7.6 & 0.67 & 0.14 & 1.5 & $\\dots$ \\\\\n    \t20 & blanco & 0 & 6.6 & 0.27 & 0.41 & 1.3 & $\\dots$ \\\\\n    \t21 & blanco & 0 & 7.0 & 0.25 & 0.32 & 9.0 & $\\dots$ \\\\\n    \t22 & blanco & 0 & 6.9 & 0.24 & 0.35 & 1.0 & $\\dots$ \\\\\n    \t23 & blanco & 0 & 7.0 & 0.28 & 0.39 & 8.7 & $\\dots$ \\\\\n    \t24 & blanco & 0 & 7.4 & 0.27 & 0.48 & 1.1 & $\\dots$ \\\\\n    \t25 & blanco & 0 & 7.2 & 0.32 & 0.36 & 2.0 & $\\dots$ \\\\\n    \t26 & blanco & 0 & 8.5 & 0.24 & 0.39 & 10.4 & $\\dots$ \\\\\n    \t27 & blanco & 0 & 8.3 & 0.14 & 0.34 & 1.1 & $\\dots$ \\\\\n    \t28 & blanco & 0 & 7.4 & 0.25 & 0.36 & 2.05 & $\\dots$ \\\\\n    \t29 & blanco & 0 & 6.2 & 0.12 & 0.34 & 1.5 & $\\dots$ \\\\\n    \t30 & blanco & 0 & 5.8 & 0.27 & 0.2 & 14.95 & $\\dots$ \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Mostrar los tipos de cada variable del data frame.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `schema` del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/).\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=25}\n    ``` {.julia .cell-code}\n    using MLJ\n    schema(df)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    WARNING: using MLJ.fit! in module Main conflicts with an existing identifier.\n    WARNING: using MLJ.predict in module Main conflicts with an existing identifier.\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=26}\n    ```\n    ┌──────────────────────┬────────────┬─────────┐\n    │ names                │ scitypes   │ types   │\n    ├──────────────────────┼────────────┼─────────┤\n    │ tipo                 │ Textual    │ String7 │\n    │ meses_barrica        │ Count      │ Int64   │\n    │ acided_fija          │ Continuous │ Float64 │\n    │ acided_volatil       │ Continuous │ Float64 │\n    │ acido_citrico        │ Continuous │ Float64 │\n    │ azucar_residual      │ Continuous │ Float64 │\n    │ cloruro_sodico       │ Continuous │ Float64 │\n    │ dioxido_azufre_libre │ Continuous │ Float64 │\n    │ dioxido_azufre_total │ Continuous │ Float64 │\n    │ densidad             │ Continuous │ Float64 │\n    │ ph                   │ Continuous │ Float64 │\n    │ sulfatos             │ Continuous │ Float64 │\n    │ alcohol              │ Continuous │ Float64 │\n    │ calidad              │ Count      │ Int64   │\n    └──────────────────────┴────────────┴─────────┘\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Hacer un análisis de los datos perdidos en el data frame.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=26}\n    ``` {.julia .cell-code}\n    describe(df, :nmissing)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=27}\n    ```{=tex}\n    \\begin{tabular}{r|cc}\n    \t& variable & nmissing\\\\\n    \t\\hline\n    \t& Symbol & Int64\\\\\n    \t\\hline\n    \t1 & tipo & 0 \\\\\n    \t2 & meses\\_barrica & 0 \\\\\n    \t3 & acided\\_fija & 0 \\\\\n    \t4 & acided\\_volatil & 0 \\\\\n    \t5 & acido\\_citrico & 0 \\\\\n    \t6 & azucar\\_residual & 0 \\\\\n    \t7 & cloruro\\_sodico & 0 \\\\\n    \t8 & dioxido\\_azufre\\_libre & 0 \\\\\n    \t9 & dioxido\\_azufre\\_total & 0 \\\\\n    \t10 & densidad & 0 \\\\\n    \t11 & ph & 0 \\\\\n    \t12 & sulfatos & 0 \\\\\n    \t13 & alcohol & 0 \\\\\n    \t14 & calidad & 0 \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Se considera que un vino es bueno si tiene una puntuación de calidad mayor que $6.5$. Recodificar la variable `calidad` en una variable categórica que tome el valor 1 si la calidad es mayor que $6.5$ y 0 en caso contrario.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=27}\n    ``` {.julia .cell-code}\n    using CategoricalArrays\n    # Recodificamos la variable calidad.\n    df.calidad = cut(df.calidad, [0, 6.5, 10], labels = [0, 1])\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=28}\n    ```\n    5320-element CategoricalArray{Int64,1,UInt32}:\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     1\n     0\n     1\n     0\n     ⋮\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n     0\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Descomponer el data frame en un data frame con las variables predictivas y un vector con la variable objetivo `bueno`.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=28}\n    ``` {.julia .cell-code}\n    y, X = unpack(df, ==(:calidad), rng = 123)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=29}\n    ```\n    (CategoricalValue{Int64, UInt32}[0, 0, 0, 0, 0, 1, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 5320×13 DataFrame\n      Row │ tipo     meses_barrica  acided_fija  acided_volatil  acido_citrico  az ⋯\n          │ String7  Int64          Float64      Float64         Float64        Fl ⋯\n    ──────┼─────────────────────────────────────────────────────────────────────────\n        1 │ blanco               0          6.7           0.5             0.36     ⋯\n        2 │ blanco               0          6.3           0.2             0.3\n        3 │ blanco               0          6.2           0.35            0.03\n        4 │ tinto                3          8.0           0.39            0.3\n        5 │ blanco               0          7.9           0.255           0.26     ⋯\n        6 │ blanco               0          6.1           0.31            0.37\n        7 │ blanco               0          6.8           0.28            0.36\n        8 │ blanco               0          8.2           0.34            0.49\n        9 │ tinto                0          6.7           0.48            0.02     ⋯\n       10 │ blanco               0          7.4           0.35            0.2\n       11 │ tinto                5          7.5           0.53            0.06\n      ⋮   │    ⋮           ⋮             ⋮             ⋮               ⋮           ⋱\n     5311 │ blanco               0          7.2           0.14            0.35\n     5312 │ tinto                3          7.6           0.41            0.24     ⋯\n     5313 │ tinto                0          7.3           0.4             0.3\n     5314 │ tinto                4          7.1           0.48            0.28\n     5315 │ blanco               0          6.4           0.29            0.2\n     5316 │ blanco               0          9.4           0.24            0.29     ⋯\n     5317 │ blanco               0          6.3           0.25            0.27\n     5318 │ blanco               0          5.5           0.16            0.26\n     5319 │ blanco               0          7.4           0.36            0.32\n     5320 │ blanco               0          7.6           0.51            0.24     ⋯\n                                                     8 columns and 5299 rows omitted)\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Para poder entrenar un modelo de un arbol de decisión, las variables predictivas deben ser cuantitativas. Transmformar las variables categóricas en variables numéricas.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=29}\n    ``` {.julia .cell-code}\n    # Convertir las variables categóricas a enteros.\n    coerce!(X, :tipo => OrderedFactor, :meses_barrica => Continuous)\n    schema(X)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=30}\n    ```\n    ┌──────────────────────┬──────────────────┬───────────────────────────────────┐\n    │ names                │ scitypes         │ types                             │\n    ├──────────────────────┼──────────────────┼───────────────────────────────────┤\n    │ tipo                 │ OrderedFactor{2} │ CategoricalValue{String7, UInt32} │\n    │ meses_barrica        │ Continuous       │ Float64                           │\n    │ acided_fija          │ Continuous       │ Float64                           │\n    │ acided_volatil       │ Continuous       │ Float64                           │\n    │ acido_citrico        │ Continuous       │ Float64                           │\n    │ azucar_residual      │ Continuous       │ Float64                           │\n    │ cloruro_sodico       │ Continuous       │ Float64                           │\n    │ dioxido_azufre_libre │ Continuous       │ Float64                           │\n    │ dioxido_azufre_total │ Continuous       │ Float64                           │\n    │ densidad             │ Continuous       │ Float64                           │\n    │ ph                   │ Continuous       │ Float64                           │\n    │ sulfatos             │ Continuous       │ Float64                           │\n    │ alcohol              │ Continuous       │ Float64                           │\n    └──────────────────────┴──────────────────┴───────────────────────────────────┘\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Definir un modelo de árbol de decisión con profundidad máxima 3.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Cargar el modelo `DecisionTreeClassifier` del paquete [`DecisionTree`](https://docs.juliahub.com/DecisionTree/) con la macros `@iload`.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=30}\n    ``` {.julia .cell-code}\n    Tree = @iload DecisionTreeClassifier pkg = \"DecisionTree\"\n    tree = Tree(max_depth = 3, rng = 123)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    [ Info: For silent loading, specify `verbosity=0`. \n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    import MLJDecisionTreeInterface ✔\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=31}\n    ```\n    DecisionTreeClassifier(\n      max_depth = 3, \n      min_samples_leaf = 1, \n      min_samples_split = 2, \n      min_purity_increase = 0.0, \n      n_subfeatures = 0, \n      post_prune = false, \n      merge_purity_threshold = 1.0, \n      display_depth = 5, \n      feature_importance = :impurity, \n      rng = 123)\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Evaluar el modelo mediante validación cruzada usando las métricas de la pérdida de entropía cruzada estratificada, la matriz de confusión, la tasa de verdaderos positivos, la tasa de verdaderos negativos, el valor predictivo positivo, el valor predictivo negativo y la precisión. ¿Es un buen modelo?\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función [`evaluate`](https://juliaai.github.io/MLJ.jl/stable/evaluating_model_performance/#MLJBase.evaluate!) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para evaluar el modelo. \n    \n    Para indicar que se utilice como método de muestreo la validación cruzada se utiliza el parámetro `resampling = CV(shuffle=true)`, mientras que para usar validación cruzada estratificada se utiliza `resampling = StratifiedCV(shuffle=true)`. \n    \n    Para indicar las métricas a utilizar se utiliza el parámetro `measures = [cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy]`.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=31}\n    ``` {.julia .cell-code}\n    evaluate(tree, X, y, resampling = StratifiedCV(shuffle=true), measures=[cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy])\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    \rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:09\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:04\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=32}\n    ```\n    PerformanceEvaluation object with these fields:\n      model, measure, operation,\n      measurement, per_fold, per_observation,\n      fitted_params_per_fold, report_per_fold,\n      train_test_rows, resampling, repeats\n    Extract:\n    ┌───┬──────────────────────────┬──────────────┬─────────────────────────────────\n    │   │ measure                  │ operation    │ measurement                    ⋯\n    ├───┼──────────────────────────┼──────────────┼─────────────────────────────────\n    │ A │ LogLoss(                 │ predict      │ 0.376                          ⋯\n    │   │   tol = 2.22045e-16)     │              │                                ⋯\n    │ B │ ConfusionMatrix(         │ predict_mode │ ConfusionMatrix{2}([3819762 77 ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   perm = nothing,        │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ C │ TruePositiveRate(        │ predict_mode │ 0.129                          ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ D │ TrueNegativeRate(        │ predict_mode │ 0.999                          ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ E │ PositivePredictiveValue( │ predict_mode │ 0.978                          ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ F │ NegativePredictiveValue( │ predict_mode │ 0.831                          ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │ ⋮ │            ⋮             │      ⋮       │                         ⋮      ⋱\n    └───┴──────────────────────────┴──────────────┴─────────────────────────────────\n                                                         1 column and 2 rows omitted\n    ┌───┬───────────────────────────────────────────────────────────────────────────\n    │   │ per_fold                                                                 ⋯\n    ├───┼───────────────────────────────────────────────────────────────────────────\n    │ A │ [0.364, 0.399, 0.409, 0.363, 0.361, 0.362]                               ⋯\n    │ B │ ConfusionMatrix{2, true, CategoricalValue{Int64, UInt32}}[ConfusionMatri ⋯\n    │ C │ [0.179, 0.149, 0.107, 0.101, 0.137, 0.101]                               ⋯\n    │ D │ [1.0, 1.0, 0.999, 1.0, 0.997, 1.0]                                       ⋯\n    │ E │ [1.0, 1.0, 0.947, 1.0, 0.92, 1.0]                                        ⋯\n    │ F │ [0.839, 0.834, 0.827, 0.825, 0.832, 0.826]                               ⋯\n    │ G │ [0.844, 0.839, 0.83, 0.829, 0.834, 0.83]                                 ⋯\n    └───┴───────────────────────────────────────────────────────────────────────────\n                                                                   2 columns omitted\n    ```\n    :::\n    :::\n    \n    \n    La precisión del modelo es de $0.834$ que no está mal, pero si consdieramos la tasa de verdadero positivos, que es $0.13$ y la tasa de verdaderos negativos, que es prácticamente 1, el modelo tiene un buen rendimiento en la clasificación de los vinos malos, pero un mal rendimiento en la clasificación de los vinos malos. Por lo tanto, el modelo no es un buen modelo.\n    :::\n:::\n\n",
    "supporting": [
      "07-arboles-decision_files/figure-pdf"
    ],
    "filters": []
  }
}