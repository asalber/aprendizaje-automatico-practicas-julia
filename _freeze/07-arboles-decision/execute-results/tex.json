{
  "hash": "5926da2e5f25303f6501b503f71dcde2",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Árboles de decisión\nlang: es\n---\n\nLos árboles de decisión son modelos de aprendizaje simples e intuitivos que pueden utilizarse para tanto para predecir variables cuantitativas (regresión) como categóricas (clasificación). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje basados en árboles de decisión con Julia.\n\n## Ejercicios Resueltos\n\nPara la realización de esta práctica se requieren los siguientes paquetes:\n\n```julia\nusing CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing Tidier # Para el preprocesamiento de datos.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de gráficas.\nusing GLMakie  # Para obtener gráficos interactivos.\nusing AlgebraOfGraphics # Para generar gráficos mediante la gramática de gráficos.\nusing DecisionTree # Para construir árboles de decisión.\nusing GraphMakie # Para la visualización de árboles de decisión.\n```\n\n:::{#exr-arboles-decision-1}\nEl conjunto de datos [`tenis.csv`](/datos/tenis.csv) contiene información sobre las condiciones meteorológicas de varios días y si se pudo jugar al tenis o no.\n\na.  Cargar los datos del archivo `tenis.csv` en un data frame.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=1}\n    ``` {.julia .cell-code}\n    using CSV, DataFrames\n    df = CSV.read(\"datos/tenis.csv\", DataFrame)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=2}\n    ```{=tex}\n    \\begin{tabular}{r|ccccc}\n    \t& Cielo & Temperatura & Humedad & Viento & Tenis\\\\\n    \t\\hline\n    \t& String15 & String15 & String7 & String7 & String3\\\\\n    \t\\hline\n    \t1 & Soleado & Caluroso & Alta & Suave & No \\\\\n    \t2 & Soleado & Caluroso & Alta & Fuerte & No \\\\\n    \t3 & Nublado & Caluroso & Alta & Suave & Sí \\\\\n    \t4 & Lluvioso & Moderado & Alta & Suave & Sí \\\\\n    \t5 & Lluvioso & Frío & Normal & Suave & Sí \\\\\n    \t6 & Lluvioso & Frío & Normal & Fuerte & No \\\\\n    \t7 & Nublado & Frío & Normal & Fuerte & Sí \\\\\n    \t8 & Soleado & Moderado & Alta & Suave & No \\\\\n    \t9 & Soleado & Frío & Normal & Suave & Sí \\\\\n    \t10 & Lluvioso & Moderado & Normal & Suave & Sí \\\\\n    \t11 & Soleado & Moderado & Normal & Fuerte & Sí \\\\\n    \t12 & Nublado & Moderado & Alta & Fuerte & Sí \\\\\n    \t13 & Nublado & Caluroso & Normal & Suave & Sí \\\\\n    \t14 & Lluvioso & Moderado & Alta & Fuerte & No \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Crear un diagrama de barras que muestre la distribución de frecuencias de cada variable meteorológica según si se pudo jugar al tenis o no. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=2}\n    ``` {.julia .cell-code}\n    using GLMakie, AlgebraOfGraphics\n    \n    function frecuencias(df::DataFrame, var::Symbol)\n        # Calculamos el número de días de cada clase que se juega al tenis.\n        frec = combine(groupby(df, [var, :Tenis]), nrow => :Días)\n        # Dibujamos el diagrama de barras.\n        plt = data(frec) * \n        mapping(var, :Días, stack = :Tenis, color = :Tenis, ) * \n        visual(BarPlot) \n        # Devolvemos el gráfico.\n        return plt\n    end\n    \n    fig = Figure()\n    draw!(fig[1, 1], frecuencias(df, :Cielo))\n    draw!(fig[1, 2], frecuencias(df, :Temperatura))\n    draw!(fig[1, 3], frecuencias(df, :Humedad))\n    draw!(fig[1, 4], frecuencias(df, :Viento))\n    fig\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=3}\n    ![](07-arboles-decision_files/figure-pdf/cell-3-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    A la vista de las frecuencias de cada variable, las variable `Cielo` y `Humedad` parecen ser las que más influye en la decisión de jugar al tenis.\n    :::\n\na.  Calcular la impureza del conjunto de datos utilizando el índice de Gini. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    El [índice de Gini](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) se calcula mediante la fórmula\n    \n    $$ GI = 1 - \\sum_{i=1}^{n} p_i^2 $$\n\n    donde $p_i$ es la proporción de cada clase en el conjunto de datos y $n$ es el número de clases.\n    \n    El índice de Gini toma valores entre $0$ y $1-\\frac{1}{n}$ ($0.5$ en el caso de clasificación binaria), donde $0$ indica que todas las instancias pertenecen a una sola clase (mínima impureza) y $1-\\frac{1}{n}$ indica que las instancias están distribuidas uniformemente entre todas las clases (máxima impureza).\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=3}\n    ``` {.julia .cell-code}\n    function gini(df::DataFrame, var::Symbol)\n        # Calculamos el número de ejemplos.\n        n = nrow(df)\n        # Calculamos las frecuencias absolutas de cada clase.\n        frec = combine(groupby(df, var), nrow => :ni)\n        # Calculamos la proporción de cada clase.\n        frec.p = frec.ni ./ n\n        # Calculamos el índice de Gini.\n        gini = 1 - sum(frec.p .^ 2)\n        return gini\n    end\n    \n    g0 = gini(df, :Tenis)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=4}\n    ```\n    0.4591836734693877\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  ¿Qué reducción del índice Gini se obtiene si dividimos el conjunto de ejemplos según la variable `Humedad`? ¿Y si dividimos el conjunto con respecto a la variable `Viento`?\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    La reducción del índice de Gini se calcula como la diferencia entre el índice de Gini del conjunto original y el índice de Gini del conjunto dividido.\n    \n    $$ \\Delta GI = GI_{original} - GI_{dividido} $$\n\n    donde el índice de Gini del conjunto dividido es la media ponderada de los índices de Gini de los subconjuntos resultantes de la división.\n    :::\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n    Calculamos primero la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Humedad`.\n\n\n    ::: {.cell execution_count=4}\n    ``` {.julia .cell-code}\n    using Tidier\n    # Dividimos el conjunto de ejemplos según la variable Humedad.\n    df_humedad_alta = @filter(df, Humedad == \"Alta\")\n    df_humedad_normal = @filter(df, Humedad == \"Normal\")\n    # Calculamos los tamaños de los subconjuntos de ejemplos.\n    n = nrow(df_humedad_alta), nrow(df_humedad_normal)\n    # Calculamos el índice de Gini de cada subconjunto.\n    gis = gini(df_humedad_alta, :Tenis), gini(df_humedad_normal, :Tenis)\n    # Calculamos media ponderada de los índices de Gini de los subconjuntos \n    g_humedad = sum(gis .* n) / sum(n)\n    # Calculamos la reducción del índice de Gini.\n    g0 - g_humedad\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=5}\n    ```\n    0.09183673469387743\n    ```\n    :::\n    :::\n    \n    \n    Calculamos ahora la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Viento`.\n\n\n    ::: {.cell execution_count=5}\n    ``` {.julia .cell-code}\n    # Dividimos el conjunto de ejemplos según la variable `Viento`\n    df_viento_fuerte = @filter(df, Viento == \"Fuerte\")\n    df_viento_suave = @filter(df, Viento == \"Suave\")\n    # Calculamos los tamaños de los subconjuntos de ejemplos\n    n = nrow(df_viento_fuerte), nrow(df_viento_suave)\n    # Calculamos el índice de Gini de cada subconjunto\n    gis = gini(df_viento_fuerte, :Tenis), gini(df_viento_suave, :Tenis)\n    # Calculamos media ponderada de los índices de Gini de los subconjuntos\n    g_viento = sum(gis .* n) / sum(n)\n    # Calculamos la reducción del índice de Gini\n    g0 - g_viento\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=6}\n    ```\n    0.030612244897959162\n    ```\n    :::\n    :::\n    \n    \n    Como se puede observar, la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable `Humedad` es mayor que la reducción del índice de Gini al dividir el conjunto con respecto a la variable `Viento`. Por lo tanto, la variable `Humedad` parece tener más influencia en la decisión de jugar al tenis y sería la variable que se debería elegir para dividir el conjunto de ejemplos.\n    :::\n\na.  Construir un árbol de decisión que explique si se puede jugar al tenis en función de las variables meteorológicas.\n    \n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `DecisionTreeClassifier` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/). \n\n    Los parámetros más importantes de esta función son:\n\n    - `max_depth`: Profundidad máxima del árbol. Si no se indica, el árbol crecerá hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de `min_samples_split` ejemplos.\n    - `min_samples_leaf`: Número mínimo de ejemplos en una hoja (1 por defecto).\n    - `min_samples_split`: Número mínimo de ejemplos para dividir un nodo (2 por defecto).\n    - `min_impurity_decrease`: Reducción mínima de la impureza para dividir un nodo (0 por defecto).\n    - `post-prune`: Si se indica `true`, se poda el árbol después de que se ha construido. La poda reduce el tamaño del árbol eliminando nodos que no aportan información útil.\n    - `merge_purity_threshold`: Umbral de pureza para fusionar nodos. Si se indica, se fusionan los nodos que tienen una pureza menor que este umbral.\n    - `feature_importance`: Indica la medida para calcular la importancia de las variables a la hora de dividir el conjunto de datos. Puede ser `:impurity` o `:split`. Si no se indica, se utiliza la impureza de Gini.\n    - `rng`: Indica la semilla para la generación de números aleatorios. Si no se indica, se utiliza el generador de números aleatorios por defecto.\n    :::\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=6}\n    ``` {.julia .cell-code}\n    using DecisionTree, CategoricalArrays\n    # Variables predictoras.\n    X = Matrix(select(df, Not(:Tenis)))\n    # Variable objetivo.\n    y = df.Tenis\n    # Convertir las variables categóricas a enteros.\n    X = hcat([levelcode.(categorical(X[:, j])) for j in 1:size(X, 2)]...)\n    # Convertir la variable objetivo a enteros.\n    y = levelcode.(categorical(y))\n    tree = DecisionTreeClassifier(max_depth=3)\n    fit!(tree, X, y)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=7}\n    ```\n    DecisionTreeClassifier\n    max_depth:                3\n    min_samples_leaf:         1\n    min_samples_split:        2\n    min_purity_increase:      0.0\n    pruning_purity_threshold: 1.0\n    n_subfeatures:            0\n    classes:                  [1, 2]\n    root:                     Decision Tree\n    Leaves: 6\n    Depth:  3\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Visualizar el árbol de decisión construido.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `plot_tree` del paquete [`DecisionTree.jl`](https://docs.juliahub.com/DecisionTree/).\n    :::\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=7}\n    ``` {.julia .cell-code}\n    print_tree(tree, feature_names=names(df)[1:end-1])\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Feature 3: \"Humedad\" < 2.0 ?\n    ├─ Feature 1: \"Cielo\" < 3.0 ?\n        ├─ Feature 1: \"Cielo\" < 2.0 ?\n            ├─ 2 : 1/2\n            └─ 2 : 2/2\n        └─ 1 : 3/3\n    └─ Feature 4: \"Viento\" < 2.0 ?\n        ├─ Feature 1: \"Cielo\" < 2.0 ?\n            ├─ 1 : 1/1\n            └─ 2 : 2/2\n        └─ 2 : 4/4\n    ```\n    :::\n    :::\n    \n    \n    :::\n:::\n\n:::{#exr-arboles-decision-2}\nEl conjunto de datos [pingüinos.csv]() contiene un conjunto de datos sobre tres eEspecie de pingüinos con las siguientes variables:\n\n- Especie: Especie de pingüino, comúnmente Adelie, Chinstrap o Gentoo.\n- Isla: Isla del archipiélago Palmer donde se realizó la observación.\n- Longitud_pico: Longitud del pico en mm.\n- Profundidad_pico: Profundidad del pico en mm\n- Longitud_ala: Longitud de la aleta en mm.\n- Peso: Masa corporal en gramos.\n- Sexo: Sexo\n\na.  Cargar los datos del archivo `pinguïnos.csv` en un data frame.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=8}\n    ``` {.julia .cell-code}\n    using CSV, DataFrames\n    df = CSV.read(\"datos/pingüinos.csv\", DataFrame, missingstring=\"NA\")\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=9}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& Especie & Isla & Longitud\\_pico & Profundidad\\_pico & Longitud\\_ala & Peso & Sexo\\\\\n    \t\\hline\n    \t& String15 & String15 & Float64? & Float64? & Int64? & Int64? & String7?\\\\\n    \t\\hline\n    \t1 & Adelie & Torgersen & 39.1 & 18.7 & 181 & 3750 & macho \\\\\n    \t2 & Adelie & Torgersen & 39.5 & 17.4 & 186 & 3800 & hembra \\\\\n    \t3 & Adelie & Torgersen & 40.3 & 18.0 & 195 & 3250 & hembra \\\\\n    \t4 & Adelie & Torgersen & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} & \\emph{missing} \\\\\n    \t5 & Adelie & Torgersen & 36.7 & 19.3 & 193 & 3450 & hembra \\\\\n    \t6 & Adelie & Torgersen & 39.3 & 20.6 & 190 & 3650 & macho \\\\\n    \t7 & Adelie & Torgersen & 38.9 & 17.8 & 181 & 3625 & hembra \\\\\n    \t8 & Adelie & Torgersen & 39.2 & 19.6 & 195 & 4675 & macho \\\\\n    \t9 & Adelie & Torgersen & 34.1 & 18.1 & 193 & 3475 & \\emph{missing} \\\\\n    \t10 & Adelie & Torgersen & 42.0 & 20.2 & 190 & 4250 & \\emph{missing} \\\\\n    \t11 & Adelie & Torgersen & 37.8 & 17.1 & 186 & 3300 & \\emph{missing} \\\\\n    \t12 & Adelie & Torgersen & 37.8 & 17.3 & 180 & 3700 & \\emph{missing} \\\\\n    \t13 & Adelie & Torgersen & 41.1 & 17.6 & 182 & 3200 & hembra \\\\\n    \t14 & Adelie & Torgersen & 38.6 & 21.2 & 191 & 3800 & macho \\\\\n    \t15 & Adelie & Torgersen & 34.6 & 21.1 & 198 & 4400 & macho \\\\\n    \t16 & Adelie & Torgersen & 36.6 & 17.8 & 185 & 3700 & hembra \\\\\n    \t17 & Adelie & Torgersen & 38.7 & 19.0 & 195 & 3450 & hembra \\\\\n    \t18 & Adelie & Torgersen & 42.5 & 20.7 & 197 & 4500 & macho \\\\\n    \t19 & Adelie & Torgersen & 34.4 & 18.4 & 184 & 3325 & hembra \\\\\n    \t20 & Adelie & Torgersen & 46.0 & 21.5 & 194 & 4200 & macho \\\\\n    \t21 & Adelie & Biscoe & 37.8 & 18.3 & 174 & 3400 & hembra \\\\\n    \t22 & Adelie & Biscoe & 37.7 & 18.7 & 180 & 3600 & macho \\\\\n    \t23 & Adelie & Biscoe & 35.9 & 19.2 & 189 & 3800 & hembra \\\\\n    \t24 & Adelie & Biscoe & 38.2 & 18.1 & 185 & 3950 & macho \\\\\n    \t25 & Adelie & Biscoe & 38.8 & 17.2 & 180 & 3800 & macho \\\\\n    \t26 & Adelie & Biscoe & 35.3 & 18.9 & 187 & 3800 & hembra \\\\\n    \t27 & Adelie & Biscoe & 40.6 & 18.6 & 183 & 3550 & macho \\\\\n    \t28 & Adelie & Biscoe & 40.5 & 17.9 & 187 & 3200 & hembra \\\\\n    \t29 & Adelie & Biscoe & 37.9 & 18.6 & 172 & 3150 & hembra \\\\\n    \t30 & Adelie & Biscoe & 40.5 & 18.9 & 180 & 3950 & macho \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Hacer un análisis de los datos perdidos en el data frame. \n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=9}\n    ``` {.julia .cell-code}\n    describe(df, :nmissing)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=10}\n    ```{=tex}\n    \\begin{tabular}{r|cc}\n    \t& variable & nmissing\\\\\n    \t\\hline\n    \t& Symbol & Int64\\\\\n    \t\\hline\n    \t1 & Especie & 0 \\\\\n    \t2 & Isla & 0 \\\\\n    \t3 & Longitud\\_pico & 2 \\\\\n    \t4 & Profundidad\\_pico & 2 \\\\\n    \t5 & Longitud\\_ala & 2 \\\\\n    \t6 & Peso & 2 \\\\\n    \t7 & Sexo & 11 \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Eliminar del data frame los casos con valores perdidos.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=10}\n    ``` {.julia .cell-code}\n    dropmissing!(df)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=11}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& Especie & Isla & Longitud\\_pico & Profundidad\\_pico & Longitud\\_ala & Peso & Sexo\\\\\n    \t\\hline\n    \t& String15 & String15 & Float64 & Float64 & Int64 & Int64 & String7\\\\\n    \t\\hline\n    \t1 & Adelie & Torgersen & 39.1 & 18.7 & 181 & 3750 & macho \\\\\n    \t2 & Adelie & Torgersen & 39.5 & 17.4 & 186 & 3800 & hembra \\\\\n    \t3 & Adelie & Torgersen & 40.3 & 18.0 & 195 & 3250 & hembra \\\\\n    \t4 & Adelie & Torgersen & 36.7 & 19.3 & 193 & 3450 & hembra \\\\\n    \t5 & Adelie & Torgersen & 39.3 & 20.6 & 190 & 3650 & macho \\\\\n    \t6 & Adelie & Torgersen & 38.9 & 17.8 & 181 & 3625 & hembra \\\\\n    \t7 & Adelie & Torgersen & 39.2 & 19.6 & 195 & 4675 & macho \\\\\n    \t8 & Adelie & Torgersen & 41.1 & 17.6 & 182 & 3200 & hembra \\\\\n    \t9 & Adelie & Torgersen & 38.6 & 21.2 & 191 & 3800 & macho \\\\\n    \t10 & Adelie & Torgersen & 34.6 & 21.1 & 198 & 4400 & macho \\\\\n    \t11 & Adelie & Torgersen & 36.6 & 17.8 & 185 & 3700 & hembra \\\\\n    \t12 & Adelie & Torgersen & 38.7 & 19.0 & 195 & 3450 & hembra \\\\\n    \t13 & Adelie & Torgersen & 42.5 & 20.7 & 197 & 4500 & macho \\\\\n    \t14 & Adelie & Torgersen & 34.4 & 18.4 & 184 & 3325 & hembra \\\\\n    \t15 & Adelie & Torgersen & 46.0 & 21.5 & 194 & 4200 & macho \\\\\n    \t16 & Adelie & Biscoe & 37.8 & 18.3 & 174 & 3400 & hembra \\\\\n    \t17 & Adelie & Biscoe & 37.7 & 18.7 & 180 & 3600 & macho \\\\\n    \t18 & Adelie & Biscoe & 35.9 & 19.2 & 189 & 3800 & hembra \\\\\n    \t19 & Adelie & Biscoe & 38.2 & 18.1 & 185 & 3950 & macho \\\\\n    \t20 & Adelie & Biscoe & 38.8 & 17.2 & 180 & 3800 & macho \\\\\n    \t21 & Adelie & Biscoe & 35.3 & 18.9 & 187 & 3800 & hembra \\\\\n    \t22 & Adelie & Biscoe & 40.6 & 18.6 & 183 & 3550 & macho \\\\\n    \t23 & Adelie & Biscoe & 40.5 & 17.9 & 187 & 3200 & hembra \\\\\n    \t24 & Adelie & Biscoe & 37.9 & 18.6 & 172 & 3150 & hembra \\\\\n    \t25 & Adelie & Biscoe & 40.5 & 18.9 & 180 & 3950 & macho \\\\\n    \t26 & Adelie & Dream & 39.5 & 16.7 & 178 & 3250 & hembra \\\\\n    \t27 & Adelie & Dream & 37.2 & 18.1 & 178 & 3900 & macho \\\\\n    \t28 & Adelie & Dream & 39.5 & 17.8 & 188 & 3300 & hembra \\\\\n    \t29 & Adelie & Dream & 40.9 & 18.9 & 184 & 3900 & macho \\\\\n    \t30 & Adelie & Dream & 36.4 & 17.0 & 195 & 3325 & hembra \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Crear diagramas que muestren la distribución de frecuencias de cada variable según la especie de pingüino. ¿Qué variable parece tener más influencia en la especie de pingüino?\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n    Para las variables cualitativas dibujamos diagramas de barras.\n\n\n    ::: {.cell execution_count=11}\n    ``` {.julia .cell-code}\n    using GLMakie, AlgebraOfGraphics\n    \n    frec_isla = combine(groupby(df, [:Isla, :Especie]), nrow => :Frecuencia)\n    data(frec_isla) * \n        mapping(:Isla, :Frecuencia, stack = :Especie, color =:Especie) *\n        visual(BarPlot) |> draw\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=12}\n    ![](07-arboles-decision_files/figure-pdf/cell-12-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=12}\n    ``` {.julia .cell-code}\n    frec_sexo = combine(groupby(df, [:Sexo, :Especie]), nrow => :Frecuencia)\n    data(frec_sexo) * \n        mapping(:Sexo, :Frecuencia, stack = :Especie, color =:Especie) *\n        visual(BarPlot) |> draw\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=13}\n    ![](07-arboles-decision_files/figure-pdf/cell-13-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    Para las variables cuantitativas dibujamos diagramas de cajas.\n\n\n    ::: {.cell execution_count=13}\n    ``` {.julia .cell-code}\n    function cajas(df, var, clase)\n        data(df) *\n            mapping(clase, var, color = clase) *\n            visual(BoxPlot) |> \n            draw\n    end\n    \n    cajas(df, :Longitud_pico, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=14}\n    ![](07-arboles-decision_files/figure-pdf/cell-14-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=14}\n    ``` {.julia .cell-code}\n    cajas(df, :Profundidad_pico, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=15}\n    ![](07-arboles-decision_files/figure-pdf/cell-15-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=15}\n    ``` {.julia .cell-code}\n    cajas(df, :Longitud_ala, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=16}\n    ![](07-arboles-decision_files/figure-pdf/cell-16-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n\n    ::: {.cell execution_count=16}\n    ``` {.julia .cell-code}\n    cajas(df, :Peso, :Especie)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=17}\n    ![](07-arboles-decision_files/figure-pdf/cell-17-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    :::\n\na.  ¿Cuál es la reducción de la impureza del conjunto de datos si dividimos el conjunto de datos en dos conjuntos según si la longitud del pico es mayor o menor que 44 mm?\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=17}\n    ``` {.julia .cell-code}\n    using Tidier\n    function gini(df::DataFrame, var::Symbol)\n        n = nrow(df)\n        frec = combine(groupby(df, var), nrow => :ni)\n        frec.p = frec.ni ./ n\n        gini = 1 - sum(frec.p .^ 2)\n        return gini\n    end\n    \n    function reduccion_impureza(df::DataFrame, var::Symbol, val::Number)\n        # Dividimos el conjunto de ejemplos según la longitud del pico es menor de 44.\n        df_menor = @eval @filter($df, $var <= $val)\n        df_mayor = @eval @filter($df, $var > $val)\n        # Calculamos los tamaños de los subconjuntos de ejemplos.\n        n = nrow(df_menor), nrow(df_mayor)\n        # Calculamos el índice de Gini de cada subconjunto.\n        gis = gini(df_menor, :Especie), gini(df_mayor, :Especie)\n        # Calculamos media ponderada de los índices de Gini de los subconjuntos.\n        g1 = sum(gis .* n) / sum(n)\n        # Calculamos la reducción del índice de Gini.\n        gini(df, :Especie) - g1\n    end\n    \n    reduccion_impureza(df, :Longitud_pico, 44)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=18}\n    ```\n    0.26577182779353914\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Determinar el valor óptimo de división del conjunto de datos según la longitud del pico. Para ello, calcular la reducción de la impureza para cada valor de longitud del pico y dibujar el resultado.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n    Dibujamos la reducción de la impureza en función de la longitud del pico.\n\n\n    ::: {.cell execution_count=18}\n    ``` {.julia .cell-code}\n    using Plots\n    # Valores únicos de longitud del pico.\n    valores = unique(df.Longitud_pico)\n    # Reducción de la impureza para cada valor.\n    reducciones = [reduccion_impureza(df, :Longitud_pico, val) for val in valores]\n    # Graficamos el resultado.\n    Plots.scatter(valores, reducciones, xlabel = \"Longitud del pico\", ylabel = \"Reducción de la impureza\", legend = false)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=19}\n    ![](07-arboles-decision_files/figure-pdf/cell-19-output-1.svg){fig-pos='H'}\n    :::\n    :::\n    \n    \n    Y ahora obtenemos el valor óptimo de división del conjunto de datos según la longitud del pico.\n\n\n    ::: {.cell execution_count=19}\n    ``` {.julia .cell-code}\n    val_optimo = valores[argmax(reducciones)]\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=20}\n    ```\n    42.3\n    ```\n    :::\n    :::\n    \n    \n    ::: \n\na.  Dividir aleatoriamente el dataframe en un conjunto de entrenamiento y un conjunto de test con proporciones $3/4$ y $1/4$ respectivamente.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Utilizar la función [`shuffle`](https://docs.julialang.org/en/v1/stdlib/Random/#Random.shuffle) del paquete [`Random`](https://docs.julialang.org/en/v1/stdlib/Random/) para barajar el dataframe y luego dividirlo en dos subconjuntos.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución    \n\n\n    ::: {.cell execution_count=20}\n    ``` {.julia .cell-code}\n    using Random\n    # Establecemos la semilla para la reproducibilidad.\n    Random.seed!(1234)\n    # Barajamos el dataframe.\n    df = shuffle(df)\n    # Dividimos el dataframe en un conjunto de entrenamiento y un conjunto de test.\n    n = nrow(df)\n    df_test = df[1:div(n, 4), :]\n    df_train = df[div(n, 4)+1:end, :]\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=21}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& Especie & Isla & Longitud\\_pico & Profundidad\\_pico & Longitud\\_ala & Peso & Sexo\\\\\n    \t\\hline\n    \t& String15 & String15 & Float64 & Float64 & Int64 & Int64 & String7\\\\\n    \t\\hline\n    \t1 & Adelie & Dream & 39.0 & 18.7 & 185 & 3650 & macho \\\\\n    \t2 & Chinstrap & Dream & 52.8 & 20.0 & 205 & 4550 & macho \\\\\n    \t3 & Chinstrap & Dream & 55.8 & 19.8 & 207 & 4000 & macho \\\\\n    \t4 & Adelie & Torgersen & 35.1 & 19.4 & 193 & 4200 & macho \\\\\n    \t5 & Adelie & Torgersen & 34.6 & 21.1 & 198 & 4400 & macho \\\\\n    \t6 & Gentoo & Biscoe & 50.0 & 15.2 & 218 & 5700 & macho \\\\\n    \t7 & Chinstrap & Dream & 50.6 & 19.4 & 193 & 3800 & macho \\\\\n    \t8 & Chinstrap & Dream & 43.5 & 18.1 & 202 & 3400 & hembra \\\\\n    \t9 & Adelie & Dream & 36.9 & 18.6 & 189 & 3500 & hembra \\\\\n    \t10 & Adelie & Dream & 36.6 & 18.4 & 184 & 3475 & hembra \\\\\n    \t11 & Chinstrap & Dream & 46.6 & 17.8 & 193 & 3800 & hembra \\\\\n    \t12 & Gentoo & Biscoe & 50.8 & 17.3 & 228 & 5600 & macho \\\\\n    \t13 & Chinstrap & Dream & 52.2 & 18.8 & 197 & 3450 & macho \\\\\n    \t14 & Adelie & Dream & 39.6 & 18.8 & 190 & 4600 & macho \\\\\n    \t15 & Adelie & Torgersen & 42.8 & 18.5 & 195 & 4250 & macho \\\\\n    \t16 & Adelie & Biscoe & 36.5 & 16.6 & 181 & 2850 & hembra \\\\\n    \t17 & Gentoo & Biscoe & 49.1 & 14.8 & 220 & 5150 & hembra \\\\\n    \t18 & Chinstrap & Dream & 43.2 & 16.6 & 187 & 2900 & hembra \\\\\n    \t19 & Gentoo & Biscoe & 43.3 & 13.4 & 209 & 4400 & hembra \\\\\n    \t20 & Gentoo & Biscoe & 49.5 & 16.1 & 224 & 5650 & macho \\\\\n    \t21 & Adelie & Biscoe & 37.8 & 20.0 & 190 & 4250 & macho \\\\\n    \t22 & Gentoo & Biscoe & 50.4 & 15.3 & 224 & 5550 & macho \\\\\n    \t23 & Adelie & Biscoe & 45.6 & 20.3 & 191 & 4600 & macho \\\\\n    \t24 & Chinstrap & Dream & 45.4 & 18.7 & 188 & 3525 & hembra \\\\\n    \t25 & Adelie & Dream & 39.2 & 18.6 & 190 & 4250 & macho \\\\\n    \t26 & Gentoo & Biscoe & 48.4 & 14.4 & 203 & 4625 & hembra \\\\\n    \t27 & Adelie & Torgersen & 35.2 & 15.9 & 186 & 3050 & hembra \\\\\n    \t28 & Gentoo & Biscoe & 48.4 & 16.3 & 220 & 5400 & macho \\\\\n    \t29 & Adelie & Dream & 33.1 & 16.1 & 178 & 2900 & hembra \\\\\n    \t30 & Adelie & Dream & 36.8 & 18.5 & 193 & 3500 & hembra \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Construir un árbol de decisión con el conjunto de entrenamiento sin tener en cuenta la variable `Isla` y visualizarlo.\n    \n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=21}\n    ``` {.julia .cell-code}\n    using DecisionTree, CategoricalArrays\n    # Variables predictivas.\n    X_train = Matrix(select(df_train, Not(:Isla, :Especie)))\n    # Variable objetivo.\n    y_train = df_train.Especie\n    # Convertir las variables categóricas a enteros.\n    X_train = hcat([levelcode.(categorical(X_train[:, j])) for j in 1:size(X_train, 2)]...)\n    # Convertir la variable objetivo a enteros\n    y_train = levelcode.(categorical(y_train))\n    \n    # Construimos el árbol de decisión con profundidad máxima 3.\n    tree = DecisionTreeClassifier(max_depth = 3)\n    fit!(tree, X_train, y_train)\n    print_tree(tree, feature_names=names(df)[3:end])\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    Feature 3: \"Longitud_ala\" < 29.0 ?\n    ├─ Feature 1: \"Longitud_pico\" < 62.0 ?\n        ├─ 1 : 96/96\n        └─ Feature 1: \"Longitud_pico\" < 87.0 ?\n            ├─ 2 : 10/20\n            └─ 2 : 37/38\n    └─ Feature 2: \"Profundidad_pico\" < 46.0 ?\n        ├─ 3 : 90/90\n        └─ Feature 1: \"Longitud_pico\" < 109.0 ?\n            ├─ 1 : 2/2\n            └─ 2 : 4/4\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Predecir la especie de los pingüinos del conjunto de test y calcular la matriz de confusión de las predicciones.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Utilizar la función [`confmat`](https://juliaai.github.io/StatisticalMeasures.jl/stable/confusion_matrices/#StatisticalMeasures.ConfusionMatrices.confmat) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para barajar el dataframe y luego dividirlo en dos subconjuntos.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=22}\n    ``` {.julia .cell-code}\n    using StatisticalMeasures\n    # Variables predictivas\n    X_test = Matrix(select(df_test, Not(:Isla, :Especie)))\n    # Variable objetivo\n    y_test = df_test.Especie\n    # Convertir las variables categóricas a enteros\n    X_test = hcat([levelcode.(categorical(X_test[:, j])) for j in 1:size(X_test, 2)]...)\n    # Convertir la variable objetivo a enteros\n    y_test = levelcode.(categorical(y_test))\n    # Predecimos la especie de pingüino del conjunto de test\n    y_pred = predict(tree, X_test)\n    # Calculamos la precisión del modelo\n    confmat(y_pred, y_test)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=23}\n    ```\n              ┌──────────────┐\n              │ Ground Truth │\n    ┌─────────┼────┬────┬────┤\n    │Predicted│ 1  │ 2  │ 3  │\n    ├─────────┼────┼────┼────┤\n    │    1    │ 38 │ 11 │ 9  │\n    ├─────────┼────┼────┼────┤\n    │    2    │ 0  │ 6  │ 0  │\n    ├─────────┼────┼────┼────┤\n    │    3    │ 0  │ 0  │ 19 │\n    └─────────┴────┴────┴────┘\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Calcular la precisión del modelo.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    La precisión es la proporción de predicciones correctas sobre el total de predicciones.\n\n    Utilizar la función [`accuracy`](https://juliaai.github.io/StatisticalMeasures.jl/stable/auto_generated_list_of_measures/#StatisticalMeasures.Accuracy) del paquete [`StatisticalMeaures`](https://juliaai.github.io/StatisticalMeasures.jl) para calcular la precisión del modelo.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=23}\n    ``` {.julia .cell-code}\n    # Calculamos la precisión del modelo\n    accuracy(y_pred, y_test)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=24}\n    ```\n    0.7590361445783133\n    ```\n    :::\n    :::\n    \n    \n    :::\n\n:::\n\n:::{#exr-arboles-decision-3}\nEl fichero [`vinos.csv`](datos/vinos.csv) contiene información sobre las características de una muestra de vinos portugueses de la denominación \"Vinho Verde\". Las variables que contiene son:\n\n| Variable             | Descripción                                                           | Tipo (unidades)        |\n|----------------------------------------|-----------------------------------------------------------------------|------------------------|\n| tipo                 | Tipo de vino                                                          | Categórica (blanco, tinto) |\n| meses.barrica        | Mesesde envejecimiento en barrica                               | Numérica(meses)  |\n| acided.fija          | Cantidadde ácidotartárico                                 | Numérica(g/dm3)  |\n| acided.volatil       | Cantidad de ácido acético                                             | Numérica(g/dm3)  |\n| acido.citrico        | Cantidad de ácidocítrico                                        | Numérica(g/dm3)  |\n| azucar.residual      | Cantidad de azúcarremanente después de la fermentación          | Numérica(g/dm3)  |\n| cloruro.sodico       | Cantidad de clorurosódico                                       | Numérica(g/dm3)  |\n| dioxido.azufre.libre | Cantidad de dióxido de azufreen formalibre                | Numérica(mg/dm3) |\n| dioxido.azufre.total | Cantidadde dióxido de azufretotal en forma libre o ligada | Numérica(mg/dm3) |\n| densidad             | Densidad                                                              | Numérica(g/cm3)  |\n| ph                   | pH                                                                    | Numérica(0-14)   |\n| sulfatos             | Cantidadde sulfato de potasio                                   | Numérica(g/dm3)  |\n| alcohol              | Porcentajede contenidode alcohol                          | Numérica(0-100)  |\n| calidad              | Calificación otorgada porun panel de expertos                   | Numérica(0-10)   |\n\na.  Crear un data frame con los datos de los vinos a partir del fichero [`vinos.csv`](datos/vinos.csv).\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=24}\n    ``` {.julia .cell-code}\n    using CSV, DataFrames\n    df = CSV.read(\"datos/vinos.csv\", DataFrame)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=25}\n    ```{=tex}\n    \\begin{tabular}{r|ccccccc}\n    \t& tipo & meses\\_barrica & acided\\_fija & acided\\_volatil & acido\\_citrico & azucar\\_residual & \\\\\n    \t\\hline\n    \t& String7 & Int64 & Float64 & Float64 & Float64 & Float64 & \\\\\n    \t\\hline\n    \t1 & blanco & 0 & 7.0 & 0.27 & 0.36 & 20.7 & $\\dots$ \\\\\n    \t2 & blanco & 0 & 6.3 & 0.3 & 0.34 & 1.6 & $\\dots$ \\\\\n    \t3 & blanco & 0 & 8.1 & 0.28 & 0.4 & 6.9 & $\\dots$ \\\\\n    \t4 & blanco & 0 & 7.2 & 0.23 & 0.32 & 8.5 & $\\dots$ \\\\\n    \t5 & blanco & 0 & 6.2 & 0.32 & 0.16 & 7.0 & $\\dots$ \\\\\n    \t6 & blanco & 0 & 8.1 & 0.22 & 0.43 & 1.5 & $\\dots$ \\\\\n    \t7 & blanco & 0 & 8.1 & 0.27 & 0.41 & 1.45 & $\\dots$ \\\\\n    \t8 & blanco & 0 & 8.6 & 0.23 & 0.4 & 4.2 & $\\dots$ \\\\\n    \t9 & blanco & 0 & 7.9 & 0.18 & 0.37 & 1.2 & $\\dots$ \\\\\n    \t10 & blanco & 0 & 6.6 & 0.16 & 0.4 & 1.5 & $\\dots$ \\\\\n    \t11 & blanco & 0 & 8.3 & 0.42 & 0.62 & 19.25 & $\\dots$ \\\\\n    \t12 & blanco & 0 & 6.6 & 0.17 & 0.38 & 1.5 & $\\dots$ \\\\\n    \t13 & blanco & 0 & 6.3 & 0.48 & 0.04 & 1.1 & $\\dots$ \\\\\n    \t14 & blanco & 0 & 6.2 & 0.66 & 0.48 & 1.2 & $\\dots$ \\\\\n    \t15 & blanco & 0 & 7.4 & 0.34 & 0.42 & 1.1 & $\\dots$ \\\\\n    \t16 & blanco & 0 & 6.5 & 0.31 & 0.14 & 7.5 & $\\dots$ \\\\\n    \t17 & blanco & 0 & 6.4 & 0.31 & 0.38 & 2.9 & $\\dots$ \\\\\n    \t18 & blanco & 0 & 6.8 & 0.26 & 0.42 & 1.7 & $\\dots$ \\\\\n    \t19 & blanco & 0 & 7.6 & 0.67 & 0.14 & 1.5 & $\\dots$ \\\\\n    \t20 & blanco & 0 & 6.6 & 0.27 & 0.41 & 1.3 & $\\dots$ \\\\\n    \t21 & blanco & 0 & 7.0 & 0.25 & 0.32 & 9.0 & $\\dots$ \\\\\n    \t22 & blanco & 0 & 6.9 & 0.24 & 0.35 & 1.0 & $\\dots$ \\\\\n    \t23 & blanco & 0 & 7.0 & 0.28 & 0.39 & 8.7 & $\\dots$ \\\\\n    \t24 & blanco & 0 & 7.4 & 0.27 & 0.48 & 1.1 & $\\dots$ \\\\\n    \t25 & blanco & 0 & 7.2 & 0.32 & 0.36 & 2.0 & $\\dots$ \\\\\n    \t26 & blanco & 0 & 8.5 & 0.24 & 0.39 & 10.4 & $\\dots$ \\\\\n    \t27 & blanco & 0 & 8.3 & 0.14 & 0.34 & 1.1 & $\\dots$ \\\\\n    \t28 & blanco & 0 & 7.4 & 0.25 & 0.36 & 2.05 & $\\dots$ \\\\\n    \t29 & blanco & 0 & 6.2 & 0.12 & 0.34 & 1.5 & $\\dots$ \\\\\n    \t30 & blanco & 0 & 5.8 & 0.27 & 0.2 & 14.95 & $\\dots$ \\\\\n    \t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Mostrar los tipos de cada variable del data frame.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `schema` del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/).\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=25}\n    ``` {.julia .cell-code}\n    using MLJ\n    schema(df)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    WARNING: using MLJ.fit! in module Main conflicts with an existing identifier.\n    WARNING: using MLJ.predict in module Main conflicts with an existing identifier.\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=26}\n    ```\n    ┌──────────────────────┬────────────┬─────────┐\n    │ names                │ scitypes   │ types   │\n    ├──────────────────────┼────────────┼─────────┤\n    │ tipo                 │ Textual    │ String7 │\n    │ meses_barrica        │ Count      │ Int64   │\n    │ acided_fija          │ Continuous │ Float64 │\n    │ acided_volatil       │ Continuous │ Float64 │\n    │ acido_citrico        │ Continuous │ Float64 │\n    │ azucar_residual      │ Continuous │ Float64 │\n    │ cloruro_sodico       │ Continuous │ Float64 │\n    │ dioxido_azufre_libre │ Continuous │ Float64 │\n    │ dioxido_azufre_total │ Continuous │ Float64 │\n    │ densidad             │ Continuous │ Float64 │\n    │ ph                   │ Continuous │ Float64 │\n    │ sulfatos             │ Continuous │ Float64 │\n    │ alcohol              │ Continuous │ Float64 │\n    │ calidad              │ Count      │ Int64   │\n    └──────────────────────┴────────────┴─────────┘\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Hacer un análisis de los datos perdidos en el data frame.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=26}\n    ``` {.julia .cell-code}\n    describe(df, :nmissing)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=27}\n    ```{=tex}\n    \\begin{tabular}{r|cc}\n    \t& variable & nmissing\\\\\n    \t\\hline\n    \t& Symbol & Int64\\\\\n    \t\\hline\n    \t1 & tipo & 0 \\\\\n    \t2 & meses\\_barrica & 0 \\\\\n    \t3 & acided\\_fija & 0 \\\\\n    \t4 & acided\\_volatil & 0 \\\\\n    \t5 & acido\\_citrico & 0 \\\\\n    \t6 & azucar\\_residual & 0 \\\\\n    \t7 & cloruro\\_sodico & 0 \\\\\n    \t8 & dioxido\\_azufre\\_libre & 0 \\\\\n    \t9 & dioxido\\_azufre\\_total & 0 \\\\\n    \t10 & densidad & 0 \\\\\n    \t11 & ph & 0 \\\\\n    \t12 & sulfatos & 0 \\\\\n    \t13 & alcohol & 0 \\\\\n    \t14 & calidad & 0 \\\\\n    \\end{tabular}\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Mostrar la distribución de frecuencias de las variables cuantitativas del data frame mediante histogramas.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=27}\n    ``` {.julia .cell-code}\n    using GLMakie\n    fig = Figure() \n    ax = [Axis(fig[trunc(Int, i / 3), i % 3], title = names(df)[i+2]) for i in 0:12]\n    for i in 1:13\n        hist!(ax[i], df[!, i+1], strokewidth = 0.5, strokecolor = (:white, 0.5))\n    end\n    fig\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=28}\n    ![](07-arboles-decision_files/figure-pdf/cell-28-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    :::\n\na.  Se considera que un vino es bueno si tiene una puntuación de calidad mayor que $6.5$. Recodificar la variable `calidad` en una variable categórica que tome el valor 1 si la calidad es mayor que $6.5$ y 0 en caso contrario.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=28}\n    ``` {.julia .cell-code}\n    using CategoricalArrays\n    # Recodificamos la variable calidad.\n    df.calidad = cut(df.calidad, [0, 6.5, 10], labels = [\" ☹️ \", \" 😊 \"])\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=29}\n    ```\n    5320-element CategoricalArray{String,1,UInt32}:\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" 😊 \"\n     \" ☹️ \"\n     \" 😊 \"\n     \" ☹️ \"\n     ⋮\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Descomponer el data frame en un data frame con las variables predictivas y un vector con la variable objetivo `bueno`.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=29}\n    ``` {.julia .cell-code}\n    y, X = unpack(df, ==(:calidad), rng = 123)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=30}\n    ```\n    (CategoricalValue{String, UInt32}[\" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" 😊 \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \"  …  \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \"], 5320×13 DataFrame\n      Row │ tipo     meses_barrica  acided_fija  acided_volatil  acido_citrico  az ⋯\n          │ String7  Int64          Float64      Float64         Float64        Fl ⋯\n    ──────┼─────────────────────────────────────────────────────────────────────────\n        1 │ blanco               0          6.7           0.5             0.36     ⋯\n        2 │ blanco               0          6.3           0.2             0.3\n        3 │ blanco               0          6.2           0.35            0.03\n        4 │ tinto                3          8.0           0.39            0.3\n        5 │ blanco               0          7.9           0.255           0.26     ⋯\n        6 │ blanco               0          6.1           0.31            0.37\n        7 │ blanco               0          6.8           0.28            0.36\n        8 │ blanco               0          8.2           0.34            0.49\n        9 │ tinto                0          6.7           0.48            0.02     ⋯\n       10 │ blanco               0          7.4           0.35            0.2\n       11 │ tinto                5          7.5           0.53            0.06\n      ⋮   │    ⋮           ⋮             ⋮             ⋮               ⋮           ⋱\n     5311 │ blanco               0          7.2           0.14            0.35\n     5312 │ tinto                3          7.6           0.41            0.24     ⋯\n     5313 │ tinto                0          7.3           0.4             0.3\n     5314 │ tinto                4          7.1           0.48            0.28\n     5315 │ blanco               0          6.4           0.29            0.2\n     5316 │ blanco               0          9.4           0.24            0.29     ⋯\n     5317 │ blanco               0          6.3           0.25            0.27\n     5318 │ blanco               0          5.5           0.16            0.26\n     5319 │ blanco               0          7.4           0.36            0.32\n     5320 │ blanco               0          7.6           0.51            0.24     ⋯\n                                                     8 columns and 5299 rows omitted)\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Para poder entrenar un modelo de un arbol de decisión, las variables predictivas deben ser cuantitativas. Transmformar las variables categóricas en variables numéricas.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=30}\n    ``` {.julia .cell-code}\n    # Convertir las variables categóricas a enteros.\n    coerce!(X, :tipo => OrderedFactor, :meses_barrica => Continuous)\n    schema(X)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=31}\n    ```\n    ┌──────────────────────┬──────────────────┬───────────────────────────────────┐\n    │ names                │ scitypes         │ types                             │\n    ├──────────────────────┼──────────────────┼───────────────────────────────────┤\n    │ tipo                 │ OrderedFactor{2} │ CategoricalValue{String7, UInt32} │\n    │ meses_barrica        │ Continuous       │ Float64                           │\n    │ acided_fija          │ Continuous       │ Float64                           │\n    │ acided_volatil       │ Continuous       │ Float64                           │\n    │ acido_citrico        │ Continuous       │ Float64                           │\n    │ azucar_residual      │ Continuous       │ Float64                           │\n    │ cloruro_sodico       │ Continuous       │ Float64                           │\n    │ dioxido_azufre_libre │ Continuous       │ Float64                           │\n    │ dioxido_azufre_total │ Continuous       │ Float64                           │\n    │ densidad             │ Continuous       │ Float64                           │\n    │ ph                   │ Continuous       │ Float64                           │\n    │ sulfatos             │ Continuous       │ Float64                           │\n    │ alcohol              │ Continuous       │ Float64                           │\n    └──────────────────────┴──────────────────┴───────────────────────────────────┘\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Definir un modelo de árbol de decisión con profundidad máxima 3.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Cargar el modelo `DecisionTreeClassifier` del paquete [`DecisionTree`](https://docs.juliahub.com/DecisionTree/) con la macros `@iload`.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=31}\n    ``` {.julia .cell-code}\n    # Cargamos el tipo de modelo.\n    Tree = @iload DecisionTreeClassifier pkg = \"DecisionTree\"\n    # Instanciamos el modelo con sus parámetros.\n    arbol = Tree(max_depth =3, rng = 123)\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    [ Info: For silent loading, specify `verbosity=0`. \n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-stdout}\n    ```\n    import MLJDecisionTreeInterface ✔\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=32}\n    ```\n    DecisionTreeClassifier(\n      max_depth = 3, \n      min_samples_leaf = 1, \n      min_samples_split = 2, \n      min_purity_increase = 0.0, \n      n_subfeatures = 0, \n      post_prune = false, \n      merge_purity_threshold = 1.0, \n      display_depth = 5, \n      feature_importance = :impurity, \n      rng = 123)\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Evaluar el modelo tomando un 70% de ejemplos en el conjunto de entrenamiento y un 30% en el conjunto de test. Utilizar como métrica la precisión.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función [`evaluate`](https://juliaai.github.io/MLJ.jl/stable/evaluating_model_performance/#MLJBase.evaluate!) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para evaluar el modelo. Los parámetros más importantes de esta función son:\n\n    - `resampling`: Indica el método de muestreo para definir los conjuntos de entrenamiento y test. Los métodos más habituales son:\n        - `Holdout(fraction_train = p)`: Divide el conjunto de datos tomando una proporción de $p$ ejemplos en el conjunto de entrenamiento y $1-p$ en el conjunto de test.\n        - `CV(nfolds = n, shuffle = true|false)`: Utiliza validación cruzada con `n` iteraciones. Si se indica `shuffle = true`, se utiliza validación cruzada aleatoria.\n        - `StratifiedCV(nfolds = n, shuffle = true|false)`: Utiliza validación cruzada estratificada con `n` iteraciones. Si se indica `shuffle = true`, se utiliza validación cruzada estratificada aleatoria.\n        - `InSample()`: Utiliza el conjunto de entrenamiento como conjunto de test.\n  \n    - `measures`: Indica las métricas a utilizar para evaluar el modelo. Las métricas más habituales son:\n        - `cross_entropy`: Pérdida de entropía cruzada.\n        - `confusion_matrix`: Matriz de confusión.\n        - `true_positive_rate`: Tasa de verdaderos positivos.\n        - `true_negative_rate`: Tasa de verdaderos negativos.\n        - `ppv`: Valor predictivo positivo.\n        - `npv`: Valor predictivo negativo.\n        - `accuracy`: Precisión.\n    \n        Se puede indicar más de una en un vector.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=32}\n    ``` {.julia .cell-code}\n    evaluate(arbol, X, y, resampling = Holdout(fraction_train = 0.7, rng = 123), measures = accuracy)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=33}\n    ```\n    PerformanceEvaluation object with these fields:\n      model, measure, operation,\n      measurement, per_fold, per_observation,\n      fitted_params_per_fold, report_per_fold,\n      train_test_rows, resampling, repeats\n    Extract:\n    ┌────────────┬──────────────┬─────────────┐\n    │ measure    │ operation    │ measurement │\n    ├────────────┼──────────────┼─────────────┤\n    │ Accuracy() │ predict_mode │ 0.843       │\n    └────────────┴──────────────┴─────────────┘\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Evaluar el modelo mediante validación cruzada estratificada usando las métricas de la pérdida de entropía cruzada, la matriz de confusión, la tasa de verdaderos positivos, la tasa de verdaderos negativos, el valor predictivo positivo, el valor predictivo negativo y la precisión. ¿Es un buen modelo?\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=33}\n    ``` {.julia .cell-code}\n    evaluate(arbol, X, y, resampling = StratifiedCV(rng = 123), measures = [cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy])\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    \rEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:02\rEvaluating over 6 folds: 100%[=========================] Time: 0:00:01\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=34}\n    ```\n    PerformanceEvaluation object with these fields:\n      model, measure, operation,\n      measurement, per_fold, per_observation,\n      fitted_params_per_fold, report_per_fold,\n      train_test_rows, resampling, repeats\n    Extract:\n    ┌───┬──────────────────────────┬──────────────┬─────────────────────────────────\n    │   │ measure                  │ operation    │ measurement                    ⋯\n    ├───┼──────────────────────────┼──────────────┼─────────────────────────────────\n    │ A │ LogLoss(                 │ predict      │ 0.375                          ⋯\n    │   │   tol = 2.22045e-16)     │              │                                ⋯\n    │ B │ ConfusionMatrix(         │ predict_mode │ ConfusionMatrix{2}([3821534 78 ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   perm = nothing,        │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ C │ TruePositiveRate(        │ predict_mode │ 0.128                          ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ D │ TrueNegativeRate(        │ predict_mode │ 1.0                            ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ E │ PositivePredictiveValue( │ predict_mode │ 0.994                          ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │   │   checks = true)         │              │                                ⋯\n    │ F │ NegativePredictiveValue( │ predict_mode │ 0.83                           ⋯\n    │   │   levels = nothing,      │              │                                ⋯\n    │   │   rev = nothing,         │              │                                ⋯\n    │ ⋮ │            ⋮             │      ⋮       │                        ⋮       ⋱\n    └───┴──────────────────────────┴──────────────┴─────────────────────────────────\n                                                         1 column and 2 rows omitted\n    ┌───┬───────────────────────────────────────────────────────────────────────────\n    │   │ per_fold                                                                 ⋯\n    ├───┼───────────────────────────────────────────────────────────────────────────\n    │ A │ [0.391, 0.394, 0.35, 0.358, 0.365, 0.391]                                ⋯\n    │ B │ ConfusionMatrix{2, true, CategoricalValue{String, UInt32}}[ConfusionMatr ⋯\n    │ C │ [0.125, 0.167, 0.155, 0.112, 0.113, 0.0952]                              ⋯\n    │ D │ [1.0, 0.999, 1.0, 1.0, 1.0, 1.0]                                         ⋯\n    │ E │ [1.0, 0.966, 1.0, 1.0, 1.0, 1.0]                                         ⋯\n    │ F │ [0.83, 0.837, 0.835, 0.827, 0.828, 0.825]                                ⋯\n    │ G │ [0.834, 0.841, 0.84, 0.831, 0.832, 0.828]                                ⋯\n    └───┴───────────────────────────────────────────────────────────────────────────\n                                                                   2 columns omitted\n    ```\n    :::\n    :::\n    \n    \n    La precisión del modelo es de $0.834$ que no está mal, pero si consdieramos la tasa de verdadero positivos, que es $0.13$ y la tasa de verdaderos negativos, que es prácticamente 1, el modelo tiene un buen rendimiento en la clasificación de los vinos malos, pero un mal rendimiento en la clasificación de los vinos buenos. Por lo tanto, no podemos decir que sea un buen modelo.\n    :::\n\na.  Construir árboles de decisión con profundidades máximas de 2 a 10 y evaluar el modelo con validación cruzada estratificada. ¿Cuál es la profundidad máxima que da mejor resultado?\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función [`TunedModel`](https://juliaai.github.io/MLJ.jl/stable/tuning_models/#MLJTuning.TunedModel) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para ajustar los parámetros del modelo.\n\n    Los parámetros más importantes de esta función son:\n    - `model`: Indica el modelo a ajustar.\n    - `resampling`: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.\n    - `tuning`: Indica el método de ajuste de los parámetros del modelo. Los métodos más habituales son:\n        - `Grid(resolution = n)`: Ajusta los parámetros del modelo utilizando una cuadrícula de búsqueda con `n` valores.\n        - `RandomSearch(resolution = n)`: Ajusta los parámetros del modelo utilizando una búsqueda aleatoria con `n` valores.\n    - range: Indica el rango de valores a utilizar para ajustar los parámetros del modelo. Se puede indicar un rango de valores o un vector de valores.\n    - `measure`: Indica la métrica a utilizar para evaluar el modelo.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=34}\n    ``` {.julia .cell-code}\n    # Instanciamos el modelo de árbol de decisión.\n    arbol = Tree()\n    # Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.\n    r = range(arbol, :max_depth, lower=2, upper=10)\n    # Ajustamos los parámetros del modelo utilizando una cuadrícula de búsqueda con 9 valores.\n    arbol_parametrizado = TunedModel(\n        model = arbol,\n        resampling = StratifiedCV(rng = 123),\n        tuning = Grid(resolution = 9),\n        range = r,\n        measure = accuracy)\n    # Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\n    mach = machine(arbol_parametrizado, X, y)\n    # Ajustamos los parámetros del modelo.\n    MLJ.fit!(mach)\n    # Mostramos los parámetros del mejor modelo.\n    fitted_params(mach).best_model\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    [ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).\n    [ Info: Attempting to evaluate 9 models.\n    \rEvaluating over 9 metamodels:  22%[=====>                   ]  ETA: 0:00:03\rEvaluating over 9 metamodels:  33%[========>                ]  ETA: 0:00:02\rEvaluating over 9 metamodels:  44%[===========>             ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  56%[=============>           ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  67%[================>        ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  78%[===================>     ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  89%[======================>  ]  ETA: 0:00:00\rEvaluating over 9 metamodels: 100%[=========================] Time: 0:00:02\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=35}\n    ```\n    DecisionTreeClassifier(\n      max_depth = 5, \n      min_samples_leaf = 1, \n      min_samples_split = 2, \n      min_purity_increase = 0.0, \n      n_subfeatures = 0, \n      post_prune = false, \n      merge_purity_threshold = 1.0, \n      display_depth = 5, \n      feature_importance = :impurity, \n      rng = TaskLocalRNG())\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Dibujar la curva de aprendizaje del modelo en función de la profundidad del árbol de decisión.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función [`learning_curve`](https://juliaai.github.io/MLJ.jl/stable/learning_curves/#MLJBase.learning_curve) del paquete [`MLJ`](https://juliaai.github.io/MLJ.jl/) para dibujar la curva de aprendizaje.\n    Los parámetros más importantes de esta función son:\n    - `mach`: Indica la máquina de aprendizaje a utilizar.\n    - `range`: Indica el rango de valores a utilizar para ajustar los parámetros del modelo.\n    - `resampling`: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.\n    - `measure`: Indica la métrica a utilizar para evaluar el modelo.\n    - `rngs`: Indica la semilla para la generación de números aleatorios. Se pueden indicar varias semillas en un vector y se genera una curva de aprendizaje para cada semilla.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=35}\n    ``` {.julia .cell-code}\n    # Instanciamos el modelo de árbol de decisión.\n    arbol = Tree()\n    # Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\n    mach = machine(arbol, X, y)\n    # Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.\n    r = range(arbol, :max_depth, lower=2, upper=10)\n    # Dibujamos la curva de aprendizaje.\n    curva = learning_curve(mach, range = r, resampling = StratifiedCV(rng = 123), measure = accuracy)\n    # Dibujamos la curva de aprendizaje.\n    fig = Figure()\n    ax = Axis(fig[1, 1], title = \"Curva de aprendizaje\", xlabel = \"Profundidad del árbol\", ylabel = \"Precisión\")\n    Makie.scatter!(ax, curva.parameter_values, curva.measurements)\n    fig\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    [ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).\n    [ Info: Attempting to evaluate 9 models.\n    \rEvaluating over 9 metamodels:  22%[=====>                   ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  33%[========>                ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  44%[===========>             ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  56%[=============>           ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  67%[================>        ]  ETA: 0:00:01\rEvaluating over 9 metamodels:  78%[===================>     ]  ETA: 0:00:00\rEvaluating over 9 metamodels:  89%[======================>  ]  ETA: 0:00:00\rEvaluating over 9 metamodels: 100%[=========================] Time: 0:00:01\n    ┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n    └ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=36}\n    ![](07-arboles-decision_files/figure-pdf/cell-36-output-2.png){fig-pos='H'}\n    :::\n    :::\n    \n    \n    :::\n\na.  Construir un árbol de decisión con la profundidad máxima que da mejor resultado y visualizarlo.\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=36}\n    ``` {.julia .cell-code}\n    # Instanciamos el modelo de árbol de decisión.\n    arbol = Tree(max_depth = 4)\n    # Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\n    mach = machine(arbol, X, y)\n    # Ajustamos los parámetros del modelo.\n    MLJ.fit!(mach)\n    # Visualizamos el árbol de decisión.\n    fitted_params(mach).tree\n    ```\n    \n    ::: {.cell-output .cell-output-stderr}\n    ```\n    [ Info: Training machine(DecisionTreeClassifier(max_depth = 4, …), …).\n    ```\n    :::\n    \n    ::: {.cell-output .cell-output-display execution_count=37}\n    ```\n    alcohol < 10.62\n    ├─ meses_barrica < 8.5\n    │  ├─ acided_volatil < 0.3125\n    │  │  ├─ acided_volatil < 0.2025\n    │  │  │  ├─  ☹️  (408/496)\n    │  │  │  └─  ☹️  (1095/1172)\n    │  │  └─ meses_barrica < 5.5\n    │  │     ├─  ☹️  (1334/1345)\n    │  │     └─  ☹️  (51/58)\n    │  └─  😊  (25/25)\n    └─ meses_barrica < 12.5\n       ├─ cloruro_sodico < 0.0455\n       │  ├─ alcohol < 12.55\n       │  │  ├─  ☹️  (751/1160)\n       │  │  └─  😊  (185/286)\n       │  └─ meses_barrica < 10.5\n       │     ├─  ☹️  (552/629)\n       │     └─  😊  (25/43)\n       └─ alcohol < 14.45\n          ├─  😊  (105/105)\n          └─  ☹️  (1/1)\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  ¿Cuál es la importancia de cada variable en el modelo?\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `feature_importances` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para calcular la importancia de cada variable.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n\n    ::: {.cell execution_count=37}\n    ``` {.julia .cell-code}\n    # Calculamos la importancia de cada variable.\n    feature_importances(mach)\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=38}\n    ```\n    13-element Vector{Pair{Symbol, Float64}}:\n                  :alcohol => 0.5303315899204789\n            :meses_barrica => 0.26854115615561525\n           :acided_volatil => 0.1040970236546446\n           :cloruro_sodico => 0.09703023026926123\n                     :tipo => 0.0\n              :acided_fija => 0.0\n            :acido_citrico => 0.0\n          :azucar_residual => 0.0\n     :dioxido_azufre_libre => 0.0\n     :dioxido_azufre_total => 0.0\n                 :densidad => 0.0\n                       :ph => 0.0\n                 :sulfatos => 0.0\n    ```\n    :::\n    :::\n    \n    \n    :::\n\na.  Predecir la calidad de los 10 primeros vinos del conjunto de ejemplos.\n\n    :::{.callout-note collapse=\"true\"}\n    ## Ayuda\n    Usar la función `predict` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para predecir las probabilidades de pertenecer a cada clase un ejemplo o conjunto de ejemplos.\n\n    Usar la función `predict_mode` del paquete [`DecisionTree`](https://juliaai.github.io/DecisionTree.jl/) para predecir la clase de un ejemplo o conjunto de ejemplos.\n    :::\n\n    :::{.callout-tip collapse=\"true\"}\n    ## Solución\n\n    Primero calculamos las probabilidades de cada clase.\n\n\n    ::: {.cell execution_count=38}\n    ``` {.julia .cell-code}\n    MLJ.predict(mach, X[1:10, :])\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=39}\n    ```\n    10-element CategoricalDistributions.UnivariateFiniteVector{OrderedFactor{2}, String, UInt32, Float64}:\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.992,  😊 =>0.00818)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.823,  😊 =>0.177)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.992,  😊 =>0.00818)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.992,  😊 =>0.00818)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.647,  😊 =>0.353)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.647,  😊 =>0.353)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.647,  😊 =>0.353)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.878,  😊 =>0.122)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.992,  😊 =>0.00818)\n     UnivariateFinite{OrderedFactor{2}}( ☹️ =>0.992,  😊 =>0.00818)\n    ```\n    :::\n    :::\n    \n    \n    Y ahora predecimos la clase.\n\n\n    ::: {.cell execution_count=39}\n    ``` {.julia .cell-code}\n    predict_mode(mach, X[1:10, :])\n    ```\n    \n    ::: {.cell-output .cell-output-display execution_count=40}\n    ```\n    10-element CategoricalArray{String,1,UInt32}:\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n     \" ☹️ \"\n    ```\n    :::\n    :::\n    \n    \n    :::\n:::\n\n",
    "supporting": [
      "07-arboles-decision_files/figure-pdf"
    ],
    "filters": []
  }
}