[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prácticas de Aprendizaje Automático con Julia",
    "section": "",
    "text": "Prefacio\n¡Bienvenido a Prácticas de Aprendizaje Automático con Julia!\nEste libro presenta una recopilación de prácticas de Aprendizaje Automático (Machine Learning) con el lenguaje de programación Julia.\nNo es un libro para aprender a programar con Julia, ya que solo enseña el uso del lenguaje y de algunos de sus paquetes para implementar los algoritmos más comunes de Aprendizaje Automático. Para quienes estén interesados en aprender a programar en este Julia, os recomiendo leer este manual de Julia.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Prácticas de Aprendizaje Automático con Julia",
    "section": "Licencia",
    "text": "Licencia\nEsta obra está bajo una licencia Reconocimiento – No comercial – Compartir bajo la misma licencia 3.0 España de Creative Commons. Para ver una copia de esta licencia, visite https://creativecommons.org/licenses/by-nc-sa/3.0/es/.\nCon esta licencia eres libre de:\n\nCopiar, distribuir y mostrar este trabajo.\nRealizar modificaciones de este trabajo.\n\nBajo las siguientes condiciones:\n\nReconocimiento. Debe reconocer los créditos de la obra de la manera especificada por el autor o el licenciador (pero no de una manera que sugiera que tiene su apoyo o apoyan el uso que hace de su obra).\nNo comercial. No puede utilizar esta obra para fines comerciales.\nCompartir bajo la misma licencia. Si altera o transforma esta obra, o genera una obra derivada, sólo puede distribuir la obra generada bajo una licencia idéntica a ésta.\n\nAl reutilizar o distribuir la obra, tiene que dejar bien claro los términos de la licencia de esta obra.\nEstas condiciones pueden no aplicarse si se obtiene el permiso del titular de los derechos de autor.\nNada en esta licencia menoscaba o restringe los derechos morales del autor.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 El REPL de Julia\nLa gran potencia de cálculo alcanzada por los ordenadores en las últimas décadas ha convertido a los mismos en poderosas herramientas al servicio de todas aquellas disciplinas que, como las matemáticas, requieren cálculos largos y complejos.\nJulia es un lenguaje de programación especialmente orientado al cálculo numérico y el análisis de datos. Julia permite además realizar cálculos simbólicos y dispone de una gran biblioteca de paquetes con aplicaciones en muy diversas áreas de las Matemáticas como Cálculo, Álgebra, Geometría, Matemática Discreta o Estadística.\nLa ventaja de Julia frente a otros programas habituales de cálculo como Mathematica, MATLAB o Sage radica en su potencia de cálculo y su velocidad (equiparable al lenguaje C), lo que lo hace ideal para manejar grandes volúmenes de datos o realizar tareas que requieran largos y complejos cálculos. Además, es software libre por lo que resulta ideal para introducirlo en el aula como soporte computacional para los modelos matemáticos sin coste alguno.\nEn el siguiente enlace se explica el procedimiento de instalación de Julia.\nExisten también varios entornos de desarrollo online que permiten ejecutar código en Julia sin necesidad de instalarlo en nuestro ordenador, como por ejemplo Replit, Cocalc o Codeanywhere.\nEl objetivo de esta práctica es introducir al alumno en la utilización de este lenguaje, enseñándole a realizar las operaciones básicas más habituales en Cálculo.\nPara arrancar el REPL^(REPL es el acrónimo de Read, Evaluate, Print and Loop, que describe el funcionamiento del compilador de Julia) de julia basta con abrir una terminal y teclear julia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#el-repl-de-julia",
    "href": "01-introduccion.html#el-repl-de-julia",
    "title": "1  Introducción",
    "section": "",
    "text": "prompt&gt; julia\n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.7.3 (2022-05-06)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia&gt;",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#el-gestor-de-paquetes-de-julia",
    "href": "01-introduccion.html#el-gestor-de-paquetes-de-julia",
    "title": "1  Introducción",
    "section": "1.2 El gestor de paquetes de Julia",
    "text": "1.2 El gestor de paquetes de Julia\nJulia viene con varios paquetes básicos preinstalados, como por ejemplo el paquete LinearAlgebra que define funciones básicas del Álgebra Lineal, pero en estas prácticas utilizaremos otros muchos paquetes que añaden más funcionalidades que no vienen instalados por defecto y tendremos que instalarlos aparte. Julia tiene un potente gestor de paquetes que facilita la búsqueda, instalación, actualización y eliminación de paquetes.\nPor defecto el gestor de paquetes utiliza el repositorio de paquetes oficial pero se pueden instalar paquetes de otros repositorios.\nPara entrar en el modo de gestión de paquetes hay que teclear ]. Esto produce un cambio en el prompt del REPL de Julia.\nLos comandos más habituales son:\n\nadd p: Instala el paquete p en el entorno activo de Julia.\nupdate: Actualiza los paquetes del entorno activo de Julia.\nstatus: Muestra los paquetes instalados y sus versiones en el entorno activo de Julia.\nremove p: Elimina el paquete p del entorno activo de Julia.\n\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\nPara instalar el paquete SymPy para cálculo simbólico basta con teclear add Sympy.\n(@v1.7) pkg&gt; add SymPy\n    Updating registry at `~/.julia/registries/General.toml`\n   Resolving package versions...\n    Updating `~/.julia/environments/v1.7/Project.toml`\n  [24249f21] + SymPy v1.1.6\n    Updating `~/.julia/environments/v1.7/Manifest.toml`\n  [3709ef60] + CommonEq v0.2.0\n  [38540f10] + CommonSolve v0.2.1\n  [438e738f] + PyCall v1.93.1\n  [24249f21] + SymPy v1.1.6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-aritméticos.",
    "href": "01-introduccion.html#operadores-aritméticos.",
    "title": "1  Introducción",
    "section": "1.3 Operadores aritméticos.",
    "text": "1.3 Operadores aritméticos.\nEl uso más simple de Julia es la realización de operaciones aritméticas como en una calculadora. En Julia se utilizan los siguientes operadores.\n\n\n\nOperador\nDescripción\n\n\n\n\nx + y\nSuma\n\n\nx - y\nResta\n\n\nx * y\nProducto\n\n\nx / y\nDivisión\n\n\nx ÷ y\nCociente división entera\n\n\nx % y\nResto división entera\n\n\nx ^ y\nPotencia",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-de-comparación",
    "href": "01-introduccion.html#operadores-de-comparación",
    "title": "1  Introducción",
    "section": "1.4 Operadores de comparación",
    "text": "1.4 Operadores de comparación\n\n\n\nOperador\nDescripción\n\n\n\n\n==\nIgualdad\n\n\n!=, ≠\nDesigualdad\n\n\n&lt;\nMenor que\n\n\n&lt;=, ≤\nMenor o igual que\n\n\n&gt;\nMayor que\n\n\n&gt;=, ≥\nMayor o igual que",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-booleanos",
    "href": "01-introduccion.html#operadores-booleanos",
    "title": "1  Introducción",
    "section": "1.5 Operadores booleanos",
    "text": "1.5 Operadores booleanos\n\n\n\nOperador\nDescripción\n\n\n\n\n!x\nNegación\n\n\nx && y\nConjunción (y)\n\n\nx || y\nDisyunción (o)\n\n\n\nExisten también un montón de funciones predefinidas habituales en Cálculo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-de-redondeo",
    "href": "01-introduccion.html#funciones-de-redondeo",
    "title": "1  Introducción",
    "section": "1.6 Funciones de redondeo",
    "text": "1.6 Funciones de redondeo\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nround(x)\nDevuelve el entero más próximo a x\n\n\nround(x, digits = n)\nDevuelve al valor más próximo a x con n decimales\n\n\nfloor(x)\nRedondea x al próximo entero menor\n\n\nceil(x)\nRedondea x al próximo entero mayor\n\n\ntrunc(x)\nDevuelve la parte entera de x\n\n\n\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\njulia&gt; round(2.7)\n3.0\n\njulia&gt; floor(2.7)\n2.0\n\njulia&gt; floor(-2.7)\n-3.0\n\njulia&gt; ceil(2.7)\n3.0\n\njulia&gt; ceil(-2.7)\n-2.0\n\njulia&gt; trunc(2.7)\n2.0\n\njulia&gt; trunc(-2.7)\n-2.0\n\njulia&gt; round(2.5)\n2.0\n\njulia&gt; round(2.786, digits = 2)\n2.79",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-de-división",
    "href": "01-introduccion.html#funciones-de-división",
    "title": "1  Introducción",
    "section": "1.7 Funciones de división",
    "text": "1.7 Funciones de división\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\ndiv(x,y), x÷y\nCociente de la división entera\n\n\nfld(x,y)\nCociente de la división entera redondeado hacia abajo\n\n\ncld(x,y)\nCociente de la división entera redondeado hacia arriba\n\n\nrem(x,y), x%y\nResto de la división entera. Se cumple x == div(x,y)*y + rem(x,y)\n\n\nmod(x,y)\nMódulo con respecto a y. Se cumple x == fld(x,y)*y + mod(x,y)\n\n\ngcd(x,y...)\nMáximo común divisor positivo de x, y,…\n\n\nlcm(x,y...)\nMínimo común múltiplo positivo de x, y,…\n\n\n\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\njulia&gt; div(5,3)\n1\n\njulia&gt; cld(5,3)\n2\n\njulia&gt; 5%3\n2\n\njulia&gt; -5%3\n-2\n\njulia&gt; mod(5,3)\n2\n\njulia&gt; mod(-5,3)\n1\n\njulia&gt; gcd(12,18)\n6\n\njulia&gt; lcm(12,18)\n36",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-para-el-signo-y-el-valor-absoluto",
    "href": "01-introduccion.html#funciones-para-el-signo-y-el-valor-absoluto",
    "title": "1  Introducción",
    "section": "1.8 Funciones para el signo y el valor absoluto",
    "text": "1.8 Funciones para el signo y el valor absoluto\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nabs(x)\nValor absoluto de x\n\n\nsign(x)\nDevuelve -1 si x es positivo, -1 si es negativo y 0 si es 0.\n\n\n\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\njulia&gt; abs(2.5)\n2.5\n\njulia&gt; abs(-2.5)\n2.5\n\njulia&gt; sign(-2.5)\n-1.0\n\njulia&gt; sign(0)\n0\n\njulia&gt; sign(2.5)\n1.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#raíces-exponenciales-y-logaritmos",
    "href": "01-introduccion.html#raíces-exponenciales-y-logaritmos",
    "title": "1  Introducción",
    "section": "1.9 Raíces, exponenciales y logaritmos",
    "text": "1.9 Raíces, exponenciales y logaritmos\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nsqrt(x), √x\nRaíz cuadrada de x\n\n\ncbrt(x), ∛x\nRaíz cúbica de x\n\n\nexp(x)\nExponencial de x\n\n\nlog(x)\nLogaritmo neperiano de x\n\n\nlog(b,x)\nLogaritmo en base b de x\n\n\nlog2(x)\nLogaritmo en base 2 de x\n\n\nlog10(x)\nLogaritmo en base 10 de x\n\n\n\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\njulia&gt; sqrt(4)\n2.0\n\njulia&gt; cbrt(27)\n3.0\n\njulia&gt; exp(1)\n2.718281828459045\n\njulia&gt; exp(-Inf)\n0.0\n\njulia&gt; log(1)\n0.0\n\njulia&gt; log(0)\n-Inf\n\njulia&gt; log(-1)\nERROR: DomainError with -1.0:\nlog will only return a complex result if called with a complex argument.\n...\n\njulia&gt; log(-1+0im)\n0.0 + 3.141592653589793im\n\njulia&gt; log2(2^3)\n3.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-trigonométricas",
    "href": "01-introduccion.html#funciones-trigonométricas",
    "title": "1  Introducción",
    "section": "1.10 Funciones trigonométricas",
    "text": "1.10 Funciones trigonométricas\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nhypot(x,y)\nHipotenusa del triángulo rectángulo con catetos x e y\n\n\nsin(x)\nSeno del ángulo x en radianes\n\n\nsind(x)\nSeno del ángulo x en grados\n\n\ncos(x)\nCoseno del ángulo x en radianes\n\n\ncosd(x)\nCoseno del ángulo x en grados\n\n\ntan(x)\nTangente del ángulo x en radianes\n\n\ntand(x)\nTangente del ángulo x en grados\n\n\nsec(x)\nSecante del ángulo x en radianes\n\n\ncsc(x)\nCosecante del ángulo x en radianes\n\n\ncot(x)\nCotangente del ángulo x en radianes\n\n\n\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\njulia&gt; sin(π/2)\n1.0\n\njulia&gt; cos(π/2)\n6.123233995736766e-17\n\njulia&gt; cosd(90)\n0.0\n\njulia&gt; tan(π/4)\n0.9999999999999999\n\njulia&gt; tand(45)\n1.0\n\njulia&gt; tan(π/2)\n1.633123935319537e16\n\njulia&gt; tand(90)\nInf\n\njulia&gt; sin(π/4)^2 + cos(π/4)^2\n1.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-trigonométricas-inversas",
    "href": "01-introduccion.html#funciones-trigonométricas-inversas",
    "title": "1  Introducción",
    "section": "1.11 Funciones trigonométricas inversas",
    "text": "1.11 Funciones trigonométricas inversas\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nasin(x)\nArcoseno (inversa del seno) de x en radianes\n\n\nasind(x)\nArcoseno (inversa del seno) de x en grados\n\n\nacos(x)\nArcocoseno (inversa del coseno) de x en radianes\n\n\nacosd(x)\nArcocoseno (inversa del coseno) de x en grados\n\n\natan(x)\nArcotangente (inversa de la tangente) de x en radianes\n\n\natand(x)\nArcotangente (inversa de la tangente) de x en grados\n\n\nasec(x)\nArcosecante (inversa de la secante) de x en radianes\n\n\nacsc(x)\nArcocosecante (inversa de la cosecante) de x en radianes\n\n\nacot(x)\nArcocotangente (inversa de la cotangente) de x en radianes\n\n\n\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\njulia&gt; asin(1)\n1.5707963267948966\n\njulia&gt; asind(1)\n90.0\n\njulia&gt; acos(-1)\n3.141592653589793\n\njulia&gt; atan(1)\n0.7853981633974483\n\njulia&gt; atand(tan(π/4))\n45.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#precedencia-de-operadores",
    "href": "01-introduccion.html#precedencia-de-operadores",
    "title": "1  Introducción",
    "section": "1.12 Precedencia de operadores",
    "text": "1.12 Precedencia de operadores\nA la hora de evaluar una expresión aritmética, Julia evalúa los operadores según el siguiente orden de prioridad (de mayor a menor prioridad).\n\n\n\n\n\n\n\n\nCategoría\nOperadores\nAsociatividad\n\n\n\n\nFunciones\nexp, log, sin, etc.\n\n\n\nExponenciación\n^\nDerecha\n\n\nUnarios\n+ - √\nDerecha\n\n\nFracciones\n//\nIzquierda\n\n\nMultiplicación\n* / % & \\ ÷\nIzquierda\n\n\nAdición\n+ - |\nIzquierda\n\n\nComparaciones\n&gt; &lt; &gt;= &lt;= == != !==\n\n\n\nAsignaciones\n= += -= *= /= //= ^= ÷= %= |= &=\nDerecha\n\n\n\nCuando se quiera evaluar un operador con menor prioridad antes que otro con mayor prioridad, hay que utilizar paréntesis.\n\n\n\n\n\n\nNotaEjemplo\n\n\n\n\n\njulia&gt; 1 + 4 ^ 2 / 2 - 3\n6.0\n\njulia&gt; (1 + 4 ^ 2) / 2 - 3\n5.5\n\njulia&gt; (1 + 4) ^ 2 / 2 - 3\n9.5\n\njulia&gt; 1 + 4 ^ 2 / (2 - 3)\n-15.0\n\njulia&gt; (1 + 4 ^ 2) / (2 - 3)\n-17.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#definición-de-variables",
    "href": "01-introduccion.html#definición-de-variables",
    "title": "1  Introducción",
    "section": "1.13 Definición de variables",
    "text": "1.13 Definición de variables\nPara definir variables se pueden utilizar cualquier carácter Unicode. Los nombres de las variables pueden contener más de una letra y, en tal caso, pueden usarse también números, pero siempre debe comenzar por una letra. Así, para Julia, la expresión xy, no se interpreta como el producto de la variable \\(x\\) por la variable \\(y\\), sino como la variable \\(xy\\). Además, se distingue entre mayúsculas y minúsculas, así que no es lo mismo \\(xy\\) que \\(xY\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html",
    "href": "02-preprocesamiento.html",
    "title": "2  Preprocesamiento de datos",
    "section": "",
    "text": "2.1 Ejercicios Resueltos\nEsta práctica contiene ejercicios que muestran como preprocesar un conjunto de datos con Julia. El preprocesamiento de datos es una tarea fundamental en la construcción de modelos de aprendizaje automático que consiste en la limpieza, transformación y preparación de los datos para que puedan alimentar el proceso de entrenamiento de los modelos, así como para la evaluación de su rendimiento. El preprocesamiento de datos incluye tareas como\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-resueltos",
    "href": "02-preprocesamiento.html#ejercicios-resueltos",
    "title": "2  Preprocesamiento de datos",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de gráficas.\nusing Makie  # Para obtener gráficos interactivos.\nusing StatsBase  # Para la estandarización de variables.\nusing Statistics  # Para el cálculo de estadísticas.\n\nEjercicio 2.1 La siguiente tabla contiene los ingresos y gastos de una empresa durante el primer trimestre del año.\n\nCrear un data frame con los datos de la tabla.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función DataFrame del paquete DataFrames para partir el rango de valores en intervalos y asociar a cada intervalo una categoría.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing DataFrames\ndf = DataFrame(\n    Mes = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"],\n    Ingresos = [45000, 41500, 51200, 49700],\n    Gastos = [33400, 35400, 35600, 36300],\n    Impuestos = [6450, 6300, 7100, 6850]\n    )\n\n4×4 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\n\n\n\nString\nInt64\nInt64\nInt64\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n\n\n2\nFebrero\n41500\n35400\n6300\n\n\n3\nMarzo\n51200\n35600\n7100\n\n\n4\nAbril\n49700\n36300\n6850\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con los beneficios de cada mes (ingresos - gastos - impuestos).\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndf.Beneficios = df.Ingresos - df.Gastos - df.Impuestos\ndf\n\n4×5 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\nBeneficios\n\n\n\nString\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n5150\n\n\n2\nFebrero\n41500\n35400\n6300\n-200\n\n\n3\nMarzo\n51200\n35600\n7100\n8500\n\n\n4\nAbril\n49700\n36300\n6850\n6550\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con la variable Balance con dos posibles categorías: positivo si ha habido beneficios y negativo si ha habido pérdidas.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndf.Balance = ifelse.(df.Beneficios .&gt; 0, \"positivo\", \"negativo\")\ndf\n\n4×6 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\nBeneficios\nBalance\n\n\n\nString\nInt64\nInt64\nInt64\nInt64\nString\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n5150\npositivo\n\n\n2\nFebrero\n41500\n35400\n6300\n-200\nnegativo\n\n\n3\nMarzo\n51200\n35600\n7100\n8500\npositivo\n\n\n4\nAbril\n49700\n36300\n6850\n6550\npositivo\n\n\n\n\n\n\n\n\n\nFiltrar el conjunto de datos para quedarse con los nombres de los meses y los beneficios de los meses con balance positivo.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndf[df.Balance .== \"positivo\", [:Mes, :Beneficios]]\n\n3×2 DataFrame\n\n\n\nRow\nMes\nBeneficios\n\n\n\nString\nInt64\n\n\n\n\n1\nEnero\n5150\n\n\n2\nMarzo\n8500\n\n\n3\nAbril\n6550\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.2 El fichero colesterol.csv contiene información de una muestra de pacientes donde se han medido la edad, el sexo, el peso, la altura y el nivel de colesterol, además de su nombre.\n\nCrear un data frame con los datos de todos los pacientes del estudio a partir del fichero colesterol.csv.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función CSV.read del paquete CSV para crear und data frame a partir de un fichero CSV. Si el fichero está en una url, utilizar la función download(url) para descargar el fichero y después leerlo con la función [CSV.read].\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/colesterol.csv\"), DataFrame)\n\n14×6 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con el índice de masa corporal, usando la siguiente fórmula\n\\[\n\\mbox{IMC} = \\frac{\\mbox{Peso (kg)}}{\\mbox{Altura (cm)}^2}\n\\]\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndf.imc = df.peso ./ (df.altura .^ 2)\ndf\n\n14×7 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n21.7738\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con la variable obesidad recodificando la columna imc en las siguientes categorías.\n\n\n\nRango IMC\nCategoría\n\n\n\n\nMenor de 18.5\nBajo peso\n\n\nDe 18.5 a 24.5\nSaludable\n\n\nDe 24.5 a 30\nSobrepeso\n\n\nMayor de 30\nObeso\n\n\n\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función cut del paquete CategoricalArrays para partir el rango de valores en intervalos y asociar a cada intervalo una categoría.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CategoricalArrays\ndf.obesidad = cut(df.imc, [0, 18.5, 24.5, 30, Inf],\n                labels=[\"Bajo peso\", \"Saludable\", \"Sobrepeso\", \"Obeso\"],\n                extend=true)\ndf\n\n14×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nSeleccionar las columnas nombre, sexo y edad.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndf[:, [:nombre, :sexo, :edad]]\n\n14×3 DataFrame\n\n\n\nRow\nnombre\nsexo\nedad\n\n\n\nString\nString1\nInt64\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\nH\n18\n\n\n2\nRosa Díaz Díaz\nM\n32\n\n\n3\nJavier García Sánchez\nH\n24\n\n\n4\nCarmen López Pinzón\nM\n35\n\n\n5\nMarisa López Collado\nM\n46\n\n\n6\nAntonio Ruiz Cruz\nH\n68\n\n\n7\nAntonio Fernández Ocaña\nH\n51\n\n\n8\nPilar Martín González\nM\n22\n\n\n9\nPedro Gálvez Tenorio\nH\n35\n\n\n10\nSantiago Reillo Manzano\nH\n46\n\n\n11\nMacarena Álvarez Luna\nM\n53\n\n\n12\nJosé María de la Guía Sanz\nH\n58\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\nH\n27\n\n\n14\nCarolina Rubio Moreno\nM\n20\n\n\n\n\n\n\n\n\n\nAnonimizar los datos eliminando la columna nombre.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función select del paquete DataFrames para seleccionar las columnas deseadas y eliminar las columnas no deseadas. Existe también la función select! que modifica el data frame original eliminando las columnas no seleccionadas.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nselect(df, Not(:nombre))\n\n14×7 DataFrame\n\n\n\nRow\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nReordenar las columnas poniendo la columna sexo antes que la columna edad.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nselect(df, Cols(:sexo, :edad, Not(:sexo, :edad)))\n\n14×8 DataFrame\n\n\n\nRow\nsexo\nedad\nnombre\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString1\nInt64\nString\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nH\n18\nJosé Luis Martínez Izquierdo\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nM\n32\nRosa Díaz Díaz\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nH\n24\nJavier García Sánchez\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nM\n35\nCarmen López Pinzón\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nM\n46\nMarisa López Collado\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nH\n68\nAntonio Ruiz Cruz\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nH\n51\nAntonio Fernández Ocaña\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nM\n22\nPilar Martín González\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\nH\n35\nPedro Gálvez Tenorio\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nH\n46\nSantiago Reillo Manzano\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nM\n53\nMacarena Álvarez Luna\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nH\n58\nJosé María de la Guía Sanz\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nH\n27\nMiguel Angel Cuadrado Gutiérrez\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nM\n20\nCarolina Rubio Moreno\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las mujeres.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndf[df.sexo .== \"M\", :]\n\n6×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n2\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n3\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n4\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n5\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n6\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con los hombres mayores de 30 años.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndf[(df.sexo .== \"H\") .& (df.edad .&gt; 30), :]\n\n5×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n2\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n3\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n4\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n5\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las filas sin valores perdidos.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función dropmissing del paquete DataFrames para eliminar las filas con valores perdidos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndropmissing(df)\n\n12×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n4\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n5\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n6\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n7\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n8\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n9\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n10\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n11\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n12\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para eliminar las filas con datos perdidos en la columna colesterol.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función dropmissing, col donde col es el nombre de la columna que contiene los valores perdidos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndropmissing(df, :colesterol)    \n\n13×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n9\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n10\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n11\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n12\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n13\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nImputar los valores perdidos en la columna colesterol con la media de los valores no perdidos.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función coalesce para reemplazar los valores perdidos por otros valores.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Statistics\nmedia_colesterol = mean(skipmissing(df.colesterol))\ndf.colesterol = coalesce.(df.colesterol, media_colesterol)\ndf\n\n14×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con las puntuaciones típicas del colesterol, es decir, estandarizando la columna colesterol.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función Standardizer del paquete StatsBase para estandarizar una variable.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing StatsBase\ndf.colesterol_estandarizado = standardize(ZScoreTransform, df.colesterol)\ndf\n\n14×9 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\ncolesterol_estandarizado\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\nFloat64\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n-0.998592\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n0.307414\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n-0.763511\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n-0.52843\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n-1.88668\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n0.751456\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n1.4567\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n-7.42378e-16\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n0.542495\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n1.56118\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n1.09102\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n-0.58067\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n-0.267229\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n-0.685151\n\n\n\n\n\n\n\n\n\nOrdenar el data frame según la columna nombre.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función sort para ordenar las filas del data frame según los valores de una o varias columnas. Utilizar el parámetro rev para especificar mediante un vector de booleanos si el orden es ascendente o descendente.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nsort(df, :nombre)\n\n14×9 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\ncolesterol_estandarizado\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\nFloat64\n\n\n\n\n1\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n1.4567\n\n\n2\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n0.751456\n\n\n3\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n-0.52843\n\n\n4\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n-0.685151\n\n\n5\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n-0.763511\n\n\n6\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n-0.998592\n\n\n7\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n-0.58067\n\n\n8\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n1.09102\n\n\n9\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n-1.88668\n\n\n10\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n-0.267229\n\n\n11\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n0.542495\n\n\n12\nPilar Martín González\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n-7.42378e-16\n\n\n13\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n0.307414\n\n\n14\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n1.56118\n\n\n\n\n\n\n\n\n\nOrdenar el data frame ascendentemente por la columna sexo y descendentemente por la columna edad.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nsort(df, [:sexo, :edad], rev=[false, true])\n\n14×9 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\ncolesterol_estandarizado\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\nFloat64\n\n\n\n\n1\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n0.751456\n\n\n2\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n-0.58067\n\n\n3\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n1.4567\n\n\n4\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n1.56118\n\n\n5\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n0.542495\n\n\n6\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n-0.267229\n\n\n7\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n-0.763511\n\n\n8\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n-0.998592\n\n\n9\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n1.09102\n\n\n10\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n-1.88668\n\n\n11\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n-0.52843\n\n\n12\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n0.307414\n\n\n13\nPilar Martín González\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n-7.42378e-16\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n-0.685151\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.3 El fichero notas-curso2.csv contiene información de las notas de los alumnos de un curso.\n\nCrear un data frame con los datos de los alumnos del curso a partir del fichero notas-curso2.csv.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/notas-curso2.csv\"), DataFrame; missingstring=\"NA\")\n\n120×9 DataFrame95 rows omitted\n\n\n\nRow\nsexo\nturno\ngrupo\ntrabaja\nnotaA\nnotaB\nnotaC\nnotaD\nnotaE\n\n\n\nString7\nString7\nString1\nString1\nFloat64\nFloat64?\nFloat64?\nFloat64?\nFloat64?\n\n\n\n\n1\nMujer\nTarde\nC\nN\n5.2\n6.3\n3.4\n2.3\n2.0\n\n\n2\nHombre\nMañana\nA\nN\n5.7\n5.7\n4.2\n3.5\n2.7\n\n\n3\nHombre\nMañana\nB\nN\n8.3\n8.8\n8.8\n8.0\n5.5\n\n\n4\nHombre\nMañana\nB\nN\n6.1\n6.8\n4.0\n3.5\n2.2\n\n\n5\nHombre\nMañana\nA\nN\n6.2\n9.0\n5.0\n4.4\n3.7\n\n\n6\nHombre\nMañana\nA\nS\n8.6\n8.9\n9.5\n8.4\n3.9\n\n\n7\nMujer\nMañana\nA\nN\n6.7\n7.9\n5.6\n4.8\n4.2\n\n\n8\nMujer\nTarde\nC\nS\n4.1\n5.2\n1.7\n0.3\n1.0\n\n\n9\nHombre\nTarde\nC\nN\n5.0\n5.0\n3.3\n2.7\n6.0\n\n\n10\nHombre\nTarde\nC\nN\n5.3\n6.3\n4.8\n3.6\n2.3\n\n\n11\nMujer\nMañana\nA\nN\n7.8\nmissing\n6.5\n6.7\n2.8\n\n\n12\nHombre\nMañana\nA\nN\n6.5\n8.0\n5.0\n3.2\n3.3\n\n\n13\nHombre\nMañana\nB\nN\n6.6\n7.6\n5.3\n4.0\n1.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nMujer\nTarde\nC\nN\n5.4\n7.3\n3.5\n2.5\n4.6\n\n\n110\nHombre\nMañana\nB\nN\n7.4\n7.4\n6.2\n5.8\n1.1\n\n\n111\nMujer\nTarde\nC\nS\n5.1\n8.1\n5.2\n5.1\n4.5\n\n\n112\nHombre\nMañana\nA\nN\n6.9\n7.8\n3.9\n2.8\nmissing\n\n\n113\nHombre\nTarde\nC\nN\n3.6\n4.8\n2.1\n0.5\n5.6\n\n\n114\nHombre\nTarde\nC\nS\n5.9\n6.2\n5.0\n3.9\n1.9\n\n\n115\nHombre\nMañana\nB\nN\n6.8\n7.2\n4.9\n3.8\n2.8\n\n\n116\nHombre\nMañana\nA\nN\n6.5\n6.1\n5.8\n4.9\n1.2\n\n\n117\nMujer\nMañana\nB\nN\n6.2\n7.0\n5.6\n5.4\n1.7\n\n\n118\nMujer\nTarde\nC\nN\n5.0\n6.5\n4.0\n2.8\n3.6\n\n\n119\nHombre\nTarde\nC\nN\n4.7\n6.0\n1.3\n0.4\n2.2\n\n\n120\nHombre\nTarde\nC\nS\n4.5\n4.7\n6.0\n4.9\n1.8\n\n\n\n\n\n\n\n\n\nObtener el número de datos perdidos en cada columna.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndescribe(df)[:, [:variable, :nmissing]]\n\n9×2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\nsexo\n0\n\n\n2\nturno\n0\n\n\n3\ngrupo\n0\n\n\n4\ntrabaja\n0\n\n\n5\nnotaA\n0\n\n\n6\nnotaB\n5\n\n\n7\nnotaC\n1\n\n\n8\nnotaD\n2\n\n\n9\nnotaE\n2\n\n\n\n\n\n\n\n\n\nRecodificar la variable grupo en una colección de columnas binarias.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función onehotbatch del paquete OneHotArrays para recodificar una variable categórica en una colección de columnas binarias.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing OneHotArrays\ncodificacion = permutedims(onehotbatch(df.grupo, unique(df.grupo)))\nhcat(df, DataFrame(codificacion, :auto))\n\n120×12 DataFrame95 rows omitted\n\n\n\nRow\nsexo\nturno\ngrupo\ntrabaja\nnotaA\nnotaB\nnotaC\nnotaD\nnotaE\nx1\nx2\nx3\n\n\n\nString7\nString7\nString1\nString1\nFloat64\nFloat64?\nFloat64?\nFloat64?\nFloat64?\nBool\nBool\nBool\n\n\n\n\n1\nMujer\nTarde\nC\nN\n5.2\n6.3\n3.4\n2.3\n2.0\ntrue\nfalse\nfalse\n\n\n2\nHombre\nMañana\nA\nN\n5.7\n5.7\n4.2\n3.5\n2.7\nfalse\ntrue\nfalse\n\n\n3\nHombre\nMañana\nB\nN\n8.3\n8.8\n8.8\n8.0\n5.5\nfalse\nfalse\ntrue\n\n\n4\nHombre\nMañana\nB\nN\n6.1\n6.8\n4.0\n3.5\n2.2\nfalse\nfalse\ntrue\n\n\n5\nHombre\nMañana\nA\nN\n6.2\n9.0\n5.0\n4.4\n3.7\nfalse\ntrue\nfalse\n\n\n6\nHombre\nMañana\nA\nS\n8.6\n8.9\n9.5\n8.4\n3.9\nfalse\ntrue\nfalse\n\n\n7\nMujer\nMañana\nA\nN\n6.7\n7.9\n5.6\n4.8\n4.2\nfalse\ntrue\nfalse\n\n\n8\nMujer\nTarde\nC\nS\n4.1\n5.2\n1.7\n0.3\n1.0\ntrue\nfalse\nfalse\n\n\n9\nHombre\nTarde\nC\nN\n5.0\n5.0\n3.3\n2.7\n6.0\ntrue\nfalse\nfalse\n\n\n10\nHombre\nTarde\nC\nN\n5.3\n6.3\n4.8\n3.6\n2.3\ntrue\nfalse\nfalse\n\n\n11\nMujer\nMañana\nA\nN\n7.8\nmissing\n6.5\n6.7\n2.8\nfalse\ntrue\nfalse\n\n\n12\nHombre\nMañana\nA\nN\n6.5\n8.0\n5.0\n3.2\n3.3\nfalse\ntrue\nfalse\n\n\n13\nHombre\nMañana\nB\nN\n6.6\n7.6\n5.3\n4.0\n1.0\nfalse\nfalse\ntrue\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nMujer\nTarde\nC\nN\n5.4\n7.3\n3.5\n2.5\n4.6\ntrue\nfalse\nfalse\n\n\n110\nHombre\nMañana\nB\nN\n7.4\n7.4\n6.2\n5.8\n1.1\nfalse\nfalse\ntrue\n\n\n111\nMujer\nTarde\nC\nS\n5.1\n8.1\n5.2\n5.1\n4.5\ntrue\nfalse\nfalse\n\n\n112\nHombre\nMañana\nA\nN\n6.9\n7.8\n3.9\n2.8\nmissing\nfalse\ntrue\nfalse\n\n\n113\nHombre\nTarde\nC\nN\n3.6\n4.8\n2.1\n0.5\n5.6\ntrue\nfalse\nfalse\n\n\n114\nHombre\nTarde\nC\nS\n5.9\n6.2\n5.0\n3.9\n1.9\ntrue\nfalse\nfalse\n\n\n115\nHombre\nMañana\nB\nN\n6.8\n7.2\n4.9\n3.8\n2.8\nfalse\nfalse\ntrue\n\n\n116\nHombre\nMañana\nA\nN\n6.5\n6.1\n5.8\n4.9\n1.2\nfalse\ntrue\nfalse\n\n\n117\nMujer\nMañana\nB\nN\n6.2\n7.0\n5.6\n5.4\n1.7\nfalse\nfalse\ntrue\n\n\n118\nMujer\nTarde\nC\nN\n5.0\n6.5\n4.0\n2.8\n3.6\ntrue\nfalse\nfalse\n\n\n119\nHombre\nTarde\nC\nN\n4.7\n6.0\n1.3\n0.4\n2.2\ntrue\nfalse\nfalse\n\n\n120\nHombre\nTarde\nC\nS\n4.5\n4.7\n6.0\n4.9\n1.8\ntrue\nfalse\nfalse",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-propuestos",
    "href": "02-preprocesamiento.html#ejercicios-propuestos",
    "title": "2  Preprocesamiento de datos",
    "section": "2.2 Ejercicios propuestos",
    "text": "2.2 Ejercicios propuestos\n\nEjercicio 2.4 El fichero vinos.csv contiene información sobre las características de una muestra de vinos portugueses de la denominación “Vinho Verde”. Las variables que contiene son:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nCategórica (blanco, tinto)\n\n\nmeses.barrica\nMesesde envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidadde ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcarremanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufreen formalibre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidadde dióxido de azufretotal en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidadde sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentajede contenidode alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada porun panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un data frame con los datos de los vinos a partir del fichero vinos.csv.\nObtener el número de valores perdidos en cada columna.\nImputar los valores perdidos del alcohol con la media de los valores no perdidos para cada tipo de vino.\nCrear la variable categórica Envejecimiento recodificando la variable meses.barrica en las siguientes categorías.\n\n\n\nRango en meses\nCategoría\n\n\n\n\nMenos de 3\nJoven\n\n\nEntre 3 y 12\nCrianza\n\n\nEntre 12 y 18\nReserva\n\n\nMás de 18\nGran reserva\n\n\n\nCrear la variable categórica Dulzor recodificando la variable azucar.residual en las siguientes categorías.\n\n\n\nRango azúcar\nCategoría\n\n\n\n\nMenos de 4\nSeco\n\n\nMás de 4 y menos de 12\nSemiseco\n\n\nMás de 12 y menos de 45\nSemidulce\n\n\nMás de 45\nDulce\n\n\n\nFiltrar el conjunto de datos para quedarse con los vinos Reserva o Gran Reserva con una calidad superior a 7 y ordenar el data frame por calidad de forma descendente.\n¿Cuántos vinos blancos con un contenido en alcohol superior al 12% y una calidad superior a 8 hay en el conjunto de datos?\n¿Cuáles son los 10 mejores vinos tintos crianza secos?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "03-regresion.html",
    "href": "03-regresion.html",
    "title": "3  Regresión",
    "section": "",
    "text": "3.1 Ejercicios Resueltos\nLos modelos de aprendizaje basados en regresión son modelos bastante simples que pueden utilizarse para predecir variables cuantitativas (regresión lineal) o cualitativas (regresión logística). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje de regresión lineal y regresión logística con Julia.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#ejercicios-resueltos",
    "href": "03-regresion.html#ejercicios-resueltos",
    "title": "3  Regresión",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de gráficas.\nusing GLMakie  # Para obtener gráficos interactivos.\n\nEjercicio 3.1 El conjunto de datos viviendas.csv contiene información sobre el precio de venta de viviendas en una ciudad.\n\nCargar los datos del archivo viviendas.csv en un data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/viviendas.csv\"), DataFrame)\nfirst(df, 5)\n\n5×13 DataFrame\n\n\n\nRow\nprecio\narea\ndormitorios\nbaños\nhabitaciones\ncalleprincipal\nhuespedes\nsotano\ncalentador\nclimatizacion\ngaraje\ncentrico\namueblado\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nString3\nString3\nString3\nString3\nString3\nInt64\nString3\nString15\n\n\n\n\n1\n13300000\n7420\n4\n2\n3\nsi\nno\nno\nno\nsi\n2\nsi\namueblado\n\n\n2\n12250000\n8960\n4\n4\n4\nsi\nno\nno\nno\nsi\n3\nno\namueblado\n\n\n3\n12250000\n9960\n3\n2\n2\nsi\nno\nsi\nno\nno\n2\nsi\nsemi-amueblado\n\n\n4\n12215000\n7500\n4\n2\n2\nsi\nno\nsi\nno\nsi\n3\nsi\namueblado\n\n\n5\n11410000\n7420\n4\n1\n2\nsi\nsi\nsi\nno\nsi\n2\nno\namueblado\n\n\n\n\n\n\n\n\n\nDibujar un diagrama de dispersión entre el precio y el area de las viviendas.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Plots\nplt = scatter(df.area, df.precio, xlabel=\"Area\", ylabel=\"Precio\", title=\"Precio vs Area\", label = \"Ejemplos\", fmt=:png,)\n\n\n\n\n\n\n\nDefinir un modelo lineal que explique el precio en función del área de las viviendas.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUn modelo lineal tiene ecuación \\(y = \\theta_1 + \\theta_2 x\\).\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nprecio(area, θ) = θ[1] .+ θ[2] * area\n\nprecio (generic function with 1 method)\n\n\nObserva que la función precio está vectorizada, lo que significa que puede recibir un vector de áreas y devolver un vector de precios.\n\n\n\nInicializar los parámetros del modelo lineal con valores nulos y dibujar el modelo sobre el diagrama de dispersión.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nθ = [0.0, 0.0]\nplot!(df.area, precio(df.area, θ), label = \"Modelo 0\")\n\n\n\n\n\n\n\nDefinir una función de costo para el modelo lineal y evaluar el coste para el modelo lineal construido con los parámetros iniciales. A la vista del coste obtenido, ¿cómo de bueno es el modelo?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nLa función de coste para un modelo lineal es el error cuadrático medio.\n\\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]\ndonde \\(h_\\theta\\) es el modelo, \\(h_\\theta(x^{(i)})\\) es la predicción del modelo para el ejemplo \\(i\\)-ésimo, \\(y^{(i)}\\) es el valor real observado para el ejemplo \\(i\\)-ésimo, y \\(m\\) es el número de ejemplos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction coste(θ, X, Y)\n    m = length(Y)\n    return sum((precio(X, θ) .- Y).^2) / (2 * m)\nend\n\ncoste(θ, df.area, df.precio)\n\n1.3106916364659266e13\n\n\nLa función de coste nos da una medida de lo lejos que están las predicciones del modelo de los valores reales observados. En este caso, el coste es muy alto, lo que indica que el modelo no es bueno.\n\n\n\n¿En qué dirección debemos modificar los parámetros del modelo para mejorar el modelo?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\nPara minimizar la función de coste, debemos modificar los parámetros del modelo en la dirección opuesta al gradiente de la función de coste, ya que el gradiente de una función indica la dirección de mayor crecimiento de la función.\n\n\n\nCrear una función para modificar los pesos del modelo lineal mediante el algoritmo del gradiente descendente, y aplicarla a los parámetros actuales tomando una tasa de aprendizaje de \\(10^{-8}\\). ¿Cómo han cambiado los parámetros del modelo? Dibujar el modelo actualizado sobre el diagrama de dispersión. ¿Cómo ha cambiado el coste?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nEl algoritmo del gradiente descendente actualiza los parámetros del modelo de acuerdo a la siguiente regla:\n\\[\n\\theta_j = \\theta_j - \\eta \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n\\]\ndonde \\(\\eta\\) es la tasa de aprendizaje y \\(\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\) es la derivada parcial de la función de coste con respecto al parámetro \\(\\theta_j\\).\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction gradiente_descendente!(θ, X, Y, η)\n    # Calculamos el número de ejemplos\n    m = length(Y)\n    # Actualizamos el término independiente del modelo lineal.\n    θ[1] -= η * sum(precio(X, θ) - Y) / m\n    # Actualizamos la pendiente del modelo lineal.\n    θ[2] -= η * sum((precio(X, θ) - Y) .* X) / m\n    return θ\nend\n\ngradiente_descendente! (generic function with 1 method)\n\n\nAplicamos la función a los parámetros del modelo actual y mostramos los nuevos parámetros.\n\ngradiente_descendente!(θ, df.area, df.precio, 1e-8)\nθ\n\n2-element Vector{Float64}:\n   0.04766729247706422\n 267.22919804579385\n\n\nDibujamos el nuevo modelo.\n\nplot!(df.area, precio(df.area, θ), label = \"Modelo 1\")\n\n\n\n\nSe observa que ahora la recta está más cerca de la nube de puntos, por lo que el modelo ha mejorado. Calculamos el coste del nuevo modelo.\n\ncoste(θ, df.area, df.precio)\n\n7.080823787113201e12\n\n\n\n\n\nRepetir el proceso de actualización de los parámetros del modelo mediante el algoritmo del gradiente descendente durante 9 iteraciones más y dibujar los modelos actualizados.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\nfor i = 2:10\n    gradiente_descendente!(θ, df.area, df.precio, 1e-8)\n    plot!(df.area, precio(df.area, θ), label = \"Modelo $i\", legend = true)\nend\nplt\n\n\n\n\nDibujar un gráfico con la evolución del coste del modelo a lo largo de las iteraciones. ¿Cómo se comporta el coste a lo largo de las iteraciones?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ncostes = Float64[]\nfor i = 1:10\n    gradiente_descendente!(θ, df.area, df.precio, 1e-8)\n    push!(costes, coste(θ, df.area, df.precio))\nend\ncostes\n\n10-element Vector{Float64}:\n 4.230808760870044e12\n 2.882906194020343e12\n 2.2454213686913755e12\n 1.9439256128790886e12\n 1.8013344680594421e12\n 1.7338965877160208e12\n 1.7020021263374993e12\n 1.6869177748236997e12\n 1.6797836937723748e12\n 1.6764096595632322e12\n\n\nEl coste del modelo disminuye en cada iteración, lo que indica que el modelo está mejorando. Esto se debe a que el algoritmo del gradiente descendente modifica los parámetros del modelo en la dirección que minimiza la función de coste.\n\n\n\n¿Hasta qué iteración habrá que llegar para conseguir un reducción del coste menor de un \\(0.0001\\%\\)?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nθ = [0.0, 0.0]\ncostes = [0, coste(θ, df.area, df.precio)]\ni = 1\nwhile abs(costes[end] - costes[end-1]) / costes[end-1] &gt; 0.000001\n    i += 1\n    gradiente_descendente!(θ, df.area, df.precio, 1e-8)\n    push!(costes, coste(θ, df.area, df.precio))\nend\ni\n\n23\n\n\nEn este caso, el algoritmo del gradiente descendente converge en 1000 iteraciones.\n\n\n\n¿Qué sucede si se utiliza una tasa de aprendizaje \\(\\eta = 0.0001\\)? ¿Cómo afecta al coste y a la convergencia del modelo?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nθ = [0.0, 0.0]\ncostes = [coste(θ, df.area, df.precio)]\nfor i = 1:10\n    gradiente_descendente!(θ, df.area, df.precio, 0.0001)\n    push!(costes, coste(θ, df.area, df.precio))\nend\ncostes\n\n11-element Vector{Float64}:\n 1.3106916364659266e13\n 1.114133369099188e20\n 1.0856750832581238e27\n 1.05794371802143e34\n 1.0309206941949286e41\n 1.004587918634273e48\n 9.789277603492545e54\n 9.539230386975057e61\n 9.29557011881276e68\n 9.058133657380397e75\n 8.826762028174244e82\n\n\nSi la tasa de aprendizaje es demasiado grande, el algoritmo del gradiente descendente puede no converger y el coste puede oscilar en lugar de disminuir. En este caso, el coste aumenta en cada iteración, lo que indica que la tasa de aprendizaje es demasiado grande.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "07-arboles-decision.html",
    "href": "07-arboles-decision.html",
    "title": "4  Árboles de decisión",
    "section": "",
    "text": "4.1 Ejercicios Resueltos\nLos árboles de decisión son modelos de aprendizaje simples e intuitivos que pueden utilizarse para tanto para predecir variables cuantitativas (regresión) como categóricas (clasificación). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje basados en árboles de decisión con Julia.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Árboles de decisión</span>"
    ]
  },
  {
    "objectID": "07-arboles-decision.html#ejercicios-resueltos",
    "href": "07-arboles-decision.html#ejercicios-resueltos",
    "title": "4  Árboles de decisión",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing Tidier # Para el preprocesamiento de datos.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing GLMakie  # Para obtener gráficos interactivos.\nusing AlgebraOfGraphics # Para generar gráficos mediante la gramática de gráficos.\nusing DecisionTree # Para construir árboles de decisión.\nusing GraphMakie # Para la visualización de árboles de decisión.\n\nEjercicio 4.1 El conjunto de datos tenis.csv contiene información sobre las condiciones meteorológicas de varios días y si se pudo jugar al tenis o no.\n\nCargar los datos del archivo tenis.csv en un data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/tenis.csv\"), DataFrame)\n\n14×5 DataFrame\n\n\n\nRow\nCielo\nTemperatura\nHumedad\nViento\nTenis\n\n\n\nString15\nString15\nString7\nString7\nString3\n\n\n\n\n1\nSoleado\nCaluroso\nAlta\nSuave\nNo\n\n\n2\nSoleado\nCaluroso\nAlta\nFuerte\nNo\n\n\n3\nNublado\nCaluroso\nAlta\nSuave\nSí\n\n\n4\nLluvioso\nModerado\nAlta\nSuave\nSí\n\n\n5\nLluvioso\nFrío\nNormal\nSuave\nSí\n\n\n6\nLluvioso\nFrío\nNormal\nFuerte\nNo\n\n\n7\nNublado\nFrío\nNormal\nFuerte\nSí\n\n\n8\nSoleado\nModerado\nAlta\nSuave\nNo\n\n\n9\nSoleado\nFrío\nNormal\nSuave\nSí\n\n\n10\nLluvioso\nModerado\nNormal\nSuave\nSí\n\n\n11\nSoleado\nModerado\nNormal\nFuerte\nSí\n\n\n12\nNublado\nModerado\nAlta\nFuerte\nSí\n\n\n13\nNublado\nCaluroso\nNormal\nSuave\nSí\n\n\n14\nLluvioso\nModerado\nAlta\nFuerte\nNo\n\n\n\n\n\n\n\n\n\nCrear un diagrama de barras que muestre la distribución de frecuencias de cada variable meteorológica según si se pudo jugar al tenis o no. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing GLMakie, AlgebraOfGraphics\n\nfunction frecuencias(df::DataFrame, var::Symbol)\n    # Calculamos el número de días de cada clase que se juega al tenis.\n    frec = combine(groupby(df, [var, :Tenis]), nrow =&gt; :Días)\n    # Dibujamos el diagrama de barras.\n    plt = data(frec) * \n    mapping(var, :Días, stack = :Tenis, color = :Tenis, ) * \n    visual(BarPlot) \n    # Devolvemos el gráfico.\n    return plt\nend\n\nfig = Figure()\ndraw!(fig[1, 1], frecuencias(df, :Cielo))\ndraw!(fig[1, 2], frecuencias(df, :Temperatura))\ndraw!(fig[1, 3], frecuencias(df, :Humedad))\ndraw!(fig[1, 4], frecuencias(df, :Viento))\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nA la vista de las frecuencias de cada variable, las variable Cielo y Humedad parecen ser las que más influye en la decisión de jugar al tenis.\n\n\n\nCalcular la impureza del conjunto de datos utilizando el índice de Gini. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nEl índice de Gini se calcula mediante la fórmula\n\\[ GI = 1 - \\sum_{i=1}^{n} p_i^2 \\]\ndonde \\(p_i\\) es la proporción de cada clase en el conjunto de datos y \\(n\\) es el número de clases.\nEl índice de Gini toma valores entre \\(0\\) y \\(1-\\frac{1}{n}\\) (\\(0.5\\) en el caso de clasificación binaria), donde \\(0\\) indica que todas las instancias pertenecen a una sola clase (mínima impureza) y \\(1-\\frac{1}{n}\\) indica que las instancias están distribuidas uniformemente entre todas las clases (máxima impureza).\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction gini(df::DataFrame, var::Symbol)\n    # Calculamos el número de ejemplos.\n    n = nrow(df)\n    # Calculamos las frecuencias absolutas de cada clase.\n    frec = combine(groupby(df, var), nrow =&gt; :ni)\n    # Calculamos la proporción de cada clase.\n    frec.p = frec.ni ./ n\n    # Calculamos el índice de Gini.\n    gini = 1 - sum(frec.p .^ 2)\n    return gini\nend\n\ng0 = gini(df, :Tenis)\n\n0.4591836734693877\n\n\n\n\n\n¿Qué reducción del índice Gini se obtiene si dividimos el conjunto de ejemplos según la variable Humedad? ¿Y si dividimos el conjunto con respecto a la variable Viento?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nLa reducción del índice de Gini se calcula como la diferencia entre el índice de Gini del conjunto original y el índice de Gini del conjunto dividido.\n\\[ \\Delta GI = GI_{original} - GI_{dividido} \\]\ndonde el índice de Gini del conjunto dividido es la media ponderada de los índices de Gini de los subconjuntos resultantes de la división.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\nCalculamos primero la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable Humedad.\n\nusing Tidier\n# Dividimos el conjunto de ejemplos según la variable Humedad.\ndf_humedad_alta = @filter(df, Humedad == \"Alta\")\ndf_humedad_normal = @filter(df, Humedad == \"Normal\")\n# Calculamos los tamaños de los subconjuntos de ejemplos.\nn = nrow(df_humedad_alta), nrow(df_humedad_normal)\n# Calculamos el índice de Gini de cada subconjunto.\ngis = gini(df_humedad_alta, :Tenis), gini(df_humedad_normal, :Tenis)\n# Calculamos media ponderada de los índices de Gini de los subconjuntos \ng_humedad = sum(gis .* n) / sum(n)\n# Calculamos la reducción del índice de Gini.\ng0 - g_humedad\n\n0.09183673469387743\n\n\nCalculamos ahora la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable Viento.\n\n# Dividimos el conjunto de ejemplos según la variable `Viento`\ndf_viento_fuerte = @filter(df, Viento == \"Fuerte\")\ndf_viento_suave = @filter(df, Viento == \"Suave\")\n# Calculamos los tamaños de los subconjuntos de ejemplos\nn = nrow(df_viento_fuerte), nrow(df_viento_suave)\n# Calculamos el índice de Gini de cada subconjunto\ngis = gini(df_viento_fuerte, :Tenis), gini(df_viento_suave, :Tenis)\n# Calculamos media ponderada de los índices de Gini de los subconjuntos\ng_viento = sum(gis .* n) / sum(n)\n# Calculamos la reducción del índice de Gini\ng0 - g_viento\n\n0.030612244897959162\n\n\nComo se puede observar, la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable Humedad es mayor que la reducción del índice de Gini al dividir el conjunto con respecto a la variable Viento. Por lo tanto, la variable Humedad parece tener más influencia en la decisión de jugar al tenis y sería la variable que se debería elegir para dividir el conjunto de ejemplos.\n\n\n\nConstruir un árbol de decisión que explique si se puede jugar al tenis en función de las variables meteorológicas.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función DecisionTreeClassifier del paquete DecisionTree.jl.\nLos parámetros más importantes de esta función son:\n\nmax_depth: Profundidad máxima del árbol. Si no se indica, el árbol crecerá hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min_samples_split ejemplos.\nmin_samples_leaf: Número mínimo de ejemplos en una hoja (1 por defecto).\nmin_samples_split: Número mínimo de ejemplos para dividir un nodo (2 por defecto).\nmin_impurity_decrease: Reducción mínima de la impureza para dividir un nodo (0 por defecto).\npost-prune: Si se indica true, se poda el árbol después de que se ha construido. La poda reduce el tamaño del árbol eliminando nodos que no aportan información útil.\nmerge_purity_threshold: Umbral de pureza para fusionar nodos. Si se indica, se fusionan los nodos que tienen una pureza menor que este umbral.\nfeature_importance: Indica la medida para calcular la importancia de las variables a la hora de dividir el conjunto de datos. Puede ser :impurity o :split. Si no se indica, se utiliza la impureza de Gini.\nrng: Indica la semilla para la generación de números aleatorios. Si no se indica, se utiliza el generador de números aleatorios por defecto.\n\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing DecisionTree, CategoricalArrays\n# Variables predictoras.\nX = Matrix(select(df, Not(:Tenis)))\n# Variable objetivo.\ny = df.Tenis\n# Convertir las variables categóricas a enteros.\nX = hcat([levelcode.(categorical(X[:, j])) for j in 1:size(X, 2)]...)\n# Convertir la variable objetivo a enteros.\ny = levelcode.(categorical(y))\ntree = DecisionTreeClassifier(max_depth=3)\nfit!(tree, X, y)\n\nDecisionTreeClassifier\nmax_depth:                3\nmin_samples_leaf:         1\nmin_samples_split:        2\nmin_purity_increase:      0.0\npruning_purity_threshold: 1.0\nn_subfeatures:            0\nclasses:                  [1, 2]\nroot:                     Decision Tree\nLeaves: 6\nDepth:  3\n\n\n\n\n\nVisualizar el árbol de decisión construido.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función plot_tree del paquete DecisionTree.jl.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nprint_tree(tree, feature_names=names(df)[1:end-1])\n\nFeature 3: \"Humedad\" &lt; 2.0 ?\n├─ Feature 1: \"Cielo\" &lt; 3.0 ?\n    ├─ Feature 4: \"Viento\" &lt; 2.0 ?\n        ├─ 2 : 1/2\n        └─ 2 : 2/2\n    └─ 1 : 3/3\n└─ Feature 1: \"Cielo\" &lt; 2.0 ?\n    ├─ Feature 4: \"Viento\" &lt; 2.0 ?\n        ├─ 1 : 1/1\n        └─ 2 : 2/2\n    └─ 2 : 4/4\n\n\n\n\n\n\n\n\nEjercicio 4.2 El conjunto de datos pingüinos.csv contiene un conjunto de datos sobre tres especies de pingüinos con las siguientes variables:\n\nEspecie: Especie de pingüino, comúnmente Adelie, Chinstrap o Gentoo.\nIsla: Isla del archipiélago Palmer donde se realizó la observación.\nLongitud_pico: Longitud del pico en mm.\nProfundidad_pico: Profundidad del pico en mm\nLongitud_ala: Longitud de la aleta en mm.\nPeso: Masa corporal en gramos.\nSexo: Sexo\n\n\nCargar los datos del archivo pinguïnos.csv en un data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/pingüinos.csv\"), DataFrame, missingstring=\"NA\")\n\n344×7 DataFrame319 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64?\nFloat64?\nInt64?\nInt64?\nString7?\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmacho\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nhembra\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nhembra\n\n\n4\nAdelie\nTorgersen\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nhembra\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmacho\n\n\n7\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nhembra\n\n\n8\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmacho\n\n\n9\nAdelie\nTorgersen\n34.1\n18.1\n193\n3475\nmissing\n\n\n10\nAdelie\nTorgersen\n42.0\n20.2\n190\n4250\nmissing\n\n\n11\nAdelie\nTorgersen\n37.8\n17.1\n186\n3300\nmissing\n\n\n12\nAdelie\nTorgersen\n37.8\n17.3\n180\n3700\nmissing\n\n\n13\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nhembra\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n333\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nhembra\n\n\n334\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmacho\n\n\n335\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmacho\n\n\n336\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nhembra\n\n\n337\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmacho\n\n\n338\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nhembra\n\n\n339\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nhembra\n\n\n340\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n341\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n342\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmacho\n\n\n343\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmacho\n\n\n344\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nhembra\n\n\n\n\n\n\n\n\n\nHacer un análisis de los datos perdidos en el data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndescribe(df, :nmissing)\n\n7×2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\nEspecie\n0\n\n\n2\nIsla\n0\n\n\n3\nLongitud_pico\n2\n\n\n4\nProfundidad_pico\n2\n\n\n5\nLongitud_ala\n2\n\n\n6\nPeso\n2\n\n\n7\nSexo\n11\n\n\n\n\n\n\n\n\n\nEliminar del data frame los casos con valores perdidos.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndropmissing!(df)\n\n333×7 DataFrame308 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64\nFloat64\nInt64\nInt64\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmacho\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nhembra\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nhembra\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nhembra\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmacho\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nhembra\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmacho\n\n\n8\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nhembra\n\n\n9\nAdelie\nTorgersen\n38.6\n21.2\n191\n3800\nmacho\n\n\n10\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmacho\n\n\n11\nAdelie\nTorgersen\n36.6\n17.8\n185\n3700\nhembra\n\n\n12\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nhembra\n\n\n13\nAdelie\nTorgersen\n42.5\n20.7\n197\n4500\nmacho\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n322\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nhembra\n\n\n323\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmacho\n\n\n324\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmacho\n\n\n325\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nhembra\n\n\n326\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmacho\n\n\n327\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nhembra\n\n\n328\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nhembra\n\n\n329\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n330\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n331\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmacho\n\n\n332\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmacho\n\n\n333\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nhembra\n\n\n\n\n\n\n\n\n\nCrear diagramas que muestren la distribución de frecuencias de cada variable según la especie de pingüino. ¿Qué variable parece tener más influencia en la especie de pingüino?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\nPara las variables cualitativas dibujamos diagramas de barras.\n\nusing GLMakie, AlgebraOfGraphics\n\nfrec_isla = combine(groupby(df, [:Isla, :Especie]), nrow =&gt; :Frecuencia)\ndata(frec_isla) * \n    mapping(:Isla, :Frecuencia, stack = :Especie, color =:Especie) *\n    visual(BarPlot) |&gt; draw\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\nfrec_sexo = combine(groupby(df, [:Sexo, :Especie]), nrow =&gt; :Frecuencia)\ndata(frec_sexo) * \n    mapping(:Sexo, :Frecuencia, stack = :Especie, color =:Especie) *\n    visual(BarPlot) |&gt; draw\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nPara las variables cuantitativas dibujamos diagramas de cajas.\n\nfunction cajas(df, var, clase)\n    data(df) *\n        mapping(clase, var, color = clase) *\n        visual(BoxPlot) |&gt; \n        draw\nend\n\ncajas(df, :Longitud_pico, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Profundidad_pico, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Longitud_ala, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Peso, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es la reducción de la impureza del conjunto de datos si dividimos el conjunto de datos en dos conjuntos según si la longitud del pico es mayor o menor que 44 mm?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Tidier\nfunction gini(df::DataFrame, var::Symbol)\n    n = nrow(df)\n    frec = combine(groupby(df, var), nrow =&gt; :ni)\n    frec.p = frec.ni ./ n\n    gini = 1 - sum(frec.p .^ 2)\n    return gini\nend\n\nfunction reduccion_impureza(df::DataFrame, var::Symbol, val::Number)\n    # Dividimos el conjunto de ejemplos según la longitud del pico es menor de 44.\n    df_menor = @eval @filter($df, $var &lt;= $val)\n    df_mayor = @eval @filter($df, $var &gt; $val)\n    # Calculamos los tamaños de los subconjuntos de ejemplos.\n    n = nrow(df_menor), nrow(df_mayor)\n    # Calculamos el índice de Gini de cada subconjunto.\n    gis = gini(df_menor, :Especie), gini(df_mayor, :Especie)\n    # Calculamos media ponderada de los índices de Gini de los subconjuntos.\n    g1 = sum(gis .* n) / sum(n)\n    # Calculamos la reducción del índice de Gini.\n    gini(df, :Especie) - g1\nend\n\nreduccion_impureza(df, :Longitud_pico, 44)\n\n0.26577182779353914\n\n\n\n\n\nDeterminar el valor óptimo de división del conjunto de datos según la longitud del pico. Para ello, calcular la reducción de la impureza para cada valor de longitud del pico y dibujar el resultado.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\nDibujamos la reducción de la impureza en función de la longitud del pico.\n\n# Valores únicos de longitud del pico.\nvalores = unique(df.Longitud_pico)\n# Reducción de la impureza para cada valor.\nreducciones = [reduccion_impureza(df, :Longitud_pico, val) for val in valores]\n# Graficamos el resultado.\nusing GLMakie\nfig = Figure()\nax = Axis(fig[1, 1], title = \"Reducción de la impureza según la longitud del pico\", xlabel = \"Longitud del pico\", ylabel = \"Reducción de la impureza\")\nscatter!(ax, valores, reducciones)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\nScatter{Tuple{Vector{Point{2, Float64}}}}\n\n\nY ahora obtenemos el valor óptimo de división del conjunto de datos según la longitud del pico.\n\nval_optimo = valores[argmax(reducciones)]\n\n42.3\n\n\n\n\n\nDividir aleatoriamente el dataframe en un conjunto de entrenamiento y un conjunto de test con proporciones \\(3/4\\) y \\(1/4\\) respectivamente.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función shuffle del paquete Random para barajar el dataframe y luego dividirlo en dos subconjuntos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Random\n# Establecemos la semilla para la reproducibilidad.\nRandom.seed!(1234)\n# Barajamos el dataframe.\ndf = shuffle(df)\n# Dividimos el dataframe en un conjunto de entrenamiento y un conjunto de test.\nn = nrow(df)\ndf_test = df[1:div(n, 4), :]\ndf_train = df[div(n, 4)+1:end, :]\n\n250×7 DataFrame225 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64\nFloat64\nInt64\nInt64\nString7\n\n\n\n\n1\nAdelie\nDream\n39.0\n18.7\n185\n3650\nmacho\n\n\n2\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmacho\n\n\n3\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n4\nAdelie\nTorgersen\n35.1\n19.4\n193\n4200\nmacho\n\n\n5\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmacho\n\n\n6\nGentoo\nBiscoe\n50.0\n15.2\n218\n5700\nmacho\n\n\n7\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmacho\n\n\n8\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n9\nAdelie\nDream\n36.9\n18.6\n189\n3500\nhembra\n\n\n10\nAdelie\nDream\n36.6\n18.4\n184\n3475\nhembra\n\n\n11\nChinstrap\nDream\n46.6\n17.8\n193\n3800\nhembra\n\n\n12\nGentoo\nBiscoe\n50.8\n17.3\n228\n5600\nmacho\n\n\n13\nChinstrap\nDream\n52.2\n18.8\n197\n3450\nmacho\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n239\nAdelie\nDream\n39.8\n19.1\n184\n4650\nmacho\n\n\n240\nAdelie\nTorgersen\n43.1\n19.2\n197\n3500\nmacho\n\n\n241\nChinstrap\nDream\n49.8\n17.3\n198\n3675\nhembra\n\n\n242\nGentoo\nBiscoe\n49.8\n15.9\n229\n5950\nmacho\n\n\n243\nChinstrap\nDream\n50.8\n18.5\n201\n4450\nmacho\n\n\n244\nGentoo\nBiscoe\n50.7\n15.0\n223\n5550\nmacho\n\n\n245\nGentoo\nBiscoe\n46.2\n14.1\n217\n4375\nhembra\n\n\n246\nAdelie\nTorgersen\n35.5\n17.5\n190\n3700\nhembra\n\n\n247\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmacho\n\n\n248\nGentoo\nBiscoe\n47.7\n15.0\n216\n4750\nhembra\n\n\n249\nAdelie\nTorgersen\n42.9\n17.6\n196\n4700\nmacho\n\n\n250\nAdelie\nDream\n40.8\n18.9\n208\n4300\nmacho\n\n\n\n\n\n\n\n\n\nConstruir un árbol de decisión con el conjunto de entrenamiento sin tener en cuenta la variable Isla y visualizarlo.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing DecisionTree, CategoricalArrays\n# Variables predictivas.\nX_train = Matrix(select(df_train, Not(:Isla, :Especie)))\n# Variable objetivo.\ny_train = df_train.Especie\n# Convertir las variables categóricas a enteros.\nX_train = hcat([levelcode.(categorical(X_train[:, j])) for j in 1:size(X_train, 2)]...)\n# Convertir la variable objetivo a enteros\ny_train = levelcode.(categorical(y_train))\n\n# Construimos el árbol de decisión con profundidad máxima 3.\ntree = DecisionTreeClassifier(max_depth = 3)\nfit!(tree, X_train, y_train)\nprint_tree(tree, feature_names=names(df)[3:end])\n\nFeature 3: \"Longitud_ala\" &lt; 29.0 ?\n├─ Feature 1: \"Longitud_pico\" &lt; 62.0 ?\n    ├─ 1 : 96/96\n    └─ Feature 1: \"Longitud_pico\" &lt; 87.0 ?\n        ├─ 2 : 10/20\n        └─ 2 : 37/38\n└─ Feature 2: \"Profundidad_pico\" &lt; 46.0 ?\n    ├─ 3 : 90/90\n    └─ Feature 1: \"Longitud_pico\" &lt; 109.0 ?\n        ├─ 1 : 2/2\n        └─ 2 : 4/4\n\n\n\n\n\nPredecir la especie de los pingüinos del conjunto de test y calcular la matriz de confusión de las predicciones.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función confmat del paquete StatisticalMeaures para barajar el dataframe y luego dividirlo en dos subconjuntos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing StatisticalMeasures\n# Variables predictivas\nX_test = Matrix(select(df_test, Not(:Isla, :Especie)))\n# Variable objetivo\ny_test = df_test.Especie\n# Convertir las variables categóricas a enteros\nX_test = hcat([levelcode.(categorical(X_test[:, j])) for j in 1:size(X_test, 2)]...)\n# Convertir la variable objetivo a enteros\ny_test = levelcode.(categorical(y_test))\n# Predecimos la especie de pingüino del conjunto de test\ny_pred = predict(tree, X_test)\n# Calculamos la precisión del modelo\nconfmat(y_pred, y_test)\n\n          ┌──────────────┐\n          │ Ground Truth │\n┌─────────┼────┬────┬────┤\n│Predicted│ 1  │ 2  │ 3  │\n├─────────┼────┼────┼────┤\n│    1    │ 38 │ 11 │ 9  │\n├─────────┼────┼────┼────┤\n│    2    │ 0  │ 6  │ 0  │\n├─────────┼────┼────┼────┤\n│    3    │ 0  │ 0  │ 19 │\n└─────────┴────┴────┴────┘\n\n\n\n\n\nCalcular la precisión del modelo.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nLa precisión es la proporción de predicciones correctas sobre el total de predicciones.\nUtilizar la función accuracy del paquete StatisticalMeaures para calcular la precisión del modelo.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Calculamos la precisión del modelo\naccuracy(y_pred, y_test)\n\n0.7590361445783133\n\n\n\n\n\n\n\n\nEjercicio 4.3 El fichero vinos.csv contiene información sobre las características de una muestra de vinos portugueses de la denominación “Vinho Verde”. Las variables que contiene son:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nCategórica (blanco, tinto)\n\n\nmeses.barrica\nMesesde envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidadde ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcarremanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufreen formalibre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidadde dióxido de azufretotal en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidadde sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentajede contenidode alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada porun panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un data frame con los datos de los vinos a partir del fichero vinos.csv.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/vinos.csv\"), DataFrame)\n\n5320×14 DataFrame5295 rows omitted\n\n\n\nRow\ntipo\nmeses_barrica\nacided_fija\nacided_volatil\nacido_citrico\nazucar_residual\ncloruro_sodico\ndioxido_azufre_libre\ndioxido_azufre_total\ndensidad\nph\nsulfatos\nalcohol\ncalidad\n\n\n\nString7\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nblanco\n0\n7.0\n0.27\n0.36\n20.7\n0.045\n45.0\n170.0\n1.001\n3.0\n0.45\n8.8\n6\n\n\n2\nblanco\n0\n6.3\n0.3\n0.34\n1.6\n0.049\n14.0\n132.0\n0.994\n3.3\n0.49\n9.5\n6\n\n\n3\nblanco\n0\n8.1\n0.28\n0.4\n6.9\n0.05\n30.0\n97.0\n0.9951\n3.26\n0.44\n10.1\n6\n\n\n4\nblanco\n0\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.4\n9.9\n6\n\n\n5\nblanco\n0\n6.2\n0.32\n0.16\n7.0\n0.045\n30.0\n136.0\n0.9949\n3.18\n0.47\n9.6\n6\n\n\n6\nblanco\n0\n8.1\n0.22\n0.43\n1.5\n0.044\n28.0\n129.0\n0.9938\n3.22\n0.45\n11.0\n6\n\n\n7\nblanco\n0\n8.1\n0.27\n0.41\n1.45\n0.033\n11.0\n63.0\n0.9908\n2.99\n0.56\n12.0\n5\n\n\n8\nblanco\n0\n8.6\n0.23\n0.4\n4.2\n0.035\n17.0\n109.0\n0.9947\n3.14\n0.53\n9.7\n5\n\n\n9\nblanco\n0\n7.9\n0.18\n0.37\n1.2\n0.04\n16.0\n75.0\n0.992\n3.18\n0.63\n10.8\n5\n\n\n10\nblanco\n0\n6.6\n0.16\n0.4\n1.5\n0.044\n48.0\n143.0\n0.9912\n3.54\n0.52\n12.4\n7\n\n\n11\nblanco\n0\n8.3\n0.42\n0.62\n19.25\n0.04\n41.0\n172.0\n1.0002\n2.98\n0.67\n9.7\n5\n\n\n12\nblanco\n0\n6.6\n0.17\n0.38\n1.5\n0.032\n28.0\n112.0\n0.9914\n3.25\n0.55\n11.4\n7\n\n\n13\nblanco\n0\n6.3\n0.48\n0.04\n1.1\n0.046\n30.0\n99.0\n0.9928\n3.24\n0.36\n9.6\n6\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n5309\ntinto\n7\n7.5\n0.31\n0.41\n2.4\n0.065\n34.0\n60.0\n0.99492\n3.34\n0.85\n11.4\n6\n\n\n5310\ntinto\n7\n5.8\n0.61\n0.11\n1.8\n0.066\n18.0\n28.0\n0.99483\n3.55\n0.66\n10.9\n6\n\n\n5311\ntinto\n10\n7.2\n0.66\n0.33\n2.5\n0.068\n34.0\n102.0\n0.99414\n3.27\n0.78\n12.8\n6\n\n\n5312\ntinto\n3\n6.6\n0.725\n0.2\n7.8\n0.073\n29.0\n79.0\n0.9977\n3.29\n0.54\n9.2\n5\n\n\n5313\ntinto\n7\n6.3\n0.55\n0.15\n1.8\n0.077\n26.0\n35.0\n0.99314\n3.32\n0.82\n11.6\n6\n\n\n5314\ntinto\n9\n5.4\n0.74\n0.09\n1.7\n0.089\n16.0\n26.0\n0.99402\n3.67\n0.56\n11.6\n6\n\n\n5315\ntinto\n3\n6.3\n0.51\n0.13\n2.3\n0.076\n29.0\n40.0\n0.99574\n3.42\n0.75\n11.0\n6\n\n\n5316\ntinto\n3\n6.8\n0.62\n0.08\n1.9\n0.068\n28.0\n38.0\n0.99651\n3.42\n0.82\n9.5\n6\n\n\n5317\ntinto\n5\n6.2\n0.6\n0.08\n2.0\n0.09\n32.0\n44.0\n0.9949\n3.45\n0.58\n10.5\n5\n\n\n5318\ntinto\n10\n5.9\n0.55\n0.1\n2.2\n0.062\n39.0\n51.0\n0.99512\n3.52\n0.76\n11.2\n6\n\n\n5319\ntinto\n6\n5.9\n0.645\n0.12\n2.0\n0.075\n32.0\n44.0\n0.99547\n3.57\n0.71\n10.2\n5\n\n\n5320\ntinto\n3\n6.0\n0.31\n0.47\n3.6\n0.067\n18.0\n42.0\n0.99549\n3.39\n0.66\n11.0\n6\n\n\n\n\n\n\n\n\n\nMostrar los tipos de cada variable del data frame.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función schema del paquete MLJ.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing MLJ\nschema(df)\n\nWARNING: using MLJ.fit! in module Main conflicts with an existing identifier.\nWARNING: using MLJ.predict in module Main conflicts with an existing identifier.\n\n\n\n┌──────────────────────┬────────────┬─────────┐\n│ names                │ scitypes   │ types   │\n├──────────────────────┼────────────┼─────────┤\n│ tipo                 │ Textual    │ String7 │\n│ meses_barrica        │ Count      │ Int64   │\n│ acided_fija          │ Continuous │ Float64 │\n│ acided_volatil       │ Continuous │ Float64 │\n│ acido_citrico        │ Continuous │ Float64 │\n│ azucar_residual      │ Continuous │ Float64 │\n│ cloruro_sodico       │ Continuous │ Float64 │\n│ dioxido_azufre_libre │ Continuous │ Float64 │\n│ dioxido_azufre_total │ Continuous │ Float64 │\n│ densidad             │ Continuous │ Float64 │\n│ ph                   │ Continuous │ Float64 │\n│ sulfatos             │ Continuous │ Float64 │\n│ alcohol              │ Continuous │ Float64 │\n│ calidad              │ Count      │ Int64   │\n└──────────────────────┴────────────┴─────────┘\n\n\n\n\n\n\n\nHacer un análisis de los datos perdidos en el data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndescribe(df, :nmissing)\n\n14×2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\ntipo\n0\n\n\n2\nmeses_barrica\n0\n\n\n3\nacided_fija\n0\n\n\n4\nacided_volatil\n0\n\n\n5\nacido_citrico\n0\n\n\n6\nazucar_residual\n0\n\n\n7\ncloruro_sodico\n0\n\n\n8\ndioxido_azufre_libre\n0\n\n\n9\ndioxido_azufre_total\n0\n\n\n10\ndensidad\n0\n\n\n11\nph\n0\n\n\n12\nsulfatos\n0\n\n\n13\nalcohol\n0\n\n\n14\ncalidad\n0\n\n\n\n\n\n\n\n\n\nSe considera que un vino es bueno si tiene una puntuación de calidad mayor que \\(6.5\\). Recodificar la variable calidad en una variable categórica que tome el valor 1 si la calidad es mayor que \\(6.5\\) y 0 en caso contrario.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CategoricalArrays\n# Recodificamos la variable calidad.\ndf.calidad = cut(df.calidad, [0, 6.5, 10], labels = [\" ☹️ \", \" 😊 \"])\n\n5320-element CategoricalArray{String,1,UInt32}:\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" 😊 \"\n \" ☹️ \"\n \" 😊 \"\n \" ☹️ \"\n ⋮\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n\n\n\n\n\nDividir el data frame en un data frame con las variables predictivas y un vector con la variable objetivo bueno.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función unpack del paquete MLJ para dividir el data frame en dos partes, una con las columnas de entrada del modelo y otra con la columna de salida.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ny, X = unpack(df, ==(:calidad), rng = 123)\n\n\n(CategoricalValue{String, UInt32}[\" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" 😊 \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \"  …  \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \"], 5320×13 DataFrame\n  Row │ tipo     meses_barrica  acided_fija  acided_volatil  acido_citrico  az ⋯\n      │ String7  Int64          Float64      Float64         Float64        Fl ⋯\n──────┼─────────────────────────────────────────────────────────────────────────\n    1 │ blanco               0          6.7           0.5             0.36     ⋯\n    2 │ blanco               0          6.3           0.2             0.3\n    3 │ blanco               0          6.2           0.35            0.03\n    4 │ tinto                3          8.0           0.39            0.3\n    5 │ blanco               0          7.9           0.255           0.26     ⋯\n    6 │ blanco               0          6.1           0.31            0.37\n    7 │ blanco               0          6.8           0.28            0.36\n    8 │ blanco               0          8.2           0.34            0.49\n    9 │ tinto                0          6.7           0.48            0.02     ⋯\n   10 │ blanco               0          7.4           0.35            0.2\n   11 │ tinto                5          7.5           0.53            0.06\n  ⋮   │    ⋮           ⋮             ⋮             ⋮               ⋮           ⋱\n 5311 │ blanco               0          7.2           0.14            0.35\n 5312 │ tinto                3          7.6           0.41            0.24     ⋯\n 5313 │ tinto                0          7.3           0.4             0.3\n 5314 │ tinto                4          7.1           0.48            0.28\n 5315 │ blanco               0          6.4           0.29            0.2\n 5316 │ blanco               0          9.4           0.24            0.29     ⋯\n 5317 │ blanco               0          6.3           0.25            0.27\n 5318 │ blanco               0          5.5           0.16            0.26\n 5319 │ blanco               0          7.4           0.36            0.32\n 5320 │ blanco               0          7.6           0.51            0.24     ⋯\n                                                 8 columns and 5299 rows omitted)\n\n\n\n\n\n\nPara poder entrenar un modelo de un arbol de decisión, las variables predictivas deben ser cuantitativas. Transmformar las variables categóricas en variables numéricas.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función coerce! del paquete MLJ para transformar las variables categóricas en variables numéricas.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Convertir las variables categóricas a enteros.\ncoerce!(X, :tipo =&gt; OrderedFactor, :meses_barrica =&gt; Continuous)\nschema(X)\n\n\n┌──────────────────────┬──────────────────┬───────────────────────────────────┐\n│ names                │ scitypes         │ types                             │\n├──────────────────────┼──────────────────┼───────────────────────────────────┤\n│ tipo                 │ OrderedFactor{2} │ CategoricalValue{String7, UInt32} │\n│ meses_barrica        │ Continuous       │ Float64                           │\n│ acided_fija          │ Continuous       │ Float64                           │\n│ acided_volatil       │ Continuous       │ Float64                           │\n│ acido_citrico        │ Continuous       │ Float64                           │\n│ azucar_residual      │ Continuous       │ Float64                           │\n│ cloruro_sodico       │ Continuous       │ Float64                           │\n│ dioxido_azufre_libre │ Continuous       │ Float64                           │\n│ dioxido_azufre_total │ Continuous       │ Float64                           │\n│ densidad             │ Continuous       │ Float64                           │\n│ ph                   │ Continuous       │ Float64                           │\n│ sulfatos             │ Continuous       │ Float64                           │\n│ alcohol              │ Continuous       │ Float64                           │\n└──────────────────────┴──────────────────┴───────────────────────────────────┘\n\n\n\n\n\n\n\nDefinir un modelo de árbol de decisión con profundidad máxima 3.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nCargar el modelo DecisionTreeClassifier del paquete DecisionTree con la macros @iload.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Cargamos el tipo de modelo.\nTree = @iload DecisionTreeClassifier pkg = \"DecisionTree\"\n# Instanciamos el modelo con sus parámetros.\narbol = Tree(max_depth =3, rng = 123)\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n\nDecisionTreeClassifier(\n  max_depth = 3, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = 123)\n\n\n\n\n\nEvaluar el modelo tomando un 70% de ejemplos en el conjunto de entrenamiento y un 30% en el conjunto de test. Utilizar como métrica la precisión.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función evaluate del paquete MLJ para evaluar el modelo. Los parámetros más importantes de esta función son:\n\nresampling: Indica el método de muestreo para definir los conjuntos de entrenamiento y test. Los métodos más habituales son:\n\nHoldout(fraction_train = p): Divide el conjunto de datos tomando una proporción de \\(p\\) ejemplos en el conjunto de entrenamiento y \\(1-p\\) en el conjunto de test.\nCV(nfolds = n, shuffle = true|false): Utiliza validación cruzada con n iteraciones. Si se indica shuffle = true, se utiliza validación cruzada aleatoria.\nStratifiedCV(nfolds = n, shuffle = true|false): Utiliza validación cruzada estratificada con n iteraciones. Si se indica shuffle = true, se utiliza validación cruzada estratificada aleatoria.\nInSample(): Utiliza el conjunto de entrenamiento como conjunto de test.\n\nmeasures: Indica las métricas a utilizar para evaluar el modelo. Las métricas más habituales son:\n\ncross_entropy: Pérdida de entropía cruzada.\nconfusion_matrix: Matriz de confusión.\ntrue_positive_rate: Tasa de verdaderos positivos.\ntrue_negative_rate: Tasa de verdaderos negativos.\nppv: Valor predictivo positivo.\nnpv: Valor predictivo negativo.\naccuracy: Precisión.\n\nSe puede indicar más de una en un vector.\n\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nevaluate(arbol, X, y, resampling = Holdout(fraction_train = 0.7, rng = 123), measures = accuracy)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌────────────┬──────────────┬─────────────┐\n│ measure    │ operation    │ measurement │\n├────────────┼──────────────┼─────────────┤\n│ Accuracy() │ predict_mode │ 0.843       │\n└────────────┴──────────────┴─────────────┘\n\n\n\n\n\n\n\nEvaluar el modelo mediante validación cruzada estratificada usando las métricas de la pérdida de entropía cruzada, la matriz de confusión, la tasa de verdaderos positivos, la tasa de verdaderos negativos, el valor predictivo positivo, el valor predictivo negativo y la precisión. ¿Es un buen modelo?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nevaluate(arbol, X, y, resampling = StratifiedCV(rng = 123), measures = [cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy])\n\nEvaluating over 6 folds:  33%[========&gt;                ]  ETA: 0:00:02Evaluating over 6 folds: 100%[=========================] Time: 0:00:01\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌───┬──────────────────────────┬──────────────┬─────────────────────────────────\n│   │ measure                  │ operation    │ measurement                    ⋯\n├───┼──────────────────────────┼──────────────┼─────────────────────────────────\n│ A │ LogLoss(                 │ predict      │ 0.375                          ⋯\n│   │   tol = 2.22045e-16)     │              │                                ⋯\n│ B │ ConfusionMatrix(         │ predict_mode │ ConfusionMatrix{2}([3821534 78 ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   perm = nothing,        │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ C │ TruePositiveRate(        │ predict_mode │ 0.128                          ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ D │ TrueNegativeRate(        │ predict_mode │ 1.0                            ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ E │ PositivePredictiveValue( │ predict_mode │ 0.994                          ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ F │ NegativePredictiveValue( │ predict_mode │ 0.83                           ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│ ⋮ │            ⋮             │      ⋮       │                        ⋮       ⋱\n└───┴──────────────────────────┴──────────────┴─────────────────────────────────\n                                                     1 column and 2 rows omitted\n┌───┬───────────────────────────────────────────────────────────────────────────\n│   │ per_fold                                                                 ⋯\n├───┼───────────────────────────────────────────────────────────────────────────\n│ A │ [0.391, 0.394, 0.35, 0.358, 0.365, 0.391]                                ⋯\n│ B │ ConfusionMatrix{2, true, CategoricalValue{String, UInt32}}[ConfusionMatr ⋯\n│ C │ [0.125, 0.167, 0.155, 0.112, 0.113, 0.0952]                              ⋯\n│ D │ [1.0, 0.999, 1.0, 1.0, 1.0, 1.0]                                         ⋯\n│ E │ [1.0, 0.966, 1.0, 1.0, 1.0, 1.0]                                         ⋯\n│ F │ [0.83, 0.837, 0.835, 0.827, 0.828, 0.825]                                ⋯\n│ G │ [0.834, 0.841, 0.84, 0.831, 0.832, 0.828]                                ⋯\n└───┴───────────────────────────────────────────────────────────────────────────\n                                                               2 columns omitted\n\n\n\n\nLa precisión del modelo es de \\(0.834\\) que no está mal, pero si consdieramos la tasa de verdadero positivos, que es \\(0.13\\) y la tasa de verdaderos negativos, que es prácticamente 1, el modelo tiene un buen rendimiento en la clasificación de los vinos malos, pero un mal rendimiento en la clasificación de los vinos buenos. Por lo tanto, no podemos decir que sea un buen modelo.\n\n\n\nConstruir árboles de decisión con profundidades máximas de 2 a 10 y evaluar el modelo con validación cruzada estratificada. ¿Cuál es la profundidad máxima que da mejor resultado?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función TunedModel del paquete MLJ para ajustar los parámetros del modelo.\nLos parámetros más importantes de esta función son:\n\nmodel: Indica el modelo a ajustar.\nresampling: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.\ntuning: Indica el método de ajuste de los parámetros del modelo. Los métodos más habituales son:\n\nGrid(resolution = n): Ajusta los parámetros del modelo utilizando una cuadrícula de búsqueda con n valores.\nRandomSearch(resolution = n): Ajusta los parámetros del modelo utilizando una búsqueda aleatoria con n valores.\n\nrange: Indica el rango de valores a utilizar para ajustar los parámetros del modelo. Se puede indicar un rango de valores o un vector de valores.\nmeasure: Indica la métrica a utilizar para evaluar el modelo.\n\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Instanciamos el modelo de árbol de decisión.\narbol = Tree()\n# Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.\nr = range(arbol, :max_depth, lower=2, upper=10)\n# Ajustamos los parámetros del modelo utilizando una cuadrícula de búsqueda con 9 valores.\narbol_parametrizado = TunedModel(\n    model = arbol,\n    resampling = StratifiedCV(rng = 123),\n    tuning = Grid(resolution = 9),\n    range = r,\n    measure = accuracy)\n# Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol_parametrizado, X, y)\n# Ajustamos los parámetros del modelo.\nMLJ.fit!(mach)\n# Mostramos los parámetros del mejor modelo.\nfitted_params(mach).best_model\n\n[ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).\n[ Info: Attempting to evaluate 9 models.\nEvaluating over 9 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 9 metamodels:  11%[==&gt;                      ]  ETA: 0:00:03Evaluating over 9 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:03Evaluating over 9 metamodels:  33%[========&gt;                ]  ETA: 0:00:02Evaluating over 9 metamodels:  44%[===========&gt;             ]  ETA: 0:00:01Evaluating over 9 metamodels:  56%[=============&gt;           ]  ETA: 0:00:01Evaluating over 9 metamodels:  67%[================&gt;        ]  ETA: 0:00:01Evaluating over 9 metamodels:  78%[===================&gt;     ]  ETA: 0:00:01Evaluating over 9 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 9 metamodels: 100%[=========================] Time: 0:00:02\n\n\nDecisionTreeClassifier(\n  max_depth = 5, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = TaskLocalRNG())\n\n\n\n\n\nDibujar la curva de aprendizaje del modelo en función de la profundidad del árbol de decisión.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función learning_curve del paquete MLJ para dibujar la curva de aprendizaje. Los parámetros más importantes de esta función son:\n\nmach: Indica la máquina de aprendizaje a utilizar.\nrange: Indica el rango de valores a utilizar para ajustar los parámetros del modelo.\nresampling: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.\nmeasure: Indica la métrica a utilizar para evaluar el modelo.\nrngs: Indica la semilla para la generación de números aleatorios. Se pueden indicar varias semillas en un vector y se genera una curva de aprendizaje para cada semilla.\n\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Instanciamos el modelo de árbol de decisión.\narbol = Tree()\n# Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol, X, y)\n# Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.\nr = range(arbol, :max_depth, lower=2, upper=10)\n# Dibujamos la curva de aprendizaje.\ncurva = learning_curve(mach, range = r, resampling = StratifiedCV(rng = 123), measure = accuracy)\n# Dibujamos la curva de aprendizaje.\nfig = Figure()\nax = Axis(fig[1, 1], title = \"Curva de aprendizaje\", xlabel = \"Profundidad del árbol\", ylabel = \"Precisión\")\nMakie.scatter!(ax, curva.parameter_values, curva.measurements)\nfig\n\n[ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).\n[ Info: Attempting to evaluate 9 models.\nEvaluating over 9 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:01Evaluating over 9 metamodels:  33%[========&gt;                ]  ETA: 0:00:01Evaluating over 9 metamodels:  44%[===========&gt;             ]  ETA: 0:00:01Evaluating over 9 metamodels:  56%[=============&gt;           ]  ETA: 0:00:01Evaluating over 9 metamodels:  67%[================&gt;        ]  ETA: 0:00:01Evaluating over 9 metamodels:  78%[===================&gt;     ]  ETA: 0:00:00Evaluating over 9 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 9 metamodels: 100%[=========================] Time: 0:00:01\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\n\nConstruir un árbol de decisión con la profundidad máxima que da mejor resultado y visualizarlo.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Instanciamos el modelo de árbol de decisión.\narbol = Tree(max_depth = 4)\n# Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol, X, y)\n# Ajustamos los parámetros del modelo.\nMLJ.fit!(mach)\n# Visualizamos el árbol de decisión.\nfitted_params(mach).tree\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = 4, …), …).\n\n\nalcohol &lt; 10.62\n├─ meses_barrica &lt; 8.5\n│  ├─ acided_volatil &lt; 0.3125\n│  │  ├─ acided_volatil &lt; 0.2025\n│  │  │  ├─  ☹️  (408/496)\n│  │  │  └─  ☹️  (1095/1172)\n│  │  └─ meses_barrica &lt; 5.5\n│  │     ├─  ☹️  (1334/1345)\n│  │     └─  ☹️  (51/58)\n│  └─  😊  (25/25)\n└─ meses_barrica &lt; 12.5\n   ├─ cloruro_sodico &lt; 0.0455\n   │  ├─ alcohol &lt; 12.55\n   │  │  ├─  ☹️  (751/1160)\n   │  │  └─  😊  (185/286)\n   │  └─ meses_barrica &lt; 10.5\n   │     ├─  ☹️  (552/629)\n   │     └─  😊  (25/43)\n   └─ alcohol &lt; 14.45\n      ├─  😊  (105/105)\n      └─  ☹️  (1/1)\n\n\n\n\n\n¿Cuál es la importancia de cada variable en el modelo?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función feature_importances del paquete DecisionTree para calcular la importancia de cada variable.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Calculamos la importancia de cada variable.\nfeature_importances(mach)\n\n13-element Vector{Pair{Symbol, Float64}}:\n              :alcohol =&gt; 0.5303315899204789\n        :meses_barrica =&gt; 0.26854115615561525\n       :acided_volatil =&gt; 0.1040970236546446\n       :cloruro_sodico =&gt; 0.09703023026926123\n                 :tipo =&gt; 0.0\n          :acided_fija =&gt; 0.0\n        :acido_citrico =&gt; 0.0\n      :azucar_residual =&gt; 0.0\n :dioxido_azufre_libre =&gt; 0.0\n :dioxido_azufre_total =&gt; 0.0\n             :densidad =&gt; 0.0\n                   :ph =&gt; 0.0\n             :sulfatos =&gt; 0.0\n\n\n\n\n\nPredecir la calidad de los 10 primeros vinos del conjunto de ejemplos.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función predict del paquete DecisionTree para predecir las probabilidades de pertenecer a cada clase un ejemplo o conjunto de ejemplos.\nUsar la función predict_mode del paquete DecisionTree para predecir la clase de un ejemplo o conjunto de ejemplos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\nPrimero calculamos las probabilidades de cada clase.\n\nMLJ.predict(mach, X[1:10, :])\n\n10-element CategoricalDistributions.UnivariateFiniteVector{OrderedFactor{2}, String, UInt32, Float64}:\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.823,  😊 =&gt;0.177)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.647,  😊 =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.647,  😊 =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.647,  😊 =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.878,  😊 =&gt;0.122)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n\n\nY ahora predecimos la clase.\n\npredict_mode(mach, X[1:10, :])\n\n10-element CategoricalArray{String,1,UInt32}:\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Árboles de decisión</span>"
    ]
  },
  {
    "objectID": "09-redes-neuronales-prealimentadas.html",
    "href": "09-redes-neuronales-prealimentadas.html",
    "title": "5  Redes de neuronas artificiales",
    "section": "",
    "text": "5.1 Ejercicios Resueltos\nLas redes de neuronas artificiales son un modelo computacional inspirado en el funcionamiento del cerebro humano. Una neurona artificial es una unidad de cómputo bastante simple, que recibe una serie de entradas, las procesa y produce una salida. La salida de una neurona puede ser la entrada de otra neurona, formando así una red de neuronas interconectadas, donde cada conexión tiene un peso asociado. Es esta red, que a veces contiene miles y millones de neuronas, la que dota de gran potencia de cálculo a este modelo, siendo capaces de aprender patrones de datos muy complejos, como imágenes, texto o sonido, y por tanto, se utilizan a menudo en tareas de clasificación o regresión.\nEl aprendizaje en una red neuronal consiste en ajustar los pesos de las conexiones para minimizar el error entre la salida predicha y la salida real. Este proceso se realiza mediante algoritmos de optimización, como el del gradiente descendente que ya se vio en el capítulo de regresión.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Redes de neuronas artificiales</span>"
    ]
  },
  {
    "objectID": "09-redes-neuronales-prealimentadas.html#ejercicios-resueltos",
    "href": "09-redes-neuronales-prealimentadas.html#ejercicios-resueltos",
    "title": "5  Redes de neuronas artificiales",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing GLMakie  # Para el dibujo de gráficas.\nusing MLJ # Para la creación y entrenamiento de modelos de aprendizaje automático.\nusing Flux # Para la creación y entrenamiento de redes neuronales.\nusing MLJFlux # Interfaz de Flux para MLJ.\nusing Optimisers # Para la optimización de funciones.\nusing Statistics # Para las funciones de coste.\n\nEjercicio 5.1 El conjunto de datos viviendas.csv contiene información sobre el precio de venta de viviendas en una ciudad.\n\nCargar los datos del archivo viviendas.csv en un data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames\n# Creamos un data frame a partir del archivo CSV.\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/viviendas.csv\"), DataFrame)\n# Mostramos las primeras cinco filas del data frame.\nfirst(df, 5)\n\n5×13 DataFrame\n\n\n\nRow\nprecio\narea\ndormitorios\nbaños\nhabitaciones\ncalleprincipal\nhuespedes\nsotano\ncalentador\nclimatizacion\ngaraje\ncentrico\namueblado\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nString3\nString3\nString3\nString3\nString3\nInt64\nString3\nString15\n\n\n\n\n1\n13300000\n7420\n4\n2\n3\nsi\nno\nno\nno\nsi\n2\nsi\namueblado\n\n\n2\n12250000\n8960\n4\n4\n4\nsi\nno\nno\nno\nsi\n3\nno\namueblado\n\n\n3\n12250000\n9960\n3\n2\n2\nsi\nno\nsi\nno\nno\n2\nsi\nsemi-amueblado\n\n\n4\n12215000\n7500\n4\n2\n2\nsi\nno\nsi\nno\nsi\n3\nsi\namueblado\n\n\n5\n11410000\n7420\n4\n1\n2\nsi\nsi\nsi\nno\nsi\n2\nno\namueblado\n\n\n\n\n\n\n\n\n\nExtraer las columnas area y precio del data frame y convertirlas a un vector de tipo Float32. Pasar el precio a miles de euros.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Extraemos las columnas area y precio.\nX = Float32.(df.area)\ny = Float32.(df.precio) ./ 1000\n\n545-element Vector{Float32}:\n 13300.0\n 12250.0\n 12250.0\n 12215.0\n 11410.0\n 10850.0\n 10150.0\n 10150.0\n  9870.0\n  9800.0\n  9800.0\n  9681.0\n  9310.0\n     ⋮\n  2100.0\n  2100.0\n  2100.0\n  1960.0\n  1890.0\n  1890.0\n  1855.0\n  1820.0\n  1767.15\n  1750.0\n  1750.0\n  1750.0\n\n\n\n\n\nDibujar un diagrama de dispersión entre el precio y el area de las viviendas.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing GLMakie\nfig = Figure()\nax = Axis(fig[1, 1], title = \"Precio vs Area\", xlabel = \"Área (m²)\", ylabel = \"Precio (€)\")\nscatter!(ax, X, y)\nfig\n\n\n\n\n\n\n\nConstruir un modelo lineal simple usando un perceptrón como el de la figura, tomando como función de activación la función identidad.\n.\nInicializar los parámetros del modelo a 0 y dibujarlo en el diagrama de dispersión.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos el modelo lineal.\nperceptron(W, b, x) = @. W[1] * x + b[1]\n# Inicializamos los pesos y el término independiente.\nW = Float32[0]\nb = Float32[0]\nlines!(ax, X, perceptron(W, b, X), label = \"Modelo 0\", color = :red)\nfig\n\n\n\n\n\n\n\nAplicar el modelo a los datos y calcular el error cuadrático medio del modelo.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Statistics\n# Definimos la función de coste.\ncoste(W, b, X, y) = mean((y .- perceptron(W, b, X)).^2)\n# Calculamos el coste del modelo inicial.\nprintln(\"Error cuadrático medio: \", coste(W, b, X, y))\n\nError cuadrático medio: 2.6213836e7\n\n\nSe observa que el error cuadrático medio es bastante alto, lo que indica que el modelo no se ajusta bien a los datos.\n\n\n\n¿En qué dirección deben modificarse los parámetros del modelo para reducir el error cuadrático medio? Actualizar los parámetros del modelo en esa dirección utilizando una tasa de aprendizaje \\(\\eta = 10^{-9}\\). Comprobar que el error cuadrático medio ha disminuido con los nuevos parámetros.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nLos parámetros deben modificarse en la dirección en la que más rápidamente decrezca el error cuadrático medio. Esta dirección está dada por el gradiente del error cuadrático respecto a los parámetros, que se puede calcular como:\n\\[\n\\nabla E(W, b) = \\left( \\frac{\\partial E}{\\partial W}, \\frac{\\partial E}{\\partial b} \\right).\n\\]\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Flux\n# Declaramos los pesos como variables simbólicas\n\n# Calculamos el gradiente del coste.\n∂E_∂W, ∂E_∂b = gradient(coste, W, b, X, y)\n# Mostramos el gradiente.\nprintln(\"Gradiente del coste: ($∂E_∂W, $∂E_∂b)\")\n# Definimos la tasa de aprendizaje.\nη = 1e-8\n# Mostramos los parámetros iniciales.\nprintln(\"Parámetros iniciales: W = $W, b = $b\")\n# Actualizamos los parámetros en la dirección de\nW -= η * ∂E_∂W\nb -= η * ∂E_∂b\n# Mostramos los nuevos parámetros.\nprintln(\"Nuevos parámetros: W = $W, b = $b\")\n# Comprobamos que el error cuadrático medio ha disminuido.\nprintln(\"Error cuadrático medio: \", coste(W, b, X, y))\n# Dibujamos el nuevo modelo en el diagrama de dispersión.\nlines!(ax, X, perceptron(W, b, X), label = \"Modelo 1\")\nfig\n\nGradiente del coste: (Float32[-5.344584f7], Float32[-9533.46])\nParámetros iniciales: W = Float32[0.0], b = Float32[0.0]\nNuevos parámetros: W = [0.5344584], b = [9.5334599609375e-5]\nError cuadrático medio: 6.569670916877353e6\n\n\n\n\n\n\n\n\nDefinir una función para entrenar el perceptrón modificando los pesos en la dirección opuesta al gradiente del error cuadrático medio.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction entrenar_perceptron!(W, b, X, y, η)\n    \"\"\"\n    Función para entrenar el perceptrón.\n    W: peso del modelo.\n    b: término independiente del modelo.\n    X: vector de entradas.\n    y: vector de salidas.\n    η: tasa de aprendizaje.\n    \"\"\"\n    # Calculamos el gradiente del coste.\n    ∂E_∂W, ∂E_∂b = gradient(coste, W, b, X, y)\n    # Actualizamos los parámetros en la dirección opuesta al gradiente.\n    W .-= η * ∂E_∂W\n    b .-= η * ∂E_∂b\nend\n\nentrenar_perceptron! (generic function with 1 method)\n\n\n\n\n\nUsar la función anterior para entrenar el perceptrón y repetir el proceso durante 9 iteraciones más. Dibujar los modelos actualizados en el diagrama de dispersión.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Repetimos el proceso de entrenamiento del perceptrón.\nfor i = 2:10\n    entrenar_perceptron!(W, b, X, y, η)\n    # Mostramos los parámetros y el coste del modelo.\n    ecm = coste(W, b, X, y)\n    println(\"Iteración \", i, \", Parámetros: W = $W, b = $b, Coste: $ecm\")\n    # Dibujamos el modelo actualizado en el diagrama de dispersión.\n    lines!(ax, X, perceptron(W, b, X), label = \"Modelo $i\")\nend\naxislegend(ax)\nfig\n\nIteración 2, Parámetros: W = [0.7351053379977437], b = [0.00013561418157921425], Coste: 3.8010036630367776e6\nIteración 3, Parámetros: W = [0.8104324229132014], b = [0.0001552249559885306], Coste: 3.4107850089995693e6\nIteración 4, Parámetros: W = [0.838711796053412], b = [0.00016707622479181458], Coste: 3.355787208775712e6\nIteración 5, Parámetros: W = [0.8493284674839952], b = [0.00017601441178095897], Coste: 3.348035761002203e6\nIteración 6, Parámetros: W = [0.8533141887596891], b = [0.00018385896650121626], Coste: 3.3469432599931033e6\nIteración 7, Parámetros: W = [0.8548105117157216], b = [0.00019129294862501068], Coste: 3.34678927740313e6\nIteración 8, Parámetros: W = [0.8553722621219417], b = [0.00019857279313692855], Coste: 3.3467675705099306e6\nIteración 9, Parámetros: W = [0.855583154313161], b = [0.00020579477113007419], Coste: 3.3467645066817994e6\nIteración 10, Parámetros: W = [0.855662326942257], b = [0.00021299502480003158], Coste: 3.3467640704253544e6\n\n\n\n\n\n\n\n\nDefinir de nuevo el perceptrón como una red neuronal de una sola capa con una entrada y una salida con el paquete Flux.jl y mostrar los parámetros iniciales del modelo.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUtilizar la función Dense(n =&gt; m) del paquete Flux.jl para definir una capa de \\(m\\) neuronas con \\(n\\) entradas cada una. Por defecto la función de activación es la identidad.\nFlux inicializa los pesos de las conexiones de forma aleatoria y el término independiente a cero.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos una capa con una sola neurona con una entrada.\nmodelo = Dense(1 =&gt; 1)\n# Mostramos los parámetros del modelo.\nprintln(\"Pesos: \", modelo.weight, \", Término independiente: \", modelo.bias)\n\nPesos: Float32[-0.575185;;], Término independiente: Float32[0.0]\n\n\n\n\n\nCalcular las predicciones de los precios de las viviendas con el modelo inicial.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nPara obtener las salidas de una red neuronal definida con Flux, debe pasarse al modelo una matriz con las entradas, donde cada columna es un caso. Para convertir el vector de las areas en una matriz de una sola fila se puede usar la función reshape.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Convertimos el vector de las areas en una matriz de una sola fila.\nX = reshape(X, 1, length(X))\nŷ = modelo(X)\n\n1×545 Matrix{Float32}:\n -4267.87  -5153.66  -5728.84  -4313.89  …  -2082.17  -1673.79  -2214.46\n\n\n\n\n\nDefinir una función de coste que calcule el error cuadrático medio entre la salida del modelo y la salida real usando el paquete Flux.jl. Calcular el coste del modelo inicial.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función Flux.mse para calcular el error cuadrático medio entre la salida del modelo y la salida real.\nPara calcular el coste de un modelo en Flux, el vector con las etiquetas también debe ser una matriz de una fila.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Convertimos el vector de los precios en una matriz de una sola columna.\ny = reshape(y, 1, length(y))\n# Definimos la función de coste como el error cuadrático medio.\ncoste(modelo, X, y) = Flux.mse(modelo(X),y)\ncoste(modelo, X, y)\n\n6.728679f7\n\n\n\n\n\nDefinir una función para entrenar el modelo con y usarla para entrenar el modelo hasta que la reducción en el error cuadrático medio sea menor del \\(0.01\\)%. ¿Cuántas iteraciones hacen falta? ¿Cuál es el coste del último modelo? Dibujar el coste en cada iteración.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction entrenar_modelo!(modelo, coste, X, y, η)\n    \"\"\"\n    Función para entrenar el modelo.\n    modelo: modelo a entrenar.\n    coste: función de coste.\n    X: matriz de entradas.\n    y: matriz de salidas.\n    η: tasa de aprendizaje.\n    \"\"\"\n    # Calculamos el gradiente del coste.\n    ∇ = gradient(coste, modelo, X, y)\n    # Actualizamos los parámetros del modelo en la dirección opuesta al gradiente.\n    @. modelo.weight = modelo.weight - η * ∇[1].weight\n    @. modelo.bias = modelo.bias - η * ∇[1].bias\nend\n# Creamos un vector para guardar los costes del proceso de entrenamiento.\ncostes = [coste(modelo, X, y)]\nreduccion_coste = Inf\niteraciones = 0\n# Iteramos el proceso de aprendizaje hasta que la reducción del coste sea menor del 0.01%.\nwhile reduccion_coste &gt; 0.0001\n    iteraciones += 1\n    entrenar_modelo!(modelo, coste, X, y, η)\n    # Calculamos el nuevo coste y lo añadimos al vector de costes.\n    push!(costes, coste(modelo, X, y))\n    # Calculamos la reducción del coste.\n    reduccion_coste = abs((costes[end] - costes[end-1]) / costes[end])\nend\n# Mostramos el número de iteraciones y el coste final.\nprintln(\"Número de iteraciones: \", iteraciones)\nprintln(\"Coste final: \", costes[end])\n# Dibujamos el coste en cada iteración.\nfig2 = Figure()\nax2 = Axis(fig2[1, 1], title = \"Evolución del coste en el entrenamiento\", xlabel = \"Iteraciones\", ylabel = \"Coste\")\nlines!(ax2, 0:iteraciones, costes)\nfig2\n\nNúmero de iteraciones: 8\nCoste final: 3.3467735e6\n\n\n\n\n\n\n\n\nMostrar el modelo final en el diagrama de dispersión y compararlo con el último modelo obtenido con el perceptrón.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Dibujamos el modelo final en el diagrama de dispersión.\nlines!(ax, vec(X), vec(modelo(X)), label = \"Modelo final\", color = :red, linewidth = 2)\nfig\n\n\n\n\nSe observa que el modelo final prácticamente coincide con el último modelo obtenido con el perceptrón, lo que indica que ambos modelos son equivalentes.\n\n\n\nCrear una red neuronal para predecir el precio de las viviendas usando todas las características de las viviendas y mostrar los pesos y el término independiente del modelo.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nAhora el modelo tiene que tener tantas entradas como características tenga el conjunto de datos, en este caso 12 características de entrada y una salida para el precio.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos el modelo con 12 entradas y 1 salida.\nmodelo = Dense(12 =&gt; 1)\n# Mostramos los parámetros del modelo.\nprintln(\"Pesos: \", modelo.weight, \", Término independiente: \", modelo.bias)\n\nPesos: Float32[0.025629558 0.4026789 -0.02796562 0.3496065 -0.1496501 -0.093475975 0.31590047 -0.44209063 0.6066199 -0.05232798 -0.022571983 -0.56505185], Término independiente: Float32[0.0]\n\n\n\n\n\nExtraer las características de entrada del conjunto de datos y convertir las que sean cualitativas en cuantitativas.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función coerce! del paquete MLJScientificTypes.jl para convertir los tipos de las columnas del data frame.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing MLJ\n# Extraemos las etiquetas.\ny = Float32.(df.precio / 1000)\n# Extraemos las características de entrada.\nX = select(df, Not(:precio))\n# Convertimos las columnas cualitativas en cuantitativas.\nschema(X)\n\n\n┌────────────────┬──────────┬──────────┐\n│ names          │ scitypes │ types    │\n├────────────────┼──────────┼──────────┤\n│ area           │ Count    │ Int64    │\n│ dormitorios    │ Count    │ Int64    │\n│ baños          │ Count    │ Int64    │\n│ habitaciones   │ Count    │ Int64    │\n│ calleprincipal │ Textual  │ String3  │\n│ huespedes      │ Textual  │ String3  │\n│ sotano         │ Textual  │ String3  │\n│ calentador     │ Textual  │ String3  │\n│ climatizacion  │ Textual  │ String3  │\n│ garaje         │ Count    │ Int64    │\n│ centrico       │ Textual  │ String3  │\n│ amueblado      │ Textual  │ String15 │\n└────────────────┴──────────┴──────────┘\n\n\n\n\nLas columnas calleprincipal, huespedes, sotano, calentador, climatizacion, centrico y amueblado son cualitativas y deben convertirse a cuantitativas.\n\n# Convertimos las columnas de tipo texto a tipo categórico.\ncoerce!(X, Textual =&gt; Multiclass)\n# Convertimos las columnas categóricas a tipo numérico.\ncoerce!(X, Multiclass =&gt; Count)\n# Convertimos las columnas de tipo Int64 a tipo Int32 para ganar eficiencia.\nX = Float32.(X)\n# Observamos el nuevo esquema del data frame.\nschema(X)\n\n\n┌────────────────┬────────────┬─────────┐\n│ names          │ scitypes   │ types   │\n├────────────────┼────────────┼─────────┤\n│ area           │ Continuous │ Float32 │\n│ dormitorios    │ Continuous │ Float32 │\n│ baños          │ Continuous │ Float32 │\n│ habitaciones   │ Continuous │ Float32 │\n│ calleprincipal │ Continuous │ Float32 │\n│ huespedes      │ Continuous │ Float32 │\n│ sotano         │ Continuous │ Float32 │\n│ calentador     │ Continuous │ Float32 │\n│ climatizacion  │ Continuous │ Float32 │\n│ garaje         │ Continuous │ Float32 │\n│ centrico       │ Continuous │ Float32 │\n│ amueblado      │ Continuous │ Float32 │\n└────────────────┴────────────┴─────────┘\n\n\n\n\n\n\n\nConvertir el data frame a una matriz, transponerla y normalizar los datos.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nCuando se trabaja con variables de entrada de diferentes escalas, es recomendable normalizarlas para que todas tengan la misma importancia en el modelo. Para ello, se puede usar la función normalise del paquete Flux.jl para normalizar los datos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Convertimos el data frame a una matriz y la transponemos.\nX = Matrix(X)'\nX = Flux.normalise(X)\ny = y'\n# Definimos como función de coste el error cuadrático medio.\ncoste(modelo, X, y) = Flux.mse(modelo(X), y)\n# Calculamos el coste del modelo inicial.\nprintln(\"Error cuadrático medio: \", coste(modelo, X, y))\n\nError cuadrático medio: 2.621182e7\n\n\n\n\n\nDefinir una función para entrenar el modelo con y usarla para entrenar el modelo hasta que la reducción en el error cuadrático medio sea menor de \\(10^{-4}\\). ¿Cuántas iteraciones hacen falta? ¿Cuál es el coste del último modelo? Dibujar el coste en cada iteración. ¿Es mejor modelo que el percetrón?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction entrenar_modelo!(modelo, coste, X, y, η)\n    \"\"\"\n    Función para entrenar el modelo.\n    modelo: modelo a entrenar.\n    coste: función de coste.\n    X: matriz de entradas.\n    y: matriz de salidas.\n    η: tasa de aprendizaje.\n    \"\"\"\n    # Calculamos el gradiente del coste.\n    ∇ = gradient(coste, modelo, X, y)\n    # Actualizamos los parámetros del modelo en la dirección opuesta al gradiente.\n    @. modelo.weight = modelo.weight - η * ∇[1].weight\n    @. modelo.bias = modelo.bias - η * ∇[1].bias\nend\n# Definimos la tasa de aprendizaje.\nη = 1e-2\n# Creamos un vector para guardar los costes del proceso de entrenamiento.\ncostes = [coste(modelo, X, y)]\nreduccion_coste = Inf\niteraciones = 0\n# Iteramos el proceso de aprendizaje hasta que la reducción del coste sea menor del 0.01%.\nwhile reduccion_coste &gt; 1e-4\n    iteraciones += 1\n    entrenar_modelo!(modelo, coste, X, y, η)\n    # Calculamos el nuevo coste y lo añadimos al vector de costes.\n    push!(costes, coste(modelo, X, y))\n    # Calculamos la reducción del coste.\n    reduccion_coste = abs((costes[end] - costes[end-1]))\nend\n# Mostramos el número de iteraciones y el coste final.\nprintln(\"Número de iteraciones: \", iteraciones)\nprintln(\"Coste final: \", costes[end])\n\nNúmero de iteraciones: 409\nCoste final: 1.1411426e6\n\n\nAhora dibujamos la evolución del coste con las iteraciones en el entrenamiento.\n\n# Dibujamos el coste en cada iteración.\nfig3 = Figure()\nax3 = Axis(fig3[1, 1], title = \"Evolución del coste en el entrenamiento\", xlabel = \"Iteraciones\", ylabel = \"Coste\")\nlines!(ax3, 0:iteraciones, costes)\nfig3\n\n\n\n\nEl coste final es menor que el del perceptrón, lo que indica que el modelo es mejor.\nFinalmente, mostramos los parámetros del modelo entrenado.\n\n# Mostramos los pesos y el término independiente del modelo.\nprintln(\"Pesos: \", modelo.weight, \", Término independiente: \", modelo.bias)\n\nPesos: Float32[536.259 98.03261 504.90298 392.61963 164.35243 123.36959 179.14948 187.50192 411.0921 257.42072 279.6837 10.9778185], Término independiente: Float32[4765.4995]\n\n\n\n\n\n\n\n\nEjercicio 5.2 El conjunto de datos pingüinos.csv contiene un conjunto de datos sobre tres especies de pingüinos con las siguientes variables:\n\nEspecie: Especie de pingüino, comúnmente Adelie, Chinstrap o Gentoo.\nIsla: Isla del archipiélago Palmer donde se realizó la observación.\nLongitud_pico: Longitud del pico en mm.\nProfundidad_pico: Profundidad del pico en mm\nLongitud_ala: Longitud de la aleta en mm.\nPeso: Masa corporal en gramos.\nSexo: Sexo\n\n\nCargar los datos del archivo pinguïnos.csv en un data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/pingüinos.csv\"), DataFrame, missingstring=\"NA\")\nfirst(df, 5)\n\n5×7 DataFrame\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64?\nFloat64?\nInt64?\nInt64?\nString7?\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmacho\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nhembra\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nhembra\n\n\n4\nAdelie\nTorgersen\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nhembra\n\n\n\n\n\n\n\n\n\nSeleccionar las columnas Longitud_pico, Profundidad_pico, Longitud_ala y Especie del data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Seleccionamos las columnas Longitud_pico, Profundidad_pico, Longitud_ala y Especie.\nselect!(df, [:Longitud_pico, :Profundidad_pico, :Longitud_ala, :Especie])\nfirst(df, 5)\n\n5×4 DataFrame\n\n\n\nRow\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nEspecie\n\n\n\nFloat64?\nFloat64?\nInt64?\nString15\n\n\n\n\n1\n39.1\n18.7\n181\nAdelie\n\n\n2\n39.5\n17.4\n186\nAdelie\n\n\n3\n40.3\n18.0\n195\nAdelie\n\n\n4\nmissing\nmissing\nmissing\nAdelie\n\n\n5\n36.7\n19.3\n193\nAdelie\n\n\n\n\n\n\n\n\n\nHacer un análisis de los datos perdidos en el data frame.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndescribe(df, :nmissing)\n\n4×2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\nLongitud_pico\n2\n\n\n2\nProfundidad_pico\n2\n\n\n3\nLongitud_ala\n2\n\n\n4\nEspecie\n0\n\n\n\n\n\n\n\n\n\nEliminar del data frame los casos con valores perdidos.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ndropmissing!(df)\n\n342×4 DataFrame317 rows omitted\n\n\n\nRow\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nEspecie\n\n\n\nFloat64\nFloat64\nInt64\nString15\n\n\n\n\n1\n39.1\n18.7\n181\nAdelie\n\n\n2\n39.5\n17.4\n186\nAdelie\n\n\n3\n40.3\n18.0\n195\nAdelie\n\n\n4\n36.7\n19.3\n193\nAdelie\n\n\n5\n39.3\n20.6\n190\nAdelie\n\n\n6\n38.9\n17.8\n181\nAdelie\n\n\n7\n39.2\n19.6\n195\nAdelie\n\n\n8\n34.1\n18.1\n193\nAdelie\n\n\n9\n42.0\n20.2\n190\nAdelie\n\n\n10\n37.8\n17.1\n186\nAdelie\n\n\n11\n37.8\n17.3\n180\nAdelie\n\n\n12\n41.1\n17.6\n182\nAdelie\n\n\n13\n38.6\n21.2\n191\nAdelie\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n331\n45.2\n16.6\n191\nChinstrap\n\n\n332\n49.3\n19.9\n203\nChinstrap\n\n\n333\n50.2\n18.8\n202\nChinstrap\n\n\n334\n45.6\n19.4\n194\nChinstrap\n\n\n335\n51.9\n19.5\n206\nChinstrap\n\n\n336\n46.8\n16.5\n189\nChinstrap\n\n\n337\n45.7\n17.0\n195\nChinstrap\n\n\n338\n55.8\n19.8\n207\nChinstrap\n\n\n339\n43.5\n18.1\n202\nChinstrap\n\n\n340\n49.6\n18.2\n193\nChinstrap\n\n\n341\n50.8\n19.0\n210\nChinstrap\n\n\n342\n50.2\n18.7\n198\nChinstrap\n\n\n\n\n\n\n\n\n\nMostrar los tipos de datos científicos de cada columna del data frame.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función schema del paquete MLJ.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing MLJ\nschema(df)\n\n\n┌──────────────────┬────────────┬──────────┐\n│ names            │ scitypes   │ types    │\n├──────────────────┼────────────┼──────────┤\n│ Longitud_pico    │ Continuous │ Float64  │\n│ Profundidad_pico │ Continuous │ Float64  │\n│ Longitud_ala     │ Count      │ Int64    │\n│ Especie          │ Textual    │ String15 │\n└──────────────────┴────────────┴──────────┘\n\n\n\n\n\n\n\nConvertir las columnas Longitud_pico, Profundidad_pico, Longitud_ala a tipo científico Continuous y la columna Especie a tipo Multiclass.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función coerce! del paquete MLJ para transformar las variables categóricas en variables numéricas.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Convertimos la longitud del ala a tipo científico continuo.\ncoerce!(df, :Longitud_ala =&gt; Continuous)\n# Convertimos la columna Especie a tipo Multiclass.\ncoerce!(df, Textual =&gt; Multiclass)\n# Mostramos el nuevo esquema del data frame.\nschema(df)\n\n\n┌──────────────────┬───────────────┬────────────────────────────────────┐\n│ names            │ scitypes      │ types                              │\n├──────────────────┼───────────────┼────────────────────────────────────┤\n│ Longitud_pico    │ Continuous    │ Float64                            │\n│ Profundidad_pico │ Continuous    │ Float64                            │\n│ Longitud_ala     │ Continuous    │ Float64                            │\n│ Especie          │ Multiclass{3} │ CategoricalValue{String15, UInt32} │\n└──────────────────┴───────────────┴────────────────────────────────────┘\n\n\n\n\n\n\n\nDividir el data frame en dos partes, una con las variables de entrada y otra con la variable de salida (Especie).\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función unpack del paquete MLJ para dividir el data frame en dos partes, una con las columnas de entrada del modelo y otra con la columna de salida.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ny, X = unpack(df, ==(:Especie), name -&gt; true);\n\n\n\n\nCrear un modelo de red neuronal con la siguiente estructura.\n\nCapa de entrada con 3 neuronas (una por cada variable de entrada).\nUna capa oculta con 6 neuronas y función de activación relu.\nCapa de salida con 3 neuronas (una por cada especie de pingüino) y función de activación softmax.\n\nUsar el algoritmo de aprendizaje Adam (Adaptative Moment Estimation) con una tasa de aprendizaje de \\(0.01\\). Introducir en el modelo 0 etapas (epochs) de entrenamiento, para trabajar con los pesos aleatorios iniciales.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nCargar el constructor de modelos de redes neuronales NeuralNetworkClassifier del paquete MLJFlux e inicializarlo con los siguientes parámetros:\n\nbuilder: Permite definir el tipo de red neuronal. En este caso, usar la función MLP (Multi Layer Perceptron) para crear el modelo de red neuronal. Indicar el número de neuronas de las capas ocultas con hidden y la función de activación con σ.\noptimiser: Permite definir el optimizador, es decir, el algoritmo de aprendizaje. En este caso usar el optimizador Adam del paquete Optimisers.jl con una tasa de aprendizaje de \\(0.01\\).\nbatch_size: Tamaño del lote de entrenamiento. En este caso usar un tamaño de 10.\nepochs: Número de etapas de entrenamiento. En este caso usar 0.\nacceleration: Permite usar la aceleración de la GPU si se dispone de tarjeta gráfica. Normalmente CUDALibs().\n\nUsar la función machine del paquete MLJ para crear una máquina de aprendizaje con el modelo y los datos.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Flux, MLJFlux, Optimisers\n# Cargamos el código que define las redes neuronales.\nRedNeuronal = @load NeuralNetworkClassifier pkg = \"MLJFlux\"\n# Creamos un modelo de red neuronal con los parámetros por defecto.\nmodelo = RedNeuronal(\n    builder = MLJFlux.MLP(; hidden = (6,), σ = relu),\n    optimiser=Optimisers.Adam(0.01),\n    batch_size = 10,\n    epochs = 0,\n    # acceleration = CUDALibs()         # Para utilizar targetas gráficas GPU\n    )\n# Creamos una máquina de aprendizaje con el modelo y los datos.\nmach = machine(modelo, X, y)\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\nimport MLJFlux ✔\n\n\nuntrained Machine; caches model-specific representations of data\n  model: NeuralNetworkClassifier(builder = MLP(hidden = (6,), …), …)\n  args: \n    1:  Source @181 ⏎ Table{AbstractVector{Continuous}}\n    2:  Source @701 ⏎ AbstractVector{Multiclass{3}}\n\n\n\n\n\nDividir el conjunto de datos en un conjunto de entrenamiento con el 70% de los ejemplos y otro de prueba con el 30% restante.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función partition del paquete MLJ para dividir el conjunto de datos en un conjunto de entrenamiento y otro de prueba.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Dividimos el conjunto de datos en un conjunto de entrenamiento y otro de prueba.\ntrain, test = partition(eachindex(y), 0.7, shuffle=true, rng=123)\n\n([279, 255, 57, 6, 34, 267, 165, 35, 148, 56  …  320, 19, 85, 89, 284, 44, 169, 182, 98, 66], [236, 304, 118, 198, 80, 297, 257, 117, 67, 65  …  9, 329, 73, 121, 309, 54, 299, 71, 265, 127])\n\n\n\n\n\nEntrenar el modelo con el conjunto de ejemplos de entrenamiento y predecir la especie de los pingüinos del conjunto de prueba. Calcular la matriz de confusión y la precisión del modelo y la entropía cruzada.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función fit! del paquete MLJ para entrenar el modelo con el conjunto de entrenamiento.\nPara predecir la especie de los pingüinos del conjunto de prueba, usar la función predict del paquete MLJ.\nPara calcular la matriz de confusión, usar la función confusion_matrix del paquete StatisticalMeasures.\nUsar la función accuracy del paquete StatisticalMeasures.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Entrenamos el modelo con el conjunto de entrenamiento.\nfit!(mach, rows = train)\n# Predecimos las probabilidades de cada ejemplo de pertenecer a cada clase.\nŷ = predict(mach, rows = test)\n\n\n[ Info: Training machine(NeuralNetworkClassifier(builder = MLP(hidden = (6,), …), …), …).\n\n[ Info: MLJFlux: converting input data to Float32\n\n\n\n\n103-element CategoricalDistributions.UnivariateFiniteVector{Multiclass{3}, String15, UInt32, Float32}:\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;5.41e-13, Chinstrap=&gt;1.0, Gentoo=&gt;6.76e-14)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;2.6e-11, Chinstrap=&gt;1.0, Gentoo=&gt;4.32e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;1.3e-11, Chinstrap=&gt;1.0, Gentoo=&gt;2.05e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;8.86e-13, Chinstrap=&gt;1.0, Gentoo=&gt;1.15e-13)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;1.1e-11, Chinstrap=&gt;1.0, Gentoo=&gt;1.72e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;6.65e-11, Chinstrap=&gt;1.0, Gentoo=&gt;1.18e-11)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;6.77e-13, Chinstrap=&gt;1.0, Gentoo=&gt;8.6e-14)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;5.34e-12, Chinstrap=&gt;1.0, Gentoo=&gt;7.89e-13)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;6.09e-11, Chinstrap=&gt;1.0, Gentoo=&gt;1.08e-11)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;2.6e-11, Chinstrap=&gt;1.0, Gentoo=&gt;4.32e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;9.82e-13, Chinstrap=&gt;1.0, Gentoo=&gt;1.28e-13)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;8.54e-12, Chinstrap=&gt;1.0, Gentoo=&gt;1.31e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;8.5699995e-14, Chinstrap=&gt;1.0, Gentoo=&gt;9.36e-15)\n ⋮\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;9.42e-11, Chinstrap=&gt;1.0, Gentoo=&gt;1.72e-11)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;8.26e-13, Chinstrap=&gt;1.0, Gentoo=&gt;1.07e-13)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;6.11e-11, Chinstrap=&gt;1.0, Gentoo=&gt;1.08e-11)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;6.69e-11, Chinstrap=&gt;1.0, Gentoo=&gt;1.19e-11)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;2.61e-11, Chinstrap=&gt;1.0, Gentoo=&gt;4.34e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;6.03e-12, Chinstrap=&gt;1.0, Gentoo=&gt;9.0100003e-13)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;7.17e-11, Chinstrap=&gt;1.0, Gentoo=&gt;1.2800001e-11)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;1.86e-11, Chinstrap=&gt;1.0, Gentoo=&gt;3.01e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;3.67e-11, Chinstrap=&gt;1.0, Gentoo=&gt;6.25e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;2.88e-11, Chinstrap=&gt;1.0, Gentoo=&gt;4.82e-12)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;1.04e-13, Chinstrap=&gt;1.0, Gentoo=&gt;1.14999995e-14)\n UnivariateFinite{Multiclass{3}}(Adelie=&gt;1.56e-11, Chinstrap=&gt;1.0, Gentoo=&gt;2.49e-12)\n\n\nObtenemos distribuciones de probabilidad. La predicciones son las clases con mayor probabilidad.\n\n# Obtenemos la clase más probable.\nmode.(ŷ)\n\n103-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n ⋮\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n String15(\"Chinstrap\")\n\n\nA continuación obtenemos la matriz de confusión.\n\n# Calculamos la matriz de confusión.\ncm = confusion_matrix(y[test], mode.(ŷ))\n\n          ┌─────────────────────────────┐\n          │        Ground Truth         │\n┌─────────┼─────────┬─────────┬─────────┤\n│Predicted│ Adelie  │Chinstrap│ Gentoo  │\n├─────────┼─────────┼─────────┼─────────┤\n│ Adelie  │    0    │   45    │    0    │\n├─────────┼─────────┼─────────┼─────────┤\n│Chinstrap│    0    │   21    │    0    │\n├─────────┼─────────┼─────────┼─────────┤\n│ Gentoo  │    0    │   37    │    0    │\n└─────────┴─────────┴─────────┴─────────┘\n\n\nFinalmente calculamos la precisión del modelo y la entropía cruzada.\n\n# Calculamos la precisión del modelo.\nprecision = sum(mode.(ŷ) .== y[test]) / length(test)\n# O directamente usando la función accuracy\naccuracy(mode.(ŷ), y[test])\nprintln(\"Precisión del modelo: \", precision)\n# Calculamos la entropía cruzada.\nprintln(\"Entropía cruzada: \", cross_entropy(ŷ, y[test]))\n\nPrecisión del modelo: 0.20388349514563106\nEntropía cruzada: 21.672155912910565\n\n\n\n\n\nEntrenar el modelo durante 100 etapas y evaluar de nuevo la precisión del modelo y la entropía cruzada. ¿Ha mejorado el modelo?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función evaluate del paquete MLJ para evaluar el modelo. Los parámetros más importantes de esta función son:\n\nresampling: Indica el método de muestreo para definir los conjuntos de entrenamiento y test. Los métodos más habituales son:\n\nHoldout(fraction_train = p): Divide el conjunto de datos tomando una proporción de \\(p\\) ejemplos en el conjunto de entrenamiento y \\(1-p\\) en el conjunto de test.\nCV(nfolds = n, shuffle = true|false): Utiliza validación cruzada con n iteraciones. Si se indica shuffle = true, se utiliza validación cruzada aleatoria.\nStratifiedCV(nfolds = n, shuffle = true|false): Utiliza validación cruzada estratificada con n iteraciones. Si se indica shuffle = true, se utiliza validación cruzada estratificada aleatoria.\nInSample(): Utiliza el conjunto de entrenamiento como conjunto de test.\n\nmeasures: Indica las métricas a utilizar para evaluar el modelo. Las métricas más habituales son:\n\ncross_entropy: Pérdida de entropía cruzada.\nconfusion_matrix: Matriz de confusión.\ntrue_positive_rate: Tasa de verdaderos positivos.\ntrue_negative_rate: Tasa de verdaderos negativos.\nppv: Valor predictivo positivo.\nnpv: Valor predictivo negativo.\naccuracy: Precisión.\n\nSe puede indicar más de una en un vector.\n\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos el número de épocas de entrenamiento.\nmodelo.epochs = 100\n# Actualizamos la máquina de aprendizaje con el nuevo modelo.\nmach = machine(modelo, X, y)\n# Entrenamos el modelo con el conjunto de entrenamiento y evaluamos el modelo.\nevaluate!(mach, resampling = Holdout(fraction_train = 0.7, rng = 123), measure = [accuracy, cross_entropy])\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌───┬──────────────────────┬──────────────┬─────────────┐\n│   │ measure              │ operation    │ measurement │\n├───┼──────────────────────┼──────────────┼─────────────┤\n│ A │ Accuracy()           │ predict_mode │ 0.437       │\n│ B │ LogLoss(             │ predict      │ 20.3        │\n│   │   tol = 2.22045e-16) │              │             │\n└───┴──────────────────────┴──────────────┴─────────────┘\n\n\n\n\nLa precisión del modelo es muy baja. Esto puede deberse a que la estructura de la red neuronal no es la adecuada, o a que las variables de entrada no están normalizadas.\n\n\n\nEstandarizar las variables de entrada y volver a entrenar el modelo una sola etapa. Evaluar la precisión del modelo y la entropía cruzada.\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función Standardizer del paquete MLJ para estandarizar las variables de entrada. La estandarización consiste en restar la media y dividir por la desviación típica de cada variable.\nUsar el operador de tubería |&gt; para encadenar la estandarización y la red neuronal en un modelo.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos la transformación de estandarización.\nestandarizacion = Standardizer()\n# Definimos de nuevo la red neuronal.\nred = RedNeuronal(\n    builder = MLJFlux.MLP(; hidden = (6,), σ = relu),\n    optimiser = Optimisers.Adam(0.01),\n    batch_size = 10,\n    epochs = 1\n    )\n# Definimos el modelo mediante un flujo que aplique primero la estandarización y luego la red neuronal.\nmodelo = estandarizacion  |&gt; red\n# Creamos una máquina de aprendizaje con el modelo y los datos.\nmach = machine(modelo, X, y)\n# Entrenamos el modelo con el conjunto de entrenamiento y evaluamos el modelo.\nevaluate!(mach, resampling = Holdout(fraction_train = 0.7, rng = 123), measure = [accuracy, cross_entropy])\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌───┬──────────────────────┬──────────────┬─────────────┐\n│   │ measure              │ operation    │ measurement │\n├───┼──────────────────────┼──────────────┼─────────────┤\n│ A │ Accuracy()           │ predict_mode │ 0.922       │\n│ B │ LogLoss(             │ predict      │ 0.466       │\n│   │   tol = 2.22045e-16) │              │             │\n└───┴──────────────────────┴──────────────┴─────────────┘\n\n\n\n\n\n\n\nVolver a entrenar el modelo durante 100 etapas y evaluar de nuevo la precisión del modelo y la entropía cruzada. ¿Ha mejorado el modelo?\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos el número de épocas de entrenamiento.\nred.epochs = 100\n# Actualizamos la máquina de aprendizaje con el nuevo modelo.\nevaluate!(mach, resampling = Holdout(fraction_train = 0.7, rng = 123), measure = [accuracy, cross_entropy])\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌───┬──────────────────────┬──────────────┬─────────────┐\n│   │ measure              │ operation    │ measurement │\n├───┼──────────────────────┼──────────────┼─────────────┤\n│ A │ Accuracy()           │ predict_mode │ 0.99        │\n│ B │ LogLoss(             │ predict      │ 0.0661      │\n│   │   tol = 2.22045e-16) │              │             │\n└───┴──────────────────────┴──────────────┴─────────────┘\n\n\n\n\nEl modelo ha mejorado enormemente y ahora la precisión del modelo es casi del 100%. Esto indica que el modelo se ha ajustado muy bien a los datos de entrenamiento y es capaz de predecir a los datos de prueba casi a la perfección.\n\n\n\nVolver a repetir el proceso de entrenamiento y evaluación del modelo con validación cruzada de 10 pliegues y calcular la precisión del modelo y la entropía cruzada.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nevaluate!(mach, resampling = CV(nfolds = 10), measure = [accuracy, cross_entropy])\n\n\nEvaluating over 10 folds:  20%[=====&gt;                   ]  ETA: 0:00:01\n\nEvaluating over 10 folds: 100%[=========================] Time: 0:00:01\n\n\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌───┬──────────────────────┬──────────────┬─────────────┐\n│   │ measure              │ operation    │ measurement │\n├───┼──────────────────────┼──────────────┼─────────────┤\n│ A │ Accuracy()           │ predict_mode │ 0.982       │\n│ B │ LogLoss(             │ predict      │ 0.088       │\n│   │   tol = 2.22045e-16) │              │             │\n└───┴──────────────────────┴──────────────┴─────────────┘\n┌───┬───────────────────────────────────────────────────────────────────────────\n│   │ per_fold                                                                 ⋯\n├───┼───────────────────────────────────────────────────────────────────────────\n│ A │ [1.0, 1.0, 0.941, 1.0, 1.0, 1.0, 1.0, 1.0, 0.941, 0.941]                 ⋯\n│ B │ [0.00424, 0.00174, 0.312, 0.0356, 0.00396, 0.00288, 4.12e-5, 0.000543, 0 ⋯\n└───┴───────────────────────────────────────────────────────────────────────────\n                                                               2 columns omitted\n\n\n\n\n\n\n\nVolver a repetir el proceso de entrenamiento y evaluación del modelo con validación cruzada tomando distintas tasas de aprendizaje. ¿Para qué tasa de aprendizaje se obtiene el mejor modelo? ¿Cuál es la precisión del modelo y la entropía cruzada?\n\n\n\n\n\n\nNotaAyuda\n\n\n\n\n\nUsar la función range del paquete MLJ para definir un rango etapas. La función range permite definir un rango de valores para un parámetro del modelo.\n\n\n\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos un rango de tasas de aprendizaje.\nr = range(modelo, :(neural_network_classifier.epochs), lower=1, upper=100, scale=:log)\n# Obtenemos las precisiones para cada número de etapas.\n_, _, etapas, entropia = learning_curve(mach, range = r, resampling = CV(nfolds = 10), measure = cross_entropy)\nusing GLMakie\nfig = Figure()\nax = Axis(fig[1, 1], title = \"Precisión del modelo con distintas etapas de entrenamiento\", xlabel = \"Etapas\", ylabel = \"Precisión\")\nlines!(ax, etapas, entropia)\ndisplay(fig)\n# Obtenemos el número de etapas con la mejor precisión.\netapas_optimas = etapas[argmin(entropia)]\nprintln(\"Número de etapas óptimas: \", etapas_optimas)\n\n\n[ Info: Training machine(ProbabilisticTunedModel(model = ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …), …), …).\n\n[ Info: Attempting to evaluate 24 models.\n\n\nEvaluating over 24 metamodels:   8%[==&gt;                      ]  ETA: 0:00:11\n\nEvaluating over 24 metamodels:  12%[===&gt;                     ]  ETA: 0:00:07\n\nEvaluating over 24 metamodels:  17%[====&gt;                    ]  ETA: 0:00:05\n\nEvaluating over 24 metamodels:  21%[=====&gt;                   ]  ETA: 0:00:04\n\nEvaluating over 24 metamodels:  25%[======&gt;                  ]  ETA: 0:00:04\n\nEvaluating over 24 metamodels:  29%[=======&gt;                 ]  ETA: 0:00:03\n\nEvaluating over 24 metamodels:  33%[========&gt;                ]  ETA: 0:00:03\n\nEvaluating over 24 metamodels:  38%[=========&gt;               ]  ETA: 0:00:02\n\nEvaluating over 24 metamodels:  42%[==========&gt;              ]  ETA: 0:00:02\n\nEvaluating over 24 metamodels:  46%[===========&gt;             ]  ETA: 0:00:02\n\nEvaluating over 24 metamodels:  50%[============&gt;            ]  ETA: 0:00:02\n\nEvaluating over 24 metamodels:  54%[=============&gt;           ]  ETA: 0:00:02\n\nEvaluating over 24 metamodels:  58%[==============&gt;          ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  62%[===============&gt;         ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  67%[================&gt;        ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  71%[=================&gt;       ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  75%[==================&gt;      ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  79%[===================&gt;     ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  83%[====================&gt;    ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  88%[=====================&gt;   ]  ETA: 0:00:01\n\nEvaluating over 24 metamodels:  92%[======================&gt;  ]  ETA: 0:00:00\n\nEvaluating over 24 metamodels:  96%[=======================&gt; ]  ETA: 0:00:00\n\nEvaluating over 24 metamodels: 100%[=========================] Time: 0:00:06\n\n\n\n\nNúmero de etapas óptimas: 53\n\n\n\n\n\nEntrenar de nuevo el modelo con todo el conjunto de ejemplos y con el número de etapas óptimas, y predecir la especie de los 5 primeros pingüinos del conjunto de ejemplos.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\n# Definimos el número de épocas de entrenamiento.\nred.epochs = etapas_optimas\n# Entrenamos el modelo con todo el conjunto de ejemplos.\nfit!(mach)\n# Predecimos la especie de los 5 primeros pingüinos del conjunto de ejemplos.\npredict_mode(mach, X[1:5, :])\n\n\n[ Info: Training machine(ProbabilisticPipeline(standardizer = Standardizer(features = Symbol[], …), …), …).\n\n[ Info: Training machine(:standardizer, …).\n\n[ Info: Training machine(:neural_network_classifier, …).\n\n[ Info: MLJFlux: converting input data to Float32\n\n\nOptimising neural net:   4%[&gt;                        ]  ETA: 0:00:00\n\nOptimising neural net:   6%[=&gt;                       ]  ETA: 0:00:00\n\nOptimising neural net:   7%[=&gt;                       ]  ETA: 0:00:00\n\nOptimising neural net:   9%[==&gt;                      ]  ETA: 0:00:00\n\nOptimising neural net:  11%[==&gt;                      ]  ETA: 0:00:00\n\nOptimising neural net:  13%[===&gt;                     ]  ETA: 0:00:00\n\nOptimising neural net:  15%[===&gt;                     ]  ETA: 0:00:00\n\nOptimising neural net:  17%[====&gt;                    ]  ETA: 0:00:00\n\nOptimising neural net:  19%[====&gt;                    ]  ETA: 0:00:00\n\nOptimising neural net:  20%[=====&gt;                   ]  ETA: 0:00:00\n\nOptimising neural net:  22%[=====&gt;                   ]  ETA: 0:00:00\n\nOptimising neural net:  24%[======&gt;                  ]  ETA: 0:00:00\n\nOptimising neural net:  26%[======&gt;                  ]  ETA: 0:00:00\n\nOptimising neural net:  28%[======&gt;                  ]  ETA: 0:00:00\n\nOptimising neural net:  30%[=======&gt;                 ]  ETA: 0:00:00\n\nOptimising neural net:  31%[=======&gt;                 ]  ETA: 0:00:00\n\nOptimising neural net:  33%[========&gt;                ]  ETA: 0:00:00\n\nOptimising neural net:  35%[========&gt;                ]  ETA: 0:00:00\n\nOptimising neural net:  37%[=========&gt;               ]  ETA: 0:00:00\n\nOptimising neural net:  39%[=========&gt;               ]  ETA: 0:00:00\n\nOptimising neural net:  41%[==========&gt;              ]  ETA: 0:00:00\n\nOptimising neural net:  43%[==========&gt;              ]  ETA: 0:00:00\n\nOptimising neural net:  44%[===========&gt;             ]  ETA: 0:00:00\n\nOptimising neural net:  46%[===========&gt;             ]  ETA: 0:00:00\n\nOptimising neural net:  48%[============&gt;            ]  ETA: 0:00:00\n\nOptimising neural net:  50%[============&gt;            ]  ETA: 0:00:00\n\nOptimising neural net:  52%[============&gt;            ]  ETA: 0:00:00\n\nOptimising neural net:  54%[=============&gt;           ]  ETA: 0:00:00\n\nOptimising neural net:  56%[=============&gt;           ]  ETA: 0:00:00\n\nOptimising neural net:  57%[==============&gt;          ]  ETA: 0:00:00\n\nOptimising neural net:  59%[==============&gt;          ]  ETA: 0:00:00\n\nOptimising neural net:  61%[===============&gt;         ]  ETA: 0:00:00\n\nOptimising neural net:  63%[===============&gt;         ]  ETA: 0:00:00\n\nOptimising neural net:  65%[================&gt;        ]  ETA: 0:00:00\n\nOptimising neural net:  67%[================&gt;        ]  ETA: 0:00:00\n\nOptimising neural net:  69%[=================&gt;       ]  ETA: 0:00:00\n\nOptimising neural net:  70%[=================&gt;       ]  ETA: 0:00:00\n\nOptimising neural net:  72%[==================&gt;      ]  ETA: 0:00:00\n\nOptimising neural net:  74%[==================&gt;      ]  ETA: 0:00:00\n\nOptimising neural net:  76%[==================&gt;      ]  ETA: 0:00:00\n\nOptimising neural net:  78%[===================&gt;     ]  ETA: 0:00:00\n\nOptimising neural net:  80%[===================&gt;     ]  ETA: 0:00:00\n\nOptimising neural net:  81%[====================&gt;    ]  ETA: 0:00:00\n\nOptimising neural net:  83%[====================&gt;    ]  ETA: 0:00:00\n\nOptimising neural net:  85%[=====================&gt;   ]  ETA: 0:00:00\n\nOptimising neural net:  87%[=====================&gt;   ]  ETA: 0:00:00\n\nOptimising neural net:  89%[======================&gt;  ]  ETA: 0:00:00\n\nOptimising neural net:  91%[======================&gt;  ]  ETA: 0:00:00\n\nOptimising neural net:  93%[=======================&gt; ]  ETA: 0:00:00\n\nOptimising neural net:  94%[=======================&gt; ]  ETA: 0:00:00\n\nOptimising neural net:  96%[========================&gt;]  ETA: 0:00:00\n\nOptimising neural net:  98%[========================&gt;]  ETA: 0:00:00\n\nOptimising neural net: 100%[=========================] Time: 0:00:00\n\n\n\n\n5-element CategoricalArrays.CategoricalArray{String15,1,UInt32}:\n String15(\"Adelie\")\n String15(\"Adelie\")\n String15(\"Adelie\")\n String15(\"Adelie\")\n String15(\"Adelie\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Redes de neuronas artificiales</span>"
    ]
  },
  {
    "objectID": "10-redes-neuronales-recurrentes.html",
    "href": "10-redes-neuronales-recurrentes.html",
    "title": "6  Redes de neuronas artificiales recurrentes",
    "section": "",
    "text": "6.1 Ejercicios Resueltos\nLas redes de neuronas artificiales recurrentes (RNN por sus siglas en inglés) son un tipo de redes neuronales diseñadas para trabajar con datos secuenciales. A diferencia de las redes neuronales tradicionales tienen una memoria interna capaz de recordar información de una secuencia temporal de entradas, y utilizar esa información para predecir nuevos valores de la serie o clasificarla. Esto las hace ideales para tareas de predicción de series temporales, traducción de textos o audios, y procesamiento del lenguaje natural entre otras.\nEn esta práctica veremos cómo funcionan estas redes neuronales y aprenderemos a implementarlas con el paquete Lux de Julia.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes de neuronas artificiales recurrentes</span>"
    ]
  },
  {
    "objectID": "10-redes-neuronales-recurrentes.html#ejercicios-resueltos",
    "href": "10-redes-neuronales-recurrentes.html#ejercicios-resueltos",
    "title": "6  Redes de neuronas artificiales recurrentes",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing GLMakie  # Para el dibujo de gráficas.\nusing Random  # Para la generación de números aleatorios.\nusing Lux  # Para la implementación de redes neuronales.\nusing Zygote  # Cálculo automático de derivadas y gradientes.\nusing Optimisers # Para la optimización de funciones.\nusing Statistics # Para las funciones de coste.\n\nEjercicio 6.1 La sucesión de Fibonacci es una sucesión recurrente que se define como\n\\[a_1 = 1,\\quad a_2 =1, \\quad a_n = a_{n-1} + a_{n-2}\\quad \\forall n\\geq 3.\\]\n\nConstruir una función para generar la sucesión de Fibonacci.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction fibonacci(n::Int)\n    a = zeros(Float32, n)\n    a[1] = 1\n    a[2] = 1\n    for i in 3:n\n        a[i] = a[i-1] + a[i-2]\n    end\n    return a\nend\n\nfibonacci (generic function with 1 method)\n\n\n\n\n\nDividir la secuencia de los 30 primeros términos de la sucesión de Fibonacci en subsecuencias de longitud 5. Para cada una de estas subsecuencias, guardar los vectores de los cuatro primeros términos en una matriz de entrada y el último valor en una matriz de salida.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfib = fibonacci(25) \ntamaño = 4\nn = 25 - tamaño\n# La red esperar una entrada con dimensiones (características, tamaño, lote).\nX =  Array{Float32}(undef, 1, tamaño, n)\nY = Array{Float32}(undef, 1, n)  \nfor i = 1:n\n    ventana = fib[i:i+tamaño-1]\n    etiqueta = fib[i+tamaño]\n    X[1, : , i] .= ventana\n    Y[1, i] = etiqueta\nend\n\n\n\n\nDefinir una red neuronal con una neurona recurrente con un estado oculto de tamaño 1, sin término independiente (bias). Inicializar la red con pesos aleatorios.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Lux, Random\n\nmodelo = Chain(\n    Recurrence(RNNCell(1 =&gt; 1, identity; use_bias = false); return_sequence = false)\n)\n\nrng = Random.default_rng()\n# Semilla aleatoria para reproducibilidad\nRandom.seed!(rng, 1234)\nps, st = Lux.setup(rng, modelo)\n\n((layer_1 = (weight_ih = Float32[-1.7194579;;], weight_hh = Float32[1.174417;;]),), (layer_1 = (rng = TaskLocalRNG(),),))\n\n\n\n\n\nDefinir como función de coste el error cuadrático medio para todas las secuencias del conjunto de entrenamiento.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Statistics\nfunction coste(ps, st, X, Y)\n    ŷ, estado = modelo(X, ps, st)          # ŷ: (1, batch)\n    return mean((ŷ .- Y).^2)\nend\n\ncoste(ps, st, X, Y)\n\n6.2632223f9\n\n\n\n\n\nEntrenar la red neuronal con el algoritmo de optimización Adam tomando una tasa de aprendizaje 0.1. Realizar 200 épocas.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Optimisers, Zygote\nopt = Optimisers.setup(Optimisers.Adam(0.1f0), ps)\n\nnepocas = 200\ncostes = []\nfor epoca in 1:nepocas\n    # gradiente de la red con respecto a los parámetros\n    gs = first(Zygote.gradient(p -&gt; coste(p, st, X, Y), ps))\n    opt, ps = Optimisers.update(opt, ps, gs) \n    push!(costes, coste(ps, st, X, Y))\n    println(\"Época $epoca | coste = \", costes[end])\nend\n\nÉpoca 1 | coste = 5.0130447e9\nÉpoca 2 | coste = 4.046983e9\nÉpoca 3 | coste = 3.3034327e9\nÉpoca 4 | coste = 2.7312077e9\nÉpoca 5 | coste = 2.2893059e9\nÉpoca 6 | coste = 1.94581e9\nÉpoca 7 | coste = 1.6764061e9\nÉpoca 8 | coste = 1.4628404e9\nÉpoca 9 | coste = 1.2915259e9\nÉpoca 10 | coste = 1.1523862e9\nÉpoca 11 | coste = 1.0379493e9\nÉpoca 12 | coste = 9.4265696e8\nÉpoca 13 | coste = 8.623539e8\nÉpoca 14 | coste = 7.9391226e8\nÉpoca 15 | coste = 7.349589e8\nÉpoca 16 | coste = 6.836777e8\nÉpoca 17 | coste = 6.386652e8\nÉpoca 18 | coste = 5.9882643e8\nÉpoca 19 | coste = 5.632986e8\nÉpoca 20 | coste = 5.313949e8\nÉpoca 21 | coste = 5.0256346e8\nÉpoca 22 | coste = 4.7635606e8\nÉpoca 23 | coste = 4.5240582e8\nÉpoca 24 | coste = 4.3040934e8\nÉpoca 25 | coste = 4.101142e8\nÉpoca 26 | coste = 3.913082e8\nÉpoca 27 | coste = 3.7381197e8\nÉpoca 28 | coste = 3.5747312e8\nÉpoca 29 | coste = 3.4216106e8\nÉpoca 30 | coste = 3.277637e8\nÉpoca 31 | coste = 3.141841e8\nÉpoca 32 | coste = 3.0133814e8\nÉpoca 33 | coste = 2.891527e8\nÉpoca 34 | coste = 2.77564e8\nÉpoca 35 | coste = 2.665161e8\nÉpoca 36 | coste = 2.5595997e8\nÉpoca 37 | coste = 2.458527e8\nÉpoca 38 | coste = 2.3615629e8\nÉpoca 39 | coste = 2.268375e8\nÉpoca 40 | coste = 2.1786688e8\nÉpoca 41 | coste = 2.0921851e8\nÉpoca 42 | coste = 2.0086955e8\nÉpoca 43 | coste = 1.927998e8\nÉpoca 44 | coste = 1.8499157e8\nÉpoca 45 | coste = 1.774291e8\nÉpoca 46 | coste = 1.7009869e8\nÉpoca 47 | coste = 1.6298826e8\nÉpoca 48 | coste = 1.5608707e8\nÉpoca 49 | coste = 1.493859e8\nÉpoca 50 | coste = 1.4287659e8\nÉpoca 51 | coste = 1.3655203e8\nÉpoca 52 | coste = 1.3040608e8\nÉpoca 53 | coste = 1.24433336e8\nÉpoca 54 | coste = 1.1862924e8\nÉpoca 55 | coste = 1.1298979e8\nÉpoca 56 | coste = 1.0751154e8\nÉpoca 57 | coste = 1.02191656e8\nÉpoca 58 | coste = 9.70276e7\nÉpoca 59 | coste = 9.201727e7\nÉpoca 60 | coste = 8.715889e7\nÉpoca 61 | coste = 8.245096e7\nÉpoca 62 | coste = 7.789219e7\nÉpoca 63 | coste = 7.348143e7\nÉpoca 64 | coste = 6.9217816e7\nÉpoca 65 | coste = 6.510044e7\nÉpoca 66 | coste = 6.1128532e7\nÉpoca 67 | coste = 5.7301384e7\nÉpoca 68 | coste = 5.361825e7\nÉpoca 69 | coste = 5.007843e7\nÉpoca 70 | coste = 4.6681136e7\nÉpoca 71 | coste = 4.3425516e7\nÉpoca 72 | coste = 4.0310664e7\nÉpoca 73 | coste = 3.7335516e7\nÉpoca 74 | coste = 3.449891e7\nÉpoca 75 | coste = 3.1799504e7\nÉpoca 76 | coste = 2.9235844e7\nÉpoca 77 | coste = 2.6806242e7\nÉpoca 78 | coste = 2.450881e7\nÉpoca 79 | coste = 2.2341502e7\nÉpoca 80 | coste = 2.030202e7\nÉpoca 81 | coste = 1.838785e7\nÉpoca 82 | coste = 1.6596277e7\nÉpoca 83 | coste = 1.4924337e7\nÉpoca 84 | coste = 1.3368829e7\nÉpoca 85 | coste = 1.1926354e7\nÉpoca 86 | coste = 1.0593308e7\nÉpoca 87 | coste = 9.36583e6\nÉpoca 88 | coste = 8.2399175e6\nÉpoca 89 | coste = 7.2113615e6\nÉpoca 90 | coste = 6.275804e6\nÉpoca 91 | coste = 5.428737e6\nÉpoca 92 | coste = 4.6655335e6\nÉpoca 93 | coste = 3.9814918e6\nÉpoca 94 | coste = 3.3718168e6\nÉpoca 95 | coste = 2.8316872e6\nÉpoca 96 | coste = 2.3562815e6\nÉpoca 97 | coste = 1.9407861e6\nÉpoca 98 | coste = 1.5804446e6\nÉpoca 99 | coste = 1.270575e6\nÉpoca 100 | coste = 1.0065965e6\nÉpoca 101 | coste = 784080.8\nÉpoca 102 | coste = 598748.06\nÉpoca 103 | coste = 446494.9\nÉpoca 104 | coste = 323433.88\nÉpoca 105 | coste = 225888.9\nÉpoca 106 | coste = 150422.62\nÉpoca 107 | coste = 93842.3\nÉpoca 108 | coste = 53206.5\nÉpoca 109 | coste = 25830.83\nÉpoca 110 | coste = 9288.164\nÉpoca 111 | coste = 1405.4087\nÉpoca 112 | coste = 258.9841\nÉpoca 113 | coste = 4165.221\nÉpoca 114 | coste = 11670.322\nÉpoca 115 | coste = 21538.016\nÉpoca 116 | coste = 32733.744\nÉpoca 117 | coste = 44411.508\nÉpoca 118 | coste = 55893.332\nÉpoca 119 | coste = 66655.56\nÉpoca 120 | coste = 76311.76\nÉpoca 121 | coste = 84591.56\nÉpoca 122 | coste = 91327.59\nÉpoca 123 | coste = 96442.8\nÉpoca 124 | coste = 99924.69\nÉpoca 125 | coste = 101825.52\nÉpoca 126 | coste = 102235.44\nÉpoca 127 | coste = 101282.234\nÉpoca 128 | coste = 99114.875\nÉpoca 129 | coste = 95898.414\nÉpoca 130 | coste = 91803.37\nÉpoca 131 | coste = 86998.22\nÉpoca 132 | coste = 81652.52\nÉpoca 133 | coste = 75922.914\nÉpoca 134 | coste = 69954.91\nÉpoca 135 | coste = 63880.133\nÉpoca 136 | coste = 57814.195\nÉpoca 137 | coste = 51860.23\nÉpoca 138 | coste = 46101.43\nÉpoca 139 | coste = 40606.44\nÉpoca 140 | coste = 35429.72\nÉpoca 141 | coste = 30609.598\nÉpoca 142 | coste = 26174.066\nÉpoca 143 | coste = 22137.898\nÉpoca 144 | coste = 18506.016\nÉpoca 145 | coste = 15274.37\nÉpoca 146 | coste = 12433.482\nÉpoca 147 | coste = 9965.116\nÉpoca 148 | coste = 7848.189\nÉpoca 149 | coste = 6057.5005\nÉpoca 150 | coste = 4566.634\nÉpoca 151 | coste = 3346.4177\nÉpoca 152 | coste = 2367.8516\nÉpoca 153 | coste = 1601.9951\nÉpoca 154 | coste = 1020.641\nÉpoca 155 | coste = 596.68134\nÉpoca 156 | coste = 305.06555\nÉpoca 157 | coste = 122.30558\nÉpoca 158 | coste = 26.97289\nÉpoca 159 | coste = 0.021119582\nÉpoca 160 | coste = 24.514503\nÉpoca 161 | coste = 85.74834\nÉpoca 162 | coste = 171.26875\nÉpoca 163 | coste = 270.41803\nÉpoca 164 | coste = 374.75803\nÉpoca 165 | coste = 477.35016\nÉpoca 166 | coste = 573.0674\nÉpoca 167 | coste = 657.9105\nÉpoca 168 | coste = 729.38\nÉpoca 169 | coste = 785.9467\nÉpoca 170 | coste = 826.7763\nÉpoca 171 | coste = 852.2167\nÉpoca 172 | coste = 862.56854\nÉpoca 173 | coste = 859.2576\nÉpoca 174 | coste = 843.40356\nÉpoca 175 | coste = 816.80646\nÉpoca 176 | coste = 781.19714\nÉpoca 177 | coste = 738.0868\nÉpoca 178 | coste = 689.4481\nÉpoca 179 | coste = 636.91144\nÉpoca 180 | coste = 581.95496\nÉpoca 181 | coste = 525.9896\nÉpoca 182 | coste = 470.28207\nÉpoca 183 | coste = 415.8274\nÉpoca 184 | coste = 363.58008\nÉpoca 185 | coste = 314.0716\nÉpoca 186 | coste = 268.0682\nÉpoca 187 | coste = 225.7752\nÉpoca 188 | coste = 187.4945\nÉpoca 189 | coste = 153.36887\nÉpoca 190 | coste = 123.32797\nÉpoca 191 | coste = 97.2789\nÉpoca 192 | coste = 75.056725\nÉpoca 193 | coste = 56.43496\nÉpoca 194 | coste = 41.149002\nÉpoca 195 | coste = 28.87258\nÉpoca 196 | coste = 19.250816\nÉpoca 197 | coste = 12.009598\nÉpoca 198 | coste = 6.7831855\nÉpoca 199 | coste = 3.2605946\nÉpoca 200 | coste = 1.1491588\n\n\n\n\n\nDibujar la evolución de los costes durante el proceso de entrenamiento.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing GLMakie\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"Época\", ylabel = \"Error cuadrático medio\", title = \"Evolución del coste\")\nlines!(ax, costes)\nfig\n\n\n\n\n\n\n\nUsar la red neuronal entrenada para predecir el siguiente término de la serie de Fibonacci a partir de los 4 últimos términos de la serie.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfib = fibonacci(30)\nX_test = reshape(fib[end-tamaño : end-1], 1, tamaño, 1) \n\ny_test, _ = modelo(X_test, ps, st)\nprintln(\"Predicción del término 30: \",  y_test[1, 1])\nprintln(\"Término 30 de la sucesión de Fibonacci: \", fib[end])\n\nPredicción del término 30: 831997.2\nTérmino 30 de la sucesión de Fibonacci: 832040.0\n\n\n\n\n\nMostrar los pesos de la red neuronal entrenada.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nprintln(\"Pesos de la entrada de la red neuronal:\", ps.layer_1.weight_ih)\nprintln(\"Pesos del estado de la red neuronal:\", ps.layer_1.weight_hh)\n\nPesos de la entrada de la red neuronal:Float32[1.662782;;]\nPesos del estado de la red neuronal:Float32[-0.04483252;;]\n\n\n\n\n\n\n\n\nEjercicio 6.2 El fichero stock.csv contiene los precios al cierre de las acciones de una empresas en bolsa de valores de los 300 primeros días del año 2020.\n\nCargar el conjunto de datos y dibujar la serie temporal.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing CSV, DataFrames, GLMakie\n\n# Cargamos el conjunto de datos en un data frame\ndf = CSV.read(\"datos/stock.csv\", DataFrame)\n\n# Creamos el gráfico de la evolución\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"Día\", ylabel = \"Precio de la acción (€)\", title = \"Evolución del precio de la acción\")\n\nlines!(ax, df.dia, df.precio)\nfig\n\n\n\n\n\n\n\nNormalizar el conjunto de datos.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Statistics\n\n# Simple normalization (z-score)\nμ = mean(df.precio)\nσ = std(df.precio)\nserie = (df.precio .- μ) ./ σ     \n\n300-element Vector{Float64}:\n -1.2770239698472932\n -1.4749432268197025\n -1.2481144154580652\n -0.8745140202741842\n -0.8789616440263692\n -1.4460336724304748\n -0.9679141190701513\n -1.1814000591752272\n -1.1591619404142832\n -1.054642782237841\n -1.1391476335294317\n -0.8923045152829369\n -1.1035666435119202\n  ⋮\n  1.4249074596075517\n  1.5049646871469575\n  1.031292757538819\n  1.0379641931671029\n  1.6472886472170039\n  0.9112069162297167\n  1.1469309750957366\n  1.4782789446338223\n  1.3137168658028255\n  1.5027408752708586\n  1.3759835983334725\n  1.6450648353409114\n\n\n\n\n\nDividir la serie de los precios en secuencias de 50 términos. Para cada una de estas secuencias, guardar los vectores de los 40 primeros términos en una matriz de entrada y el último valor en una matriz de salida.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nfunction crear_secuencias(serie, tamaño)\n    # Número de ventanas\n    n = length(serie) - tamaño  \n\n    # X: (características=1, tamaño=seq_len, lotes=n)\n    X = Array{Float32}(undef, 1, tamaño, n)\n    # Y: (etiquetas=1, lotes=n)\n    Y = Array{Float32}(undef, 1, n)\n\n    for i in 1:n\n        ventana = serie[i : i + tamaño - 1]\n        etiqueta = serie[i + tamaño]\n        X[1, :, i] .= ventana\n        Y[1, i] = etiqueta\n    end\n\n    return X, Y\nend\n\nX, Y = crear_secuencias(serie, 50)\n\n(Float32[-1.2770239 -1.4749433 … -1.7551435 -1.501629;;; -1.4749433 -1.2481145 … -1.501629 -1.6973244;;; -1.2481145 -0.87451404 … -1.6973244 -1.4393623;;; … ;;; 1.7607031 1.3515216 … 1.478279 1.3137169;;; 1.3515216 1.1980786 … 1.3137169 1.5027409;;; 1.1980786 1.6117077 … 1.5027409 1.3759836], Float32[-1.6973244 -1.4393623 … 1.3759836 1.6450648])\n\n\n\n\n\nDividir el conjunto de secuencias en un conjunto de entrenamiento con las 200 primeras secuencias y otro de prueba con las 50 restantes.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nXentrenamiento, Yentrenamiento = X[:, :, 1:200], Y[:, 1:200]\nXtest, Ytest = X[:, :, 201:end], Y[:, 201:end]\n\n(Float32[0.08839652 0.09062033 … 1.3515216 1.1980786;;; 0.09062033 0.3797159 … 1.1980786 1.6117077;;; 0.3797159 0.29298723 … 1.6117077 1.3248359;;; … ;;; 1.7607031 1.3515216 … 1.478279 1.3137169;;; 1.3515216 1.1980786 … 1.3137169 1.5027409;;; 1.1980786 1.6117077 … 1.5027409 1.3759836], Float32[1.6117077 1.3248359 … 1.3759836 1.6450648])\n\n\n\n\n\nDefinir una red neuronal con una neurona recurrente con un estado oculto de tamaño 64 y una neurona densa de 1 salida. Inicializar la red con pesos aleatorios.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Lux, Random\nmodelo = Chain(\n    Recurrence(RNNCell(1 =&gt; 64); return_sequence = false),\n    Dense(64 =&gt; 1)\n)\n\nrng = Random.default_rng()\n# Semilla aletoria para reproducibilidad\nRandom.seed!(rng, 1234)\nps, st = Lux.setup(rng, modelo)\n\n((layer_1 = (weight_ih = Float32[-0.21493223; 0.14680213; … ; 0.09006676; -0.29187012;;], weight_hh = Float32[0.16710633 -0.10049977 … -0.7339213 -0.44402117; -0.12605162 -0.42469883 … -0.2643636 -0.33319688; … ; -0.2543804 0.052617714 … -0.60686016 -0.4568765; 0.24136081 -0.34361988 … 0.12011717 0.118953556], bias_ih = Float32[-0.0951326, 0.08999576, 0.10426867, 0.11379315, 0.11712587, -0.3674832, -0.018230781, -0.12023171, -0.00048576295, 0.7689639  …  -0.06666367, 0.52615803, -0.09323904, -0.015267894, -0.29630774, -0.31387812, -0.2896284, -0.171922, 0.173794, -0.13455431], bias_hh = Float32[0.094174266, -0.32182646, -0.4835172, -0.1707672, -0.11865046, -0.6600209, -0.3815048, -0.3407301, 0.014974058, -0.2173861  …  0.036009803, 0.32550737, -0.016171448, -0.1333059, 0.18780592, -0.085141905, 0.14552, -0.021185711, -0.15243843, -0.029349051]), layer_2 = (weight = Float32[-0.04013527 -0.08779158 … -0.21502617 -0.12662868], bias = Float32[-0.08802833])), (layer_1 = (rng = TaskLocalRNG(),), layer_2 = NamedTuple()))\n\n\n\n\n\nDefinir como función de coste el error cuadrático medio para todas las secuencias del conjunto de entrenamiento.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Statistics\nfunction coste(ps, st, X, Y)\n    ŷ, estado = modelo(X, ps, st)\n    return mean((ŷ .- Y).^2)\nend\n\ncoste(ps, st, X, Y)\n\n2.4999213f0\n\n\n\n\n\nEntrenar la red neuronal con el algoritmo de optimización Adam tomando una tasa de aprendizaje 0.01. Realizar 300 épocas.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing Optimisers, Zygote\nopt = Optimisers.setup(Optimisers.Adam(0.01f0), ps)\n\nnepocas = 100\ncostes = []\nfor epoca in 1:nepocas\n    # gradiente de la red con respecto a los parámetros\n    gs = first(Zygote.gradient(p -&gt; coste(p, st, Xentrenamiento, Yentrenamiento), ps))\n    opt, ps = Optimisers.update(opt, ps, gs) \n    push!(costes, coste(ps, st, Xentrenamiento, Yentrenamiento))\n    println(\"Época $epoca | coste = \", costes[end])\nend\n\nÉpoca 1 | coste = 0.7120536\nÉpoca 2 | coste = 0.58144593\nÉpoca 3 | coste = 0.86751\nÉpoca 4 | coste = 0.9924159\nÉpoca 5 | coste = 0.8545057\nÉpoca 6 | coste = 0.6759848\nÉpoca 7 | coste = 0.5618811\nÉpoca 8 | coste = 0.56183267\nÉpoca 9 | coste = 0.6390394\nÉpoca 10 | coste = 0.7087791\nÉpoca 11 | coste = 0.7118296\nÉpoca 12 | coste = 0.6544135\nÉpoca 13 | coste = 0.5607798\nÉpoca 14 | coste = 0.4915699\nÉpoca 15 | coste = 0.49867082\nÉpoca 16 | coste = 0.5568062\nÉpoca 17 | coste = 0.5875028\nÉpoca 18 | coste = 0.566299\nÉpoca 19 | coste = 0.51768893\nÉpoca 20 | coste = 0.4785608\nÉpoca 21 | coste = 0.47465625\nÉpoca 22 | coste = 0.49831668\nÉpoca 23 | coste = 0.517101\nÉpoca 24 | coste = 0.5072868\nÉpoca 25 | coste = 0.4794915\nÉpoca 26 | coste = 0.44851258\nÉpoca 27 | coste = 0.4381218\nÉpoca 28 | coste = 0.4442783\nÉpoca 29 | coste = 0.4462868\nÉpoca 30 | coste = 0.4242044\nÉpoca 31 | coste = 0.38773945\nÉpoca 32 | coste = 0.35950392\nÉpoca 33 | coste = 0.34827664\nÉpoca 34 | coste = 0.3276915\nÉpoca 35 | coste = 0.28721783\nÉpoca 36 | coste = 0.25264022\nÉpoca 37 | coste = 0.2554782\nÉpoca 38 | coste = 0.22638088\nÉpoca 39 | coste = 0.20317441\nÉpoca 40 | coste = 0.19642892\nÉpoca 41 | coste = 0.18331131\nÉpoca 42 | coste = 0.16217327\nÉpoca 43 | coste = 0.14643814\nÉpoca 44 | coste = 0.13728029\nÉpoca 45 | coste = 0.13000613\nÉpoca 46 | coste = 0.11908289\nÉpoca 47 | coste = 0.11113613\nÉpoca 48 | coste = 0.11166415\nÉpoca 49 | coste = 0.10654645\nÉpoca 50 | coste = 0.09456818\nÉpoca 51 | coste = 0.08943724\nÉpoca 52 | coste = 0.08898305\nÉpoca 53 | coste = 0.09065049\nÉpoca 54 | coste = 0.089022696\nÉpoca 55 | coste = 0.08459871\nÉpoca 56 | coste = 0.08418649\nÉpoca 57 | coste = 0.08065779\nÉpoca 58 | coste = 0.078941554\nÉpoca 59 | coste = 0.07779598\nÉpoca 60 | coste = 0.0763044\nÉpoca 61 | coste = 0.07736759\nÉpoca 62 | coste = 0.07613584\nÉpoca 63 | coste = 0.074771434\nÉpoca 64 | coste = 0.07497347\nÉpoca 65 | coste = 0.07299402\nÉpoca 66 | coste = 0.07364231\nÉpoca 67 | coste = 0.07250167\nÉpoca 68 | coste = 0.072929665\nÉpoca 69 | coste = 0.07158016\nÉpoca 70 | coste = 0.07258179\nÉpoca 71 | coste = 0.071268335\nÉpoca 72 | coste = 0.071854874\nÉpoca 73 | coste = 0.07159285\nÉpoca 74 | coste = 0.07038627\nÉpoca 75 | coste = 0.07098476\nÉpoca 76 | coste = 0.07006505\nÉpoca 77 | coste = 0.07028242\nÉpoca 78 | coste = 0.07019345\nÉpoca 79 | coste = 0.06929663\nÉpoca 80 | coste = 0.06949752\nÉpoca 81 | coste = 0.06908774\nÉpoca 82 | coste = 0.068893895\nÉpoca 83 | coste = 0.069023445\nÉpoca 84 | coste = 0.0686395\nÉpoca 85 | coste = 0.06830147\nÉpoca 86 | coste = 0.068120345\nÉpoca 87 | coste = 0.06778442\nÉpoca 88 | coste = 0.06775368\nÉpoca 89 | coste = 0.06764672\nÉpoca 90 | coste = 0.06730844\nÉpoca 91 | coste = 0.067337014\nÉpoca 92 | coste = 0.06719287\nÉpoca 93 | coste = 0.06740736\nÉpoca 94 | coste = 0.06722504\nÉpoca 95 | coste = 0.06719639\nÉpoca 96 | coste = 0.06701797\nÉpoca 97 | coste = 0.0668914\nÉpoca 98 | coste = 0.066815436\nÉpoca 99 | coste = 0.06673246\nÉpoca 100 | coste = 0.06663198\n\n\n\n\n\nDibujar la evolución de los costes durante el proceso de entrenamiento.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\nusing GLMakie\nfig = Figure()\nax = Axis(fig[1, 1], xlabel = \"Época\", ylabel = \"Error cuadrático medio\", title = \"Evolución del coste\")\nlines!(ax, costes)\nfig\n\n\n\n\n\n\n\nPredecir el precio de las acciones de los próximos 50 días usando el conjunto de prueba.\n\n\n\n\n\n\nTipSolución\n\n\n\n\n\n\ny_test, _ = modelo(Xtest, ps, st)\npredicciones = y_test .* σ .+ μ\n\nfig = Figure()\nax = Axis(fig[1,1], xlabel = \"Día\", ylabel = \"Precio de acción\", title = \"Predicciones del precio de la acción\")\nlines!(ax, df.dia, df.precio, label = \"Precio real\")\nlines!(ax, df.dia[251:end], vec(predicciones), label = \"Predicción\")\naxislegend(ax, position = :rb)\nfig",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Redes de neuronas artificiales recurrentes</span>"
    ]
  }
]