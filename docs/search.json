[
  {
    "objectID": "09-redes-neuronales.html",
    "href": "09-redes-neuronales.html",
    "title": "5Â  Redes de neuronas artificiales",
    "section": "",
    "text": "5.1 Ejercicios Resueltos\nLas redes de neuronas artificiales son un modelo computacional inspirado en el funcionamiento del cerebro humano. Una neurona artificial es una unidad de cÃ³mputo bastante simple, que recibe una serie de entradas, las procesa y produce una salida. La salida de una neurona puede ser la entrada de otra neurona, formando asÃ­ una red de neuronas interconectadas, donde cada conexiÃ³n tiene un peso asociado. Es esta red, que a veces contiene miles y millones de neuronas, la que dota de gran potencia de cÃ¡lculo a este modelo, siendo capaces de aprender patrones de datos muy complejos, como imÃ¡genes, texto o sonido, y por tanto, se utilizan a menudo en tareas de clasificaciÃ³n o regresiÃ³n.\nEl aprendizaje en una red neuronal consiste en ajustar los pesos de las conexiones para minimizar el error entre la salida predicha y la salida real. Este proceso se realiza mediante algoritmos de optimizaciÃ³n, como el del gradiente descendente que ya se vio en el capÃ­tulo de regresiÃ³n.\nPara la realizaciÃ³n de esta prÃ¡ctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Redes de neuronas artificiales</span>"
    ]
  },
  {
    "objectID": "09-redes-neuronales.html#ejercicios-resueltos",
    "href": "09-redes-neuronales.html#ejercicios-resueltos",
    "title": "5Â  Redes de neuronas artificiales",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing GLMakie  # Para el dibujo de grÃ¡ficas.\nusing MLJ # Para la creaciÃ³n y entrenamiento de modelos de aprendizaje automÃ¡tico.\nusing Flux # Para la creaciÃ³n y entrenamiento de redes neuronales.\nusing MLJFlux # Interfaz de Flux para MLJ.\nusing Optimisers # Para la optimizaciÃ³n de funciones.\nusing Statistics # Para las funciones de coste.\n\nEjercicio 5.1 El conjunto de datos viviendas.csv contiene informaciÃ³n sobre el precio de venta de viviendas en una ciudad.\n\nCargar los datos del archivo viviendas.csv en un data frame.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CSV, DataFrames\n# Creamos un data frame a partir del archivo CSV.\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/viviendas.csv\"), DataFrame)\n# Mostramos las primeras cinco filas del data frame.\nfirst(df, 5)\n\n5Ã—13 DataFrame\n\n\n\nRow\nprecio\narea\ndormitorios\nbaÃ±os\nhabitaciones\ncalleprincipal\nhuespedes\nsotano\ncalentador\nclimatizacion\ngaraje\ncentrico\namueblado\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nString3\nString3\nString3\nString3\nString3\nInt64\nString3\nString15\n\n\n\n\n1\n13300000\n7420\n4\n2\n3\nsi\nno\nno\nno\nsi\n2\nsi\namueblado\n\n\n2\n12250000\n8960\n4\n4\n4\nsi\nno\nno\nno\nsi\n3\nno\namueblado\n\n\n3\n12250000\n9960\n3\n2\n2\nsi\nno\nsi\nno\nno\n2\nsi\nsemi-amueblado\n\n\n4\n12215000\n7500\n4\n2\n2\nsi\nno\nsi\nno\nsi\n3\nsi\namueblado\n\n\n5\n11410000\n7420\n4\n1\n2\nsi\nsi\nsi\nno\nsi\n2\nno\namueblado\n\n\n\n\n\n\n\n\n\nExtraer las columnas area y precio del data frame y convertirlas a un vector de tipo Float32. Pasar el precio a miles de euros.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Extraemos las columnas area y precio.\nX = Float32.(df.area)\ny = Float32.(df.precio) ./ 1000\n\n545-element Vector{Float32}:\n 13300.0\n 12250.0\n 12250.0\n 12215.0\n 11410.0\n 10850.0\n 10150.0\n 10150.0\n  9870.0\n  9800.0\n  9800.0\n  9681.0\n  9310.0\n     â‹®\n  2100.0\n  2100.0\n  2100.0\n  1960.0\n  1890.0\n  1890.0\n  1855.0\n  1820.0\n  1767.15\n  1750.0\n  1750.0\n  1750.0\n\n\n\n\n\nDibujar un diagrama de dispersiÃ³n entre el precio y el area de las viviendas.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing GLMakie\nfig = Figure()\nax = Axis(fig[1, 1], title = \"Precio vs Area\", xlabel = \"Ãrea (mÂ²)\", ylabel = \"Precio (â‚¬)\")\nscatter!(ax, X, y)\nfig\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\nConstruir un modelo lineal simple usando un perceptrÃ³n como el de la figura, tomando como funciÃ³n de activaciÃ³n la funciÃ³n identidad.\n.\nInicializar los parÃ¡metros del modelo a 0 y dibujarlo en el diagrama de dispersiÃ³n.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Definimos el modelo lineal.\nperceptron(W, b, x) = @. W[1] * x + b[1]\n# Inicializamos los pesos y el tÃ©rmino independiente.\nW = Float32[0]\nb = Float32[0]\nlines!(ax, X, perceptron(W, b, X), label = \"Modelo 0\", color = :red)\nfig\n\n\n\n\n\n\n\nAplicar el modelo a los datos y calcular el error cuadrÃ¡tico medio del modelo.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing Statistics\n# Definimos la funciÃ³n de coste.\ncoste(W, b, X, y) = mean((y .- perceptron(W, b, X)).^2)\n# Calculamos el coste del modelo inicial.\nprintln(\"Error cuadrÃ¡tico medio: \", coste(W, b, X, y))\n\nError cuadrÃ¡tico medio: 2.6213836e7\n\n\nSe observa que el error cuadrÃ¡tico medio es bastante alto, lo que indica que el modelo no se ajusta bien a los datos.\n\n\n\nÂ¿En quÃ© direcciÃ³n deben modificarse los parÃ¡metros del modelo para reducir el error cuadrÃ¡tico medio? Actualizar los parÃ¡metros del modelo en esa direcciÃ³n utilizando una tasa de aprendizaje \\(\\eta = 10^{-9}\\). Comprobar que el error cuadrÃ¡tico medio ha disminuido con los nuevos parÃ¡metros.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLos parÃ¡metros deben modificarse en la direcciÃ³n en la que mÃ¡s rÃ¡pidamente decrezca el error cuadrÃ¡tico medio. Esta direcciÃ³n estÃ¡ dada por el gradiente del error cuadrÃ¡tico respecto a los parÃ¡metros, que se puede calcular como:\n\\[\n\\nabla E(W, b) = \\left( \\frac{\\partial E}{\\partial W}, \\frac{\\partial E}{\\partial b} \\right).\n\\]\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing Flux\n# Declaramos los pesos como variables simbÃ³licas\n\n# Calculamos el gradiente del coste.\nâˆ‚E_âˆ‚W, âˆ‚E_âˆ‚b = gradient(coste, W, b, X, y)\n# Mostramos el gradiente.\nprintln(\"Gradiente del coste: ($âˆ‚E_âˆ‚W, $âˆ‚E_âˆ‚b)\")\n# Definimos la tasa de aprendizaje.\nÎ· = 1e-8\n# Mostramos los parÃ¡metros iniciales.\nprintln(\"ParÃ¡metros iniciales: W = $W, b = $b\")\n# Actualizamos los parÃ¡metros en la direcciÃ³n de\nW -= Î· * âˆ‚E_âˆ‚W\nb -= Î· * âˆ‚E_âˆ‚b\n# Mostramos los nuevos parÃ¡metros.\nprintln(\"Nuevos parÃ¡metros: W = $W, b = $b\")\n# Comprobamos que el error cuadrÃ¡tico medio ha disminuido.\nprintln(\"Error cuadrÃ¡tico medio: \", coste(W, b, X, y))\n# Dibujamos el nuevo modelo en el diagrama de dispersiÃ³n.\nlines!(ax, X, perceptron(W, b, X), label = \"Modelo 1\")\nfig\n\nGradiente del coste: (Float32[-5.344584f7], Float32[-9533.46])\nParÃ¡metros iniciales: W = Float32[0.0], b = Float32[0.0]\nNuevos parÃ¡metros: W = [0.5344584], b = [9.5334599609375e-5]\nError cuadrÃ¡tico medio: 6.569670916877353e6\n\n\n\n\n\n\n\n\nDefinir una funciÃ³n para entrenar el perceptrÃ³n modificando los pesos en la direcciÃ³n opuesta al gradiente del error cuadrÃ¡tico medio.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nfunction entrenar_perceptron!(W, b, X, y, Î·)\n    \"\"\"\n    FunciÃ³n para entrenar el perceptrÃ³n.\n    W: peso del modelo.\n    b: tÃ©rmino independiente del modelo.\n    X: vector de entradas.\n    y: vector de salidas.\n    Î·: tasa de aprendizaje.\n    \"\"\"\n    # Calculamos el gradiente del coste.\n    âˆ‚E_âˆ‚W, âˆ‚E_âˆ‚b = gradient(coste, W, b, X, y)\n    # Actualizamos los parÃ¡metros en la direcciÃ³n opuesta al gradiente.\n    W .-= Î· * âˆ‚E_âˆ‚W\n    b .-= Î· * âˆ‚E_âˆ‚b\nend\n\nentrenar_perceptron! (generic function with 1 method)\n\n\n\n\n\nUsar la funciÃ³n anterior para entrenar el perceptrÃ³n y repetir el proceso durante 9 iteraciones mÃ¡s. Dibujar los modelos actualizados en el diagrama de dispersiÃ³n.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Repetimos el proceso de entrenamiento del perceptrÃ³n.\nfor i = 2:10\n    entrenar_perceptron!(W, b, X, y, Î·)\n    # Mostramos los parÃ¡metros y el coste del modelo.\n    ecm = coste(W, b, X, y)\n    println(\"IteraciÃ³n \", i, \", ParÃ¡metros: W = $W, b = $b, Coste: $ecm\")\n    # Dibujamos el modelo actualizado en el diagrama de dispersiÃ³n.\n    lines!(ax, X, perceptron(W, b, X), label = \"Modelo $i\")\nend\naxislegend(ax)\nfig\n\nIteraciÃ³n 2, ParÃ¡metros: W = [0.7351053379977437], b = [0.00013561418157921425], Coste: 3.8010036630367776e6\nIteraciÃ³n 3, ParÃ¡metros: W = [0.8104324229132014], b = [0.0001552249559885306], Coste: 3.4107850089995693e6\nIteraciÃ³n 4, ParÃ¡metros: W = [0.838711796053412], b = [0.00016707622479181458], Coste: 3.355787208775712e6\nIteraciÃ³n 5, ParÃ¡metros: W = [0.8493284674839952], b = [0.00017601441178095897], Coste: 3.348035761002203e6\nIteraciÃ³n 6, ParÃ¡metros: W = [0.8533141887596891], b = [0.00018385896650121626], Coste: 3.3469432599931033e6\nIteraciÃ³n 7, ParÃ¡metros: W = [0.8548105117157216], b = [0.00019129294862501068], Coste: 3.34678927740313e6\nIteraciÃ³n 8, ParÃ¡metros: W = [0.8553722621219417], b = [0.00019857279313692855], Coste: 3.3467675705099306e6\nIteraciÃ³n 9, ParÃ¡metros: W = [0.855583154313161], b = [0.00020579477113007419], Coste: 3.3467645066817994e6\nIteraciÃ³n 10, ParÃ¡metros: W = [0.855662326942257], b = [0.00021299502480003158], Coste: 3.3467640704253544e6",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Redes de neuronas artificiales</span>"
    ]
  },
  {
    "objectID": "07-arboles-decision.html",
    "href": "07-arboles-decision.html",
    "title": "4Â  Ãrboles de decisiÃ³n",
    "section": "",
    "text": "4.1 Ejercicios Resueltos\nLos Ã¡rboles de decisiÃ³n son modelos de aprendizaje simples e intuitivos que pueden utilizarse para tanto para predecir variables cuantitativas (regresiÃ³n) como categÃ³ricas (clasificaciÃ³n). Esta prÃ¡ctica contiene ejercicios que muestran como construir modelos de aprendizaje basados en Ã¡rboles de decisiÃ³n con Julia.\nPara la realizaciÃ³n de esta prÃ¡ctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Ãrboles de decisiÃ³n</span>"
    ]
  },
  {
    "objectID": "07-arboles-decision.html#ejercicios-resueltos",
    "href": "07-arboles-decision.html#ejercicios-resueltos",
    "title": "4Â  Ãrboles de decisiÃ³n",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing Tidier # Para el preprocesamiento de datos.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing GLMakie  # Para obtener grÃ¡ficos interactivos.\nusing AlgebraOfGraphics # Para generar grÃ¡ficos mediante la gramÃ¡tica de grÃ¡ficos.\nusing DecisionTree # Para construir Ã¡rboles de decisiÃ³n.\nusing GraphMakie # Para la visualizaciÃ³n de Ã¡rboles de decisiÃ³n.\n\nEjercicio 4.1 El conjunto de datos tenis.csv contiene informaciÃ³n sobre las condiciones meteorolÃ³gicas de varios dÃ­as y si se pudo jugar al tenis o no.\n\nCargar los datos del archivo tenis.csv en un data frame.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/tenis.csv\"), DataFrame)\n\n14Ã—5 DataFrame\n\n\n\nRow\nCielo\nTemperatura\nHumedad\nViento\nTenis\n\n\n\nString15\nString15\nString7\nString7\nString3\n\n\n\n\n1\nSoleado\nCaluroso\nAlta\nSuave\nNo\n\n\n2\nSoleado\nCaluroso\nAlta\nFuerte\nNo\n\n\n3\nNublado\nCaluroso\nAlta\nSuave\nSÃ­\n\n\n4\nLluvioso\nModerado\nAlta\nSuave\nSÃ­\n\n\n5\nLluvioso\nFrÃ­o\nNormal\nSuave\nSÃ­\n\n\n6\nLluvioso\nFrÃ­o\nNormal\nFuerte\nNo\n\n\n7\nNublado\nFrÃ­o\nNormal\nFuerte\nSÃ­\n\n\n8\nSoleado\nModerado\nAlta\nSuave\nNo\n\n\n9\nSoleado\nFrÃ­o\nNormal\nSuave\nSÃ­\n\n\n10\nLluvioso\nModerado\nNormal\nSuave\nSÃ­\n\n\n11\nSoleado\nModerado\nNormal\nFuerte\nSÃ­\n\n\n12\nNublado\nModerado\nAlta\nFuerte\nSÃ­\n\n\n13\nNublado\nCaluroso\nNormal\nSuave\nSÃ­\n\n\n14\nLluvioso\nModerado\nAlta\nFuerte\nNo\n\n\n\n\n\n\n\n\n\nCrear un diagrama de barras que muestre la distribuciÃ³n de frecuencias de cada variable meteorolÃ³gica segÃºn si se pudo jugar al tenis o no. Â¿QuÃ© variable meteorolÃ³gica parece tener mÃ¡s influencia en la decisiÃ³n de jugar al tenis?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing GLMakie, AlgebraOfGraphics\n\nfunction frecuencias(df::DataFrame, var::Symbol)\n    # Calculamos el nÃºmero de dÃ­as de cada clase que se juega al tenis.\n    frec = combine(groupby(df, [var, :Tenis]), nrow =&gt; :DÃ­as)\n    # Dibujamos el diagrama de barras.\n    plt = data(frec) * \n    mapping(var, :DÃ­as, stack = :Tenis, color = :Tenis, ) * \n    visual(BarPlot) \n    # Devolvemos el grÃ¡fico.\n    return plt\nend\n\nfig = Figure()\ndraw!(fig[1, 1], frecuencias(df, :Cielo))\ndraw!(fig[1, 2], frecuencias(df, :Temperatura))\ndraw!(fig[1, 3], frecuencias(df, :Humedad))\ndraw!(fig[1, 4], frecuencias(df, :Viento))\nfig\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nA la vista de las frecuencias de cada variable, las variable Cielo y Humedad parecen ser las que mÃ¡s influye en la decisiÃ³n de jugar al tenis.\n\n\n\nCalcular la impureza del conjunto de datos utilizando el Ã­ndice de Gini. Â¿QuÃ© variable meteorolÃ³gica parece tener mÃ¡s influencia en la decisiÃ³n de jugar al tenis?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEl Ã­ndice de Gini se calcula mediante la fÃ³rmula\n\\[ GI = 1 - \\sum_{i=1}^{n} p_i^2 \\]\ndonde \\(p_i\\) es la proporciÃ³n de cada clase en el conjunto de datos y \\(n\\) es el nÃºmero de clases.\nEl Ã­ndice de Gini toma valores entre \\(0\\) y \\(1-\\frac{1}{n}\\) (\\(0.5\\) en el caso de clasificaciÃ³n binaria), donde \\(0\\) indica que todas las instancias pertenecen a una sola clase (mÃ­nima impureza) y \\(1-\\frac{1}{n}\\) indica que las instancias estÃ¡n distribuidas uniformemente entre todas las clases (mÃ¡xima impureza).\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nfunction gini(df::DataFrame, var::Symbol)\n    # Calculamos el nÃºmero de ejemplos.\n    n = nrow(df)\n    # Calculamos las frecuencias absolutas de cada clase.\n    frec = combine(groupby(df, var), nrow =&gt; :ni)\n    # Calculamos la proporciÃ³n de cada clase.\n    frec.p = frec.ni ./ n\n    # Calculamos el Ã­ndice de Gini.\n    gini = 1 - sum(frec.p .^ 2)\n    return gini\nend\n\ng0 = gini(df, :Tenis)\n\n0.4591836734693877\n\n\n\n\n\nÂ¿QuÃ© reducciÃ³n del Ã­ndice Gini se obtiene si dividimos el conjunto de ejemplos segÃºn la variable Humedad? Â¿Y si dividimos el conjunto con respecto a la variable Viento?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLa reducciÃ³n del Ã­ndice de Gini se calcula como la diferencia entre el Ã­ndice de Gini del conjunto original y el Ã­ndice de Gini del conjunto dividido.\n\\[ \\Delta GI = GI_{original} - GI_{dividido} \\]\ndonde el Ã­ndice de Gini del conjunto dividido es la media ponderada de los Ã­ndices de Gini de los subconjuntos resultantes de la divisiÃ³n.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\nCalculamos primero la reducciÃ³n del Ã­ndice de Gini al dividir el conjunto de ejemplos segÃºn la variable Humedad.\n\nusing Tidier\n# Dividimos el conjunto de ejemplos segÃºn la variable Humedad.\ndf_humedad_alta = @filter(df, Humedad == \"Alta\")\ndf_humedad_normal = @filter(df, Humedad == \"Normal\")\n# Calculamos los tamaÃ±os de los subconjuntos de ejemplos.\nn = nrow(df_humedad_alta), nrow(df_humedad_normal)\n# Calculamos el Ã­ndice de Gini de cada subconjunto.\ngis = gini(df_humedad_alta, :Tenis), gini(df_humedad_normal, :Tenis)\n# Calculamos media ponderada de los Ã­ndices de Gini de los subconjuntos \ng_humedad = sum(gis .* n) / sum(n)\n# Calculamos la reducciÃ³n del Ã­ndice de Gini.\ng0 - g_humedad\n\n0.09183673469387743\n\n\nCalculamos ahora la reducciÃ³n del Ã­ndice de Gini al dividir el conjunto de ejemplos segÃºn la variable Viento.\n\n# Dividimos el conjunto de ejemplos segÃºn la variable `Viento`\ndf_viento_fuerte = @filter(df, Viento == \"Fuerte\")\ndf_viento_suave = @filter(df, Viento == \"Suave\")\n# Calculamos los tamaÃ±os de los subconjuntos de ejemplos\nn = nrow(df_viento_fuerte), nrow(df_viento_suave)\n# Calculamos el Ã­ndice de Gini de cada subconjunto\ngis = gini(df_viento_fuerte, :Tenis), gini(df_viento_suave, :Tenis)\n# Calculamos media ponderada de los Ã­ndices de Gini de los subconjuntos\ng_viento = sum(gis .* n) / sum(n)\n# Calculamos la reducciÃ³n del Ã­ndice de Gini\ng0 - g_viento\n\n0.030612244897959162\n\n\nComo se puede observar, la reducciÃ³n del Ã­ndice de Gini al dividir el conjunto de ejemplos segÃºn la variable Humedad es mayor que la reducciÃ³n del Ã­ndice de Gini al dividir el conjunto con respecto a la variable Viento. Por lo tanto, la variable Humedad parece tener mÃ¡s influencia en la decisiÃ³n de jugar al tenis y serÃ­a la variable que se deberÃ­a elegir para dividir el conjunto de ejemplos.\n\n\n\nConstruir un Ã¡rbol de decisiÃ³n que explique si se puede jugar al tenis en funciÃ³n de las variables meteorolÃ³gicas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n DecisionTreeClassifier del paquete DecisionTree.jl.\nLos parÃ¡metros mÃ¡s importantes de esta funciÃ³n son:\n\nmax_depth: Profundidad mÃ¡xima del Ã¡rbol. Si no se indica, el Ã¡rbol crecerÃ¡ hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min_samples_split ejemplos.\nmin_samples_leaf: NÃºmero mÃ­nimo de ejemplos en una hoja (1 por defecto).\nmin_samples_split: NÃºmero mÃ­nimo de ejemplos para dividir un nodo (2 por defecto).\nmin_impurity_decrease: ReducciÃ³n mÃ­nima de la impureza para dividir un nodo (0 por defecto).\npost-prune: Si se indica true, se poda el Ã¡rbol despuÃ©s de que se ha construido. La poda reduce el tamaÃ±o del Ã¡rbol eliminando nodos que no aportan informaciÃ³n Ãºtil.\nmerge_purity_threshold: Umbral de pureza para fusionar nodos. Si se indica, se fusionan los nodos que tienen una pureza menor que este umbral.\nfeature_importance: Indica la medida para calcular la importancia de las variables a la hora de dividir el conjunto de datos. Puede ser :impurity o :split. Si no se indica, se utiliza la impureza de Gini.\nrng: Indica la semilla para la generaciÃ³n de nÃºmeros aleatorios. Si no se indica, se utiliza el generador de nÃºmeros aleatorios por defecto.\n\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing DecisionTree, CategoricalArrays\n# Variables predictoras.\nX = Matrix(select(df, Not(:Tenis)))\n# Variable objetivo.\ny = df.Tenis\n# Convertir las variables categÃ³ricas a enteros.\nX = hcat([levelcode.(categorical(X[:, j])) for j in 1:size(X, 2)]...)\n# Convertir la variable objetivo a enteros.\ny = levelcode.(categorical(y))\ntree = DecisionTreeClassifier(max_depth=3)\nfit!(tree, X, y)\n\nDecisionTreeClassifier\nmax_depth:                3\nmin_samples_leaf:         1\nmin_samples_split:        2\nmin_purity_increase:      0.0\npruning_purity_threshold: 1.0\nn_subfeatures:            0\nclasses:                  [1, 2]\nroot:                     Decision Tree\nLeaves: 6\nDepth:  3\n\n\n\n\n\nVisualizar el Ã¡rbol de decisiÃ³n construido.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n plot_tree del paquete DecisionTree.jl.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nprint_tree(tree, feature_names=names(df)[1:end-1])\n\nFeature 3: \"Humedad\" &lt; 2.0 ?\nâ”œâ”€ Feature 1: \"Cielo\" &lt; 3.0 ?\n    â”œâ”€ Feature 4: \"Viento\" &lt; 2.0 ?\n        â”œâ”€ 2 : 1/2\n        â””â”€ 2 : 2/2\n    â””â”€ 1 : 3/3\nâ””â”€ Feature 1: \"Cielo\" &lt; 2.0 ?\n    â”œâ”€ Feature 4: \"Viento\" &lt; 2.0 ?\n        â”œâ”€ 1 : 1/1\n        â””â”€ 2 : 2/2\n    â””â”€ 2 : 4/4\n\n\n\n\n\n\n\n\nEjercicio 4.2 El conjunto de datos pingÃ¼inos.csv contiene un conjunto de datos sobre tres especies de pingÃ¼inos con las siguientes variables:\n\nEspecie: Especie de pingÃ¼ino, comÃºnmente Adelie, Chinstrap o Gentoo.\nIsla: Isla del archipiÃ©lago Palmer donde se realizÃ³ la observaciÃ³n.\nLongitud_pico: Longitud del pico en mm.\nProfundidad_pico: Profundidad del pico en mm\nLongitud_ala: Longitud de la aleta en mm.\nPeso: Masa corporal en gramos.\nSexo: Sexo\n\n\nCargar los datos del archivo pinguÃ¯nos.csv en un data frame.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/pingÃ¼inos.csv\"), DataFrame, missingstring=\"NA\")\n\n344Ã—7 DataFrame319 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64?\nFloat64?\nInt64?\nInt64?\nString7?\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmacho\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nhembra\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nhembra\n\n\n4\nAdelie\nTorgersen\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nhembra\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmacho\n\n\n7\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nhembra\n\n\n8\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmacho\n\n\n9\nAdelie\nTorgersen\n34.1\n18.1\n193\n3475\nmissing\n\n\n10\nAdelie\nTorgersen\n42.0\n20.2\n190\n4250\nmissing\n\n\n11\nAdelie\nTorgersen\n37.8\n17.1\n186\n3300\nmissing\n\n\n12\nAdelie\nTorgersen\n37.8\n17.3\n180\n3700\nmissing\n\n\n13\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nhembra\n\n\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\n\n\n333\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nhembra\n\n\n334\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmacho\n\n\n335\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmacho\n\n\n336\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nhembra\n\n\n337\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmacho\n\n\n338\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nhembra\n\n\n339\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nhembra\n\n\n340\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n341\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n342\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmacho\n\n\n343\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmacho\n\n\n344\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nhembra\n\n\n\n\n\n\n\n\n\nHacer un anÃ¡lisis de los datos perdidos en el data frame.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndescribe(df, :nmissing)\n\n7Ã—2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\nEspecie\n0\n\n\n2\nIsla\n0\n\n\n3\nLongitud_pico\n2\n\n\n4\nProfundidad_pico\n2\n\n\n5\nLongitud_ala\n2\n\n\n6\nPeso\n2\n\n\n7\nSexo\n11\n\n\n\n\n\n\n\n\n\nEliminar del data frame los casos con valores perdidos.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndropmissing!(df)\n\n333Ã—7 DataFrame308 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64\nFloat64\nInt64\nInt64\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmacho\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nhembra\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nhembra\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nhembra\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmacho\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nhembra\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmacho\n\n\n8\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nhembra\n\n\n9\nAdelie\nTorgersen\n38.6\n21.2\n191\n3800\nmacho\n\n\n10\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmacho\n\n\n11\nAdelie\nTorgersen\n36.6\n17.8\n185\n3700\nhembra\n\n\n12\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nhembra\n\n\n13\nAdelie\nTorgersen\n42.5\n20.7\n197\n4500\nmacho\n\n\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\n\n\n322\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nhembra\n\n\n323\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmacho\n\n\n324\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmacho\n\n\n325\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nhembra\n\n\n326\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmacho\n\n\n327\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nhembra\n\n\n328\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nhembra\n\n\n329\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n330\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n331\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmacho\n\n\n332\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmacho\n\n\n333\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nhembra\n\n\n\n\n\n\n\n\n\nCrear diagramas que muestren la distribuciÃ³n de frecuencias de cada variable segÃºn la especie de pingÃ¼ino. Â¿QuÃ© variable parece tener mÃ¡s influencia en la especie de pingÃ¼ino?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\nPara las variables cualitativas dibujamos diagramas de barras.\n\nusing GLMakie, AlgebraOfGraphics\n\nfrec_isla = combine(groupby(df, [:Isla, :Especie]), nrow =&gt; :Frecuencia)\ndata(frec_isla) * \n    mapping(:Isla, :Frecuencia, stack = :Especie, color =:Especie) *\n    visual(BarPlot) |&gt; draw\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\nfrec_sexo = combine(groupby(df, [:Sexo, :Especie]), nrow =&gt; :Frecuencia)\ndata(frec_sexo) * \n    mapping(:Sexo, :Frecuencia, stack = :Especie, color =:Especie) *\n    visual(BarPlot) |&gt; draw\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nPara las variables cuantitativas dibujamos diagramas de cajas.\n\nfunction cajas(df, var, clase)\n    data(df) *\n        mapping(clase, var, color = clase) *\n        visual(BoxPlot) |&gt; \n        draw\nend\n\ncajas(df, :Longitud_pico, :Especie)\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Profundidad_pico, :Especie)\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Longitud_ala, :Especie)\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Peso, :Especie)\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\n\nÂ¿CuÃ¡l es la reducciÃ³n de la impureza del conjunto de datos si dividimos el conjunto de datos en dos conjuntos segÃºn si la longitud del pico es mayor o menor que 44 mm?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing Tidier\nfunction gini(df::DataFrame, var::Symbol)\n    n = nrow(df)\n    frec = combine(groupby(df, var), nrow =&gt; :ni)\n    frec.p = frec.ni ./ n\n    gini = 1 - sum(frec.p .^ 2)\n    return gini\nend\n\nfunction reduccion_impureza(df::DataFrame, var::Symbol, val::Number)\n    # Dividimos el conjunto de ejemplos segÃºn la longitud del pico es menor de 44.\n    df_menor = @eval @filter($df, $var &lt;= $val)\n    df_mayor = @eval @filter($df, $var &gt; $val)\n    # Calculamos los tamaÃ±os de los subconjuntos de ejemplos.\n    n = nrow(df_menor), nrow(df_mayor)\n    # Calculamos el Ã­ndice de Gini de cada subconjunto.\n    gis = gini(df_menor, :Especie), gini(df_mayor, :Especie)\n    # Calculamos media ponderada de los Ã­ndices de Gini de los subconjuntos.\n    g1 = sum(gis .* n) / sum(n)\n    # Calculamos la reducciÃ³n del Ã­ndice de Gini.\n    gini(df, :Especie) - g1\nend\n\nreduccion_impureza(df, :Longitud_pico, 44)\n\n0.26577182779353914\n\n\n\n\n\nDeterminar el valor Ã³ptimo de divisiÃ³n del conjunto de datos segÃºn la longitud del pico. Para ello, calcular la reducciÃ³n de la impureza para cada valor de longitud del pico y dibujar el resultado.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\nDibujamos la reducciÃ³n de la impureza en funciÃ³n de la longitud del pico.\n\n# Valores Ãºnicos de longitud del pico.\nvalores = unique(df.Longitud_pico)\n# ReducciÃ³n de la impureza para cada valor.\nreducciones = [reduccion_impureza(df, :Longitud_pico, val) for val in valores]\n# Graficamos el resultado.\nusing GLMakie\nfig = Figure()\nax = Axis(fig[1, 1], title = \"ReducciÃ³n de la impureza segÃºn la longitud del pico\", xlabel = \"Longitud del pico\", ylabel = \"ReducciÃ³n de la impureza\")\nscatter!(ax, valores, reducciones)\n\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\nScatter{Tuple{Vector{Point{2, Float64}}}}\n\n\nY ahora obtenemos el valor Ã³ptimo de divisiÃ³n del conjunto de datos segÃºn la longitud del pico.\n\nval_optimo = valores[argmax(reducciones)]\n\n42.3\n\n\n\n\n\nDividir aleatoriamente el dataframe en un conjunto de entrenamiento y un conjunto de test con proporciones \\(3/4\\) y \\(1/4\\) respectivamente.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n shuffle del paquete Random para barajar el dataframe y luego dividirlo en dos subconjuntos.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing Random\n# Establecemos la semilla para la reproducibilidad.\nRandom.seed!(1234)\n# Barajamos el dataframe.\ndf = shuffle(df)\n# Dividimos el dataframe en un conjunto de entrenamiento y un conjunto de test.\nn = nrow(df)\ndf_test = df[1:div(n, 4), :]\ndf_train = df[div(n, 4)+1:end, :]\n\n250Ã—7 DataFrame225 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64\nFloat64\nInt64\nInt64\nString7\n\n\n\n\n1\nAdelie\nDream\n39.0\n18.7\n185\n3650\nmacho\n\n\n2\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmacho\n\n\n3\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n4\nAdelie\nTorgersen\n35.1\n19.4\n193\n4200\nmacho\n\n\n5\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmacho\n\n\n6\nGentoo\nBiscoe\n50.0\n15.2\n218\n5700\nmacho\n\n\n7\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmacho\n\n\n8\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n9\nAdelie\nDream\n36.9\n18.6\n189\n3500\nhembra\n\n\n10\nAdelie\nDream\n36.6\n18.4\n184\n3475\nhembra\n\n\n11\nChinstrap\nDream\n46.6\n17.8\n193\n3800\nhembra\n\n\n12\nGentoo\nBiscoe\n50.8\n17.3\n228\n5600\nmacho\n\n\n13\nChinstrap\nDream\n52.2\n18.8\n197\n3450\nmacho\n\n\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\n\n\n239\nAdelie\nDream\n39.8\n19.1\n184\n4650\nmacho\n\n\n240\nAdelie\nTorgersen\n43.1\n19.2\n197\n3500\nmacho\n\n\n241\nChinstrap\nDream\n49.8\n17.3\n198\n3675\nhembra\n\n\n242\nGentoo\nBiscoe\n49.8\n15.9\n229\n5950\nmacho\n\n\n243\nChinstrap\nDream\n50.8\n18.5\n201\n4450\nmacho\n\n\n244\nGentoo\nBiscoe\n50.7\n15.0\n223\n5550\nmacho\n\n\n245\nGentoo\nBiscoe\n46.2\n14.1\n217\n4375\nhembra\n\n\n246\nAdelie\nTorgersen\n35.5\n17.5\n190\n3700\nhembra\n\n\n247\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmacho\n\n\n248\nGentoo\nBiscoe\n47.7\n15.0\n216\n4750\nhembra\n\n\n249\nAdelie\nTorgersen\n42.9\n17.6\n196\n4700\nmacho\n\n\n250\nAdelie\nDream\n40.8\n18.9\n208\n4300\nmacho\n\n\n\n\n\n\n\n\n\nConstruir un Ã¡rbol de decisiÃ³n con el conjunto de entrenamiento sin tener en cuenta la variable Isla y visualizarlo.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing DecisionTree, CategoricalArrays\n# Variables predictivas.\nX_train = Matrix(select(df_train, Not(:Isla, :Especie)))\n# Variable objetivo.\ny_train = df_train.Especie\n# Convertir las variables categÃ³ricas a enteros.\nX_train = hcat([levelcode.(categorical(X_train[:, j])) for j in 1:size(X_train, 2)]...)\n# Convertir la variable objetivo a enteros\ny_train = levelcode.(categorical(y_train))\n\n# Construimos el Ã¡rbol de decisiÃ³n con profundidad mÃ¡xima 3.\ntree = DecisionTreeClassifier(max_depth = 3)\nfit!(tree, X_train, y_train)\nprint_tree(tree, feature_names=names(df)[3:end])\n\nFeature 3: \"Longitud_ala\" &lt; 29.0 ?\nâ”œâ”€ Feature 1: \"Longitud_pico\" &lt; 62.0 ?\n    â”œâ”€ 1 : 96/96\n    â””â”€ Feature 1: \"Longitud_pico\" &lt; 87.0 ?\n        â”œâ”€ 2 : 10/20\n        â””â”€ 2 : 37/38\nâ””â”€ Feature 2: \"Profundidad_pico\" &lt; 46.0 ?\n    â”œâ”€ 3 : 90/90\n    â””â”€ Feature 1: \"Longitud_pico\" &lt; 109.0 ?\n        â”œâ”€ 1 : 2/2\n        â””â”€ 2 : 4/4\n\n\n\n\n\nPredecir la especie de los pingÃ¼inos del conjunto de test y calcular la matriz de confusiÃ³n de las predicciones.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n confmat del paquete StatisticalMeaures para barajar el dataframe y luego dividirlo en dos subconjuntos.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing StatisticalMeasures\n# Variables predictivas\nX_test = Matrix(select(df_test, Not(:Isla, :Especie)))\n# Variable objetivo\ny_test = df_test.Especie\n# Convertir las variables categÃ³ricas a enteros\nX_test = hcat([levelcode.(categorical(X_test[:, j])) for j in 1:size(X_test, 2)]...)\n# Convertir la variable objetivo a enteros\ny_test = levelcode.(categorical(y_test))\n# Predecimos la especie de pingÃ¼ino del conjunto de test\ny_pred = predict(tree, X_test)\n# Calculamos la precisiÃ³n del modelo\nconfmat(y_pred, y_test)\n\n          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n          â”‚ Ground Truth â”‚\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¤\nâ”‚Predictedâ”‚ 1  â”‚ 2  â”‚ 3  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚    1    â”‚ 38 â”‚ 11 â”‚ 9  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚    2    â”‚ 0  â”‚ 6  â”‚ 0  â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤\nâ”‚    3    â”‚ 0  â”‚ 0  â”‚ 19 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜\n\n\n\n\n\nCalcular la precisiÃ³n del modelo.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLa precisiÃ³n es la proporciÃ³n de predicciones correctas sobre el total de predicciones.\nUtilizar la funciÃ³n accuracy del paquete StatisticalMeaures para calcular la precisiÃ³n del modelo.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Calculamos la precisiÃ³n del modelo\naccuracy(y_pred, y_test)\n\n0.7590361445783133\n\n\n\n\n\n\n\n\nEjercicio 4.3 El fichero vinos.csv contiene informaciÃ³n sobre las caracterÃ­sticas de una muestra de vinos portugueses de la denominaciÃ³n â€œVinho Verdeâ€. Las variables que contiene son:\n\n\n\n\n\n\n\n\nVariable\nDescripciÃ³n\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nCategÃ³rica (blanco, tinto)\n\n\nmeses.barrica\nMesesde envejecimiento en barrica\nNumÃ©rica(meses)\n\n\nacided.fija\nCantidadde Ã¡cidotartÃ¡rico\nNumÃ©rica(g/dm3)\n\n\nacided.volatil\nCantidad de Ã¡cido acÃ©tico\nNumÃ©rica(g/dm3)\n\n\nacido.citrico\nCantidad de Ã¡cidocÃ­trico\nNumÃ©rica(g/dm3)\n\n\nazucar.residual\nCantidad de azÃºcarremanente despuÃ©s de la fermentaciÃ³n\nNumÃ©rica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosÃ³dico\nNumÃ©rica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de diÃ³xido de azufreen formalibre\nNumÃ©rica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidadde diÃ³xido de azufretotal en forma libre o ligada\nNumÃ©rica(mg/dm3)\n\n\ndensidad\nDensidad\nNumÃ©rica(g/cm3)\n\n\nph\npH\nNumÃ©rica(0-14)\n\n\nsulfatos\nCantidadde sulfato de potasio\nNumÃ©rica(g/dm3)\n\n\nalcohol\nPorcentajede contenidode alcohol\nNumÃ©rica(0-100)\n\n\ncalidad\nCalificaciÃ³n otorgada porun panel de expertos\nNumÃ©rica(0-10)\n\n\n\n\nCrear un data frame con los datos de los vinos a partir del fichero vinos.csv.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/vinos.csv\"), DataFrame)\n\n5320Ã—14 DataFrame5295 rows omitted\n\n\n\nRow\ntipo\nmeses_barrica\nacided_fija\nacided_volatil\nacido_citrico\nazucar_residual\ncloruro_sodico\ndioxido_azufre_libre\ndioxido_azufre_total\ndensidad\nph\nsulfatos\nalcohol\ncalidad\n\n\n\nString7\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nblanco\n0\n7.0\n0.27\n0.36\n20.7\n0.045\n45.0\n170.0\n1.001\n3.0\n0.45\n8.8\n6\n\n\n2\nblanco\n0\n6.3\n0.3\n0.34\n1.6\n0.049\n14.0\n132.0\n0.994\n3.3\n0.49\n9.5\n6\n\n\n3\nblanco\n0\n8.1\n0.28\n0.4\n6.9\n0.05\n30.0\n97.0\n0.9951\n3.26\n0.44\n10.1\n6\n\n\n4\nblanco\n0\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.4\n9.9\n6\n\n\n5\nblanco\n0\n6.2\n0.32\n0.16\n7.0\n0.045\n30.0\n136.0\n0.9949\n3.18\n0.47\n9.6\n6\n\n\n6\nblanco\n0\n8.1\n0.22\n0.43\n1.5\n0.044\n28.0\n129.0\n0.9938\n3.22\n0.45\n11.0\n6\n\n\n7\nblanco\n0\n8.1\n0.27\n0.41\n1.45\n0.033\n11.0\n63.0\n0.9908\n2.99\n0.56\n12.0\n5\n\n\n8\nblanco\n0\n8.6\n0.23\n0.4\n4.2\n0.035\n17.0\n109.0\n0.9947\n3.14\n0.53\n9.7\n5\n\n\n9\nblanco\n0\n7.9\n0.18\n0.37\n1.2\n0.04\n16.0\n75.0\n0.992\n3.18\n0.63\n10.8\n5\n\n\n10\nblanco\n0\n6.6\n0.16\n0.4\n1.5\n0.044\n48.0\n143.0\n0.9912\n3.54\n0.52\n12.4\n7\n\n\n11\nblanco\n0\n8.3\n0.42\n0.62\n19.25\n0.04\n41.0\n172.0\n1.0002\n2.98\n0.67\n9.7\n5\n\n\n12\nblanco\n0\n6.6\n0.17\n0.38\n1.5\n0.032\n28.0\n112.0\n0.9914\n3.25\n0.55\n11.4\n7\n\n\n13\nblanco\n0\n6.3\n0.48\n0.04\n1.1\n0.046\n30.0\n99.0\n0.9928\n3.24\n0.36\n9.6\n6\n\n\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\n\n\n5309\ntinto\n7\n7.5\n0.31\n0.41\n2.4\n0.065\n34.0\n60.0\n0.99492\n3.34\n0.85\n11.4\n6\n\n\n5310\ntinto\n7\n5.8\n0.61\n0.11\n1.8\n0.066\n18.0\n28.0\n0.99483\n3.55\n0.66\n10.9\n6\n\n\n5311\ntinto\n10\n7.2\n0.66\n0.33\n2.5\n0.068\n34.0\n102.0\n0.99414\n3.27\n0.78\n12.8\n6\n\n\n5312\ntinto\n3\n6.6\n0.725\n0.2\n7.8\n0.073\n29.0\n79.0\n0.9977\n3.29\n0.54\n9.2\n5\n\n\n5313\ntinto\n7\n6.3\n0.55\n0.15\n1.8\n0.077\n26.0\n35.0\n0.99314\n3.32\n0.82\n11.6\n6\n\n\n5314\ntinto\n9\n5.4\n0.74\n0.09\n1.7\n0.089\n16.0\n26.0\n0.99402\n3.67\n0.56\n11.6\n6\n\n\n5315\ntinto\n3\n6.3\n0.51\n0.13\n2.3\n0.076\n29.0\n40.0\n0.99574\n3.42\n0.75\n11.0\n6\n\n\n5316\ntinto\n3\n6.8\n0.62\n0.08\n1.9\n0.068\n28.0\n38.0\n0.99651\n3.42\n0.82\n9.5\n6\n\n\n5317\ntinto\n5\n6.2\n0.6\n0.08\n2.0\n0.09\n32.0\n44.0\n0.9949\n3.45\n0.58\n10.5\n5\n\n\n5318\ntinto\n10\n5.9\n0.55\n0.1\n2.2\n0.062\n39.0\n51.0\n0.99512\n3.52\n0.76\n11.2\n6\n\n\n5319\ntinto\n6\n5.9\n0.645\n0.12\n2.0\n0.075\n32.0\n44.0\n0.99547\n3.57\n0.71\n10.2\n5\n\n\n5320\ntinto\n3\n6.0\n0.31\n0.47\n3.6\n0.067\n18.0\n42.0\n0.99549\n3.39\n0.66\n11.0\n6\n\n\n\n\n\n\n\n\n\nMostrar los tipos de cada variable del data frame.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n schema del paquete MLJ.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing MLJ\nschema(df)\n\nWARNING: using MLJ.fit! in module Main conflicts with an existing identifier.\nWARNING: using MLJ.predict in module Main conflicts with an existing identifier.\n\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names                â”‚ scitypes   â”‚ types   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ tipo                 â”‚ Textual    â”‚ String7 â”‚\nâ”‚ meses_barrica        â”‚ Count      â”‚ Int64   â”‚\nâ”‚ acided_fija          â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ acided_volatil       â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ acido_citrico        â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ azucar_residual      â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ cloruro_sodico       â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ dioxido_azufre_libre â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ dioxido_azufre_total â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ densidad             â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ ph                   â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ sulfatos             â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ alcohol              â”‚ Continuous â”‚ Float64 â”‚\nâ”‚ calidad              â”‚ Count      â”‚ Int64   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\nHacer un anÃ¡lisis de los datos perdidos en el data frame.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndescribe(df, :nmissing)\n\n14Ã—2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\ntipo\n0\n\n\n2\nmeses_barrica\n0\n\n\n3\nacided_fija\n0\n\n\n4\nacided_volatil\n0\n\n\n5\nacido_citrico\n0\n\n\n6\nazucar_residual\n0\n\n\n7\ncloruro_sodico\n0\n\n\n8\ndioxido_azufre_libre\n0\n\n\n9\ndioxido_azufre_total\n0\n\n\n10\ndensidad\n0\n\n\n11\nph\n0\n\n\n12\nsulfatos\n0\n\n\n13\nalcohol\n0\n\n\n14\ncalidad\n0\n\n\n\n\n\n\n\n\n\nSe considera que un vino es bueno si tiene una puntuaciÃ³n de calidad mayor que \\(6.5\\). Recodificar la variable calidad en una variable categÃ³rica que tome el valor 1 si la calidad es mayor que \\(6.5\\) y 0 en caso contrario.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CategoricalArrays\n# Recodificamos la variable calidad.\ndf.calidad = cut(df.calidad, [0, 6.5, 10], labels = [\" â˜¹ï¸ \", \" ğŸ˜Š \"])\n\n5320-element CategoricalArray{String,1,UInt32}:\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" ğŸ˜Š \"\n \" â˜¹ï¸ \"\n \" ğŸ˜Š \"\n \" â˜¹ï¸ \"\n â‹®\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n\n\n\n\n\nDividir el data frame en un data frame con las variables predictivas y un vector con la variable objetivo bueno.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n unpack del paquete MLJ para dividir el data frame en dos partes, una con las columnas de entrada del modelo y otra con la columna de salida.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ny, X = unpack(df, ==(:calidad), rng = 123)\n\n\n(CategoricalValue{String, UInt32}[\" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" ğŸ˜Š \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \"  â€¦  \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \", \" â˜¹ï¸ \"], 5320Ã—13 DataFrame\n  Row â”‚ tipo     meses_barrica  acided_fija  acided_volatil  acido_citrico  az â‹¯\n      â”‚ String7  Int64          Float64      Float64         Float64        Fl â‹¯\nâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    1 â”‚ blanco               0          6.7           0.5             0.36     â‹¯\n    2 â”‚ blanco               0          6.3           0.2             0.3\n    3 â”‚ blanco               0          6.2           0.35            0.03\n    4 â”‚ tinto                3          8.0           0.39            0.3\n    5 â”‚ blanco               0          7.9           0.255           0.26     â‹¯\n    6 â”‚ blanco               0          6.1           0.31            0.37\n    7 â”‚ blanco               0          6.8           0.28            0.36\n    8 â”‚ blanco               0          8.2           0.34            0.49\n    9 â”‚ tinto                0          6.7           0.48            0.02     â‹¯\n   10 â”‚ blanco               0          7.4           0.35            0.2\n   11 â”‚ tinto                5          7.5           0.53            0.06\n  â‹®   â”‚    â‹®           â‹®             â‹®             â‹®               â‹®           â‹±\n 5311 â”‚ blanco               0          7.2           0.14            0.35\n 5312 â”‚ tinto                3          7.6           0.41            0.24     â‹¯\n 5313 â”‚ tinto                0          7.3           0.4             0.3\n 5314 â”‚ tinto                4          7.1           0.48            0.28\n 5315 â”‚ blanco               0          6.4           0.29            0.2\n 5316 â”‚ blanco               0          9.4           0.24            0.29     â‹¯\n 5317 â”‚ blanco               0          6.3           0.25            0.27\n 5318 â”‚ blanco               0          5.5           0.16            0.26\n 5319 â”‚ blanco               0          7.4           0.36            0.32\n 5320 â”‚ blanco               0          7.6           0.51            0.24     â‹¯\n                                                 8 columns and 5299 rows omitted)\n\n\n\n\n\n\nPara poder entrenar un modelo de un arbol de decisiÃ³n, las variables predictivas deben ser cuantitativas. Transmformar las variables categÃ³ricas en variables numÃ©ricas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n coerce! del paquete MLJ para transformar las variables categÃ³ricas en variables numÃ©ricas.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Convertir las variables categÃ³ricas a enteros.\ncoerce!(X, :tipo =&gt; OrderedFactor, :meses_barrica =&gt; Continuous)\nschema(X)\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ names                â”‚ scitypes         â”‚ types                             â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ tipo                 â”‚ OrderedFactor{2} â”‚ CategoricalValue{String7, UInt32} â”‚\nâ”‚ meses_barrica        â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ acided_fija          â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ acided_volatil       â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ acido_citrico        â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ azucar_residual      â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ cloruro_sodico       â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ dioxido_azufre_libre â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ dioxido_azufre_total â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ densidad             â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ ph                   â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ sulfatos             â”‚ Continuous       â”‚ Float64                           â”‚\nâ”‚ alcohol              â”‚ Continuous       â”‚ Float64                           â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\nDefinir un modelo de Ã¡rbol de decisiÃ³n con profundidad mÃ¡xima 3.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nCargar el modelo DecisionTreeClassifier del paquete DecisionTree con la macros @iload.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Cargamos el tipo de modelo.\nTree = @iload DecisionTreeClassifier pkg = \"DecisionTree\"\n# Instanciamos el modelo con sus parÃ¡metros.\narbol = Tree(max_depth =3, rng = 123)\n\nimport MLJDecisionTreeInterface âœ”\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\n\n\n\nDecisionTreeClassifier(\n  max_depth = 3, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = 123)\n\n\n\n\n\nEvaluar el modelo tomando un 70% de ejemplos en el conjunto de entrenamiento y un 30% en el conjunto de test. Utilizar como mÃ©trica la precisiÃ³n.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n evaluate del paquete MLJ para evaluar el modelo. Los parÃ¡metros mÃ¡s importantes de esta funciÃ³n son:\n\nresampling: Indica el mÃ©todo de muestreo para definir los conjuntos de entrenamiento y test. Los mÃ©todos mÃ¡s habituales son:\n\nHoldout(fraction_train = p): Divide el conjunto de datos tomando una proporciÃ³n de \\(p\\) ejemplos en el conjunto de entrenamiento y \\(1-p\\) en el conjunto de test.\nCV(nfolds = n, shuffle = true|false): Utiliza validaciÃ³n cruzada con n iteraciones. Si se indica shuffle = true, se utiliza validaciÃ³n cruzada aleatoria.\nStratifiedCV(nfolds = n, shuffle = true|false): Utiliza validaciÃ³n cruzada estratificada con n iteraciones. Si se indica shuffle = true, se utiliza validaciÃ³n cruzada estratificada aleatoria.\nInSample(): Utiliza el conjunto de entrenamiento como conjunto de test.\n\nmeasures: Indica las mÃ©tricas a utilizar para evaluar el modelo. Las mÃ©tricas mÃ¡s habituales son:\n\ncross_entropy: PÃ©rdida de entropÃ­a cruzada.\nconfusion_matrix: Matriz de confusiÃ³n.\ntrue_positive_rate: Tasa de verdaderos positivos.\ntrue_negative_rate: Tasa de verdaderos negativos.\nppv: Valor predictivo positivo.\nnpv: Valor predictivo negativo.\naccuracy: PrecisiÃ³n.\n\nSe puede indicar mÃ¡s de una en un vector.\n\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nevaluate(arbol, X, y, resampling = Holdout(fraction_train = 0.7, rng = 123), measures = accuracy)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ measure    â”‚ operation    â”‚ measurement â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Accuracy() â”‚ predict_mode â”‚ 0.843       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\nEvaluar el modelo mediante validaciÃ³n cruzada estratificada usando las mÃ©tricas de la pÃ©rdida de entropÃ­a cruzada, la matriz de confusiÃ³n, la tasa de verdaderos positivos, la tasa de verdaderos negativos, el valor predictivo positivo, el valor predictivo negativo y la precisiÃ³n. Â¿Es un buen modelo?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nevaluate(arbol, X, y, resampling = StratifiedCV(rng = 123), measures = [cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy])\n\nEvaluating over 6 folds:  33%[========&gt;                ]  ETA: 0:00:02Evaluating over 6 folds: 100%[=========================] Time: 0:00:01\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\nâ”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚   â”‚ measure                  â”‚ operation    â”‚ measurement                    â‹¯\nâ”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ A â”‚ LogLoss(                 â”‚ predict      â”‚ 0.375                          â‹¯\nâ”‚   â”‚   tol = 2.22045e-16)     â”‚              â”‚                                â‹¯\nâ”‚ B â”‚ ConfusionMatrix(         â”‚ predict_mode â”‚ ConfusionMatrix{2}([3821534 78 â‹¯\nâ”‚   â”‚   levels = nothing,      â”‚              â”‚                                â‹¯\nâ”‚   â”‚   perm = nothing,        â”‚              â”‚                                â‹¯\nâ”‚   â”‚   rev = nothing,         â”‚              â”‚                                â‹¯\nâ”‚   â”‚   checks = true)         â”‚              â”‚                                â‹¯\nâ”‚ C â”‚ TruePositiveRate(        â”‚ predict_mode â”‚ 0.128                          â‹¯\nâ”‚   â”‚   levels = nothing,      â”‚              â”‚                                â‹¯\nâ”‚   â”‚   rev = nothing,         â”‚              â”‚                                â‹¯\nâ”‚   â”‚   checks = true)         â”‚              â”‚                                â‹¯\nâ”‚ D â”‚ TrueNegativeRate(        â”‚ predict_mode â”‚ 1.0                            â‹¯\nâ”‚   â”‚   levels = nothing,      â”‚              â”‚                                â‹¯\nâ”‚   â”‚   rev = nothing,         â”‚              â”‚                                â‹¯\nâ”‚   â”‚   checks = true)         â”‚              â”‚                                â‹¯\nâ”‚ E â”‚ PositivePredictiveValue( â”‚ predict_mode â”‚ 0.994                          â‹¯\nâ”‚   â”‚   levels = nothing,      â”‚              â”‚                                â‹¯\nâ”‚   â”‚   rev = nothing,         â”‚              â”‚                                â‹¯\nâ”‚   â”‚   checks = true)         â”‚              â”‚                                â‹¯\nâ”‚ F â”‚ NegativePredictiveValue( â”‚ predict_mode â”‚ 0.83                           â‹¯\nâ”‚   â”‚   levels = nothing,      â”‚              â”‚                                â‹¯\nâ”‚   â”‚   rev = nothing,         â”‚              â”‚                                â‹¯\nâ”‚ â‹® â”‚            â‹®             â”‚      â‹®       â”‚                        â‹®       â‹±\nâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                     1 column and 2 rows omitted\nâ”Œâ”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚   â”‚ per_fold                                                                 â‹¯\nâ”œâ”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nâ”‚ A â”‚ [0.391, 0.394, 0.35, 0.358, 0.365, 0.391]                                â‹¯\nâ”‚ B â”‚ ConfusionMatrix{2, true, CategoricalValue{String, UInt32}}[ConfusionMatr â‹¯\nâ”‚ C â”‚ [0.125, 0.167, 0.155, 0.112, 0.113, 0.0952]                              â‹¯\nâ”‚ D â”‚ [1.0, 0.999, 1.0, 1.0, 1.0, 1.0]                                         â‹¯\nâ”‚ E â”‚ [1.0, 0.966, 1.0, 1.0, 1.0, 1.0]                                         â‹¯\nâ”‚ F â”‚ [0.83, 0.837, 0.835, 0.827, 0.828, 0.825]                                â‹¯\nâ”‚ G â”‚ [0.834, 0.841, 0.84, 0.831, 0.832, 0.828]                                â‹¯\nâ””â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n                                                               2 columns omitted\n\n\n\n\nLa precisiÃ³n del modelo es de \\(0.834\\) que no estÃ¡ mal, pero si consdieramos la tasa de verdadero positivos, que es \\(0.13\\) y la tasa de verdaderos negativos, que es prÃ¡cticamente 1, el modelo tiene un buen rendimiento en la clasificaciÃ³n de los vinos malos, pero un mal rendimiento en la clasificaciÃ³n de los vinos buenos. Por lo tanto, no podemos decir que sea un buen modelo.\n\n\n\nConstruir Ã¡rboles de decisiÃ³n con profundidades mÃ¡ximas de 2 a 10 y evaluar el modelo con validaciÃ³n cruzada estratificada. Â¿CuÃ¡l es la profundidad mÃ¡xima que da mejor resultado?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n TunedModel del paquete MLJ para ajustar los parÃ¡metros del modelo.\nLos parÃ¡metros mÃ¡s importantes de esta funciÃ³n son:\n\nmodel: Indica el modelo a ajustar.\nresampling: Indica el mÃ©todo de muestreo para definir los conjuntos de entrenamiento y test.\ntuning: Indica el mÃ©todo de ajuste de los parÃ¡metros del modelo. Los mÃ©todos mÃ¡s habituales son:\n\nGrid(resolution = n): Ajusta los parÃ¡metros del modelo utilizando una cuadrÃ­cula de bÃºsqueda con n valores.\nRandomSearch(resolution = n): Ajusta los parÃ¡metros del modelo utilizando una bÃºsqueda aleatoria con n valores.\n\nrange: Indica el rango de valores a utilizar para ajustar los parÃ¡metros del modelo. Se puede indicar un rango de valores o un vector de valores.\nmeasure: Indica la mÃ©trica a utilizar para evaluar el modelo.\n\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Instanciamos el modelo de Ã¡rbol de decisiÃ³n.\narbol = Tree()\n# Definimos el rango de valores a utilizar para ajustar los parÃ¡metros del modelo.\nr = range(arbol, :max_depth, lower=2, upper=10)\n# Ajustamos los parÃ¡metros del modelo utilizando una cuadrÃ­cula de bÃºsqueda con 9 valores.\narbol_parametrizado = TunedModel(\n    model = arbol,\n    resampling = StratifiedCV(rng = 123),\n    tuning = Grid(resolution = 9),\n    range = r,\n    measure = accuracy)\n# Definimos una mÃ¡quina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol_parametrizado, X, y)\n# Ajustamos los parÃ¡metros del modelo.\nMLJ.fit!(mach)\n# Mostramos los parÃ¡metros del mejor modelo.\nfitted_params(mach).best_model\n\n[ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, â€¦), â€¦), â€¦).\n[ Info: Attempting to evaluate 9 models.\nEvaluating over 9 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 9 metamodels:  11%[==&gt;                      ]  ETA: 0:00:03Evaluating over 9 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:03Evaluating over 9 metamodels:  33%[========&gt;                ]  ETA: 0:00:02Evaluating over 9 metamodels:  44%[===========&gt;             ]  ETA: 0:00:01Evaluating over 9 metamodels:  56%[=============&gt;           ]  ETA: 0:00:01Evaluating over 9 metamodels:  67%[================&gt;        ]  ETA: 0:00:01Evaluating over 9 metamodels:  78%[===================&gt;     ]  ETA: 0:00:01Evaluating over 9 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 9 metamodels: 100%[=========================] Time: 0:00:02\n\n\nDecisionTreeClassifier(\n  max_depth = 5, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = TaskLocalRNG())\n\n\n\n\n\nDibujar la curva de aprendizaje del modelo en funciÃ³n de la profundidad del Ã¡rbol de decisiÃ³n.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n learning_curve del paquete MLJ para dibujar la curva de aprendizaje. Los parÃ¡metros mÃ¡s importantes de esta funciÃ³n son:\n\nmach: Indica la mÃ¡quina de aprendizaje a utilizar.\nrange: Indica el rango de valores a utilizar para ajustar los parÃ¡metros del modelo.\nresampling: Indica el mÃ©todo de muestreo para definir los conjuntos de entrenamiento y test.\nmeasure: Indica la mÃ©trica a utilizar para evaluar el modelo.\nrngs: Indica la semilla para la generaciÃ³n de nÃºmeros aleatorios. Se pueden indicar varias semillas en un vector y se genera una curva de aprendizaje para cada semilla.\n\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Instanciamos el modelo de Ã¡rbol de decisiÃ³n.\narbol = Tree()\n# Definimos una mÃ¡quina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol, X, y)\n# Definimos el rango de valores a utilizar para ajustar los parÃ¡metros del modelo.\nr = range(arbol, :max_depth, lower=2, upper=10)\n# Dibujamos la curva de aprendizaje.\ncurva = learning_curve(mach, range = r, resampling = StratifiedCV(rng = 123), measure = accuracy)\n# Dibujamos la curva de aprendizaje.\nfig = Figure()\nax = Axis(fig[1, 1], title = \"Curva de aprendizaje\", xlabel = \"Profundidad del Ã¡rbol\", ylabel = \"PrecisiÃ³n\")\nMakie.scatter!(ax, curva.parameter_values, curva.measurements)\nfig\n\n[ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, â€¦), â€¦), â€¦).\n[ Info: Attempting to evaluate 9 models.\nEvaluating over 9 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:01Evaluating over 9 metamodels:  33%[========&gt;                ]  ETA: 0:00:01Evaluating over 9 metamodels:  44%[===========&gt;             ]  ETA: 0:00:01Evaluating over 9 metamodels:  56%[=============&gt;           ]  ETA: 0:00:01Evaluating over 9 metamodels:  67%[================&gt;        ]  ETA: 0:00:01Evaluating over 9 metamodels:  78%[===================&gt;     ]  ETA: 0:00:00Evaluating over 9 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 9 metamodels: 100%[=========================] Time: 0:00:01\nâ”Œ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\nâ”” @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\n\nConstruir un Ã¡rbol de decisiÃ³n con la profundidad mÃ¡xima que da mejor resultado y visualizarlo.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Instanciamos el modelo de Ã¡rbol de decisiÃ³n.\narbol = Tree(max_depth = 4)\n# Definimos una mÃ¡quina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol, X, y)\n# Ajustamos los parÃ¡metros del modelo.\nMLJ.fit!(mach)\n# Visualizamos el Ã¡rbol de decisiÃ³n.\nfitted_params(mach).tree\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = 4, â€¦), â€¦).\n\n\nalcohol &lt; 10.62\nâ”œâ”€ meses_barrica &lt; 8.5\nâ”‚  â”œâ”€ acided_volatil &lt; 0.3125\nâ”‚  â”‚  â”œâ”€ acided_volatil &lt; 0.2025\nâ”‚  â”‚  â”‚  â”œâ”€  â˜¹ï¸  (408/496)\nâ”‚  â”‚  â”‚  â””â”€  â˜¹ï¸  (1095/1172)\nâ”‚  â”‚  â””â”€ meses_barrica &lt; 5.5\nâ”‚  â”‚     â”œâ”€  â˜¹ï¸  (1334/1345)\nâ”‚  â”‚     â””â”€  â˜¹ï¸  (51/58)\nâ”‚  â””â”€  ğŸ˜Š  (25/25)\nâ””â”€ meses_barrica &lt; 12.5\n   â”œâ”€ cloruro_sodico &lt; 0.0455\n   â”‚  â”œâ”€ alcohol &lt; 12.55\n   â”‚  â”‚  â”œâ”€  â˜¹ï¸  (751/1160)\n   â”‚  â”‚  â””â”€  ğŸ˜Š  (185/286)\n   â”‚  â””â”€ meses_barrica &lt; 10.5\n   â”‚     â”œâ”€  â˜¹ï¸  (552/629)\n   â”‚     â””â”€  ğŸ˜Š  (25/43)\n   â””â”€ alcohol &lt; 14.45\n      â”œâ”€  ğŸ˜Š  (105/105)\n      â””â”€  â˜¹ï¸  (1/1)\n\n\n\n\n\nÂ¿CuÃ¡l es la importancia de cada variable en el modelo?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n feature_importances del paquete DecisionTree para calcular la importancia de cada variable.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\n# Calculamos la importancia de cada variable.\nfeature_importances(mach)\n\n13-element Vector{Pair{Symbol, Float64}}:\n              :alcohol =&gt; 0.5303315899204789\n        :meses_barrica =&gt; 0.26854115615561525\n       :acided_volatil =&gt; 0.1040970236546446\n       :cloruro_sodico =&gt; 0.09703023026926123\n                 :tipo =&gt; 0.0\n          :acided_fija =&gt; 0.0\n        :acido_citrico =&gt; 0.0\n      :azucar_residual =&gt; 0.0\n :dioxido_azufre_libre =&gt; 0.0\n :dioxido_azufre_total =&gt; 0.0\n             :densidad =&gt; 0.0\n                   :ph =&gt; 0.0\n             :sulfatos =&gt; 0.0\n\n\n\n\n\nPredecir la calidad de los 10 primeros vinos del conjunto de ejemplos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la funciÃ³n predict del paquete DecisionTree para predecir las probabilidades de pertenecer a cada clase un ejemplo o conjunto de ejemplos.\nUsar la funciÃ³n predict_mode del paquete DecisionTree para predecir la clase de un ejemplo o conjunto de ejemplos.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\nPrimero calculamos las probabilidades de cada clase.\n\nMLJ.predict(mach, X[1:10, :])\n\n10-element CategoricalDistributions.UnivariateFiniteVector{OrderedFactor{2}, String, UInt32, Float64}:\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.992,  ğŸ˜Š =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.823,  ğŸ˜Š =&gt;0.177)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.992,  ğŸ˜Š =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.992,  ğŸ˜Š =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.647,  ğŸ˜Š =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.647,  ğŸ˜Š =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.647,  ğŸ˜Š =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.878,  ğŸ˜Š =&gt;0.122)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.992,  ğŸ˜Š =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( â˜¹ï¸ =&gt;0.992,  ğŸ˜Š =&gt;0.00818)\n\n\nY ahora predecimos la clase.\n\npredict_mode(mach, X[1:10, :])\n\n10-element CategoricalArray{String,1,UInt32}:\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"\n \" â˜¹ï¸ \"",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Ãrboles de decisiÃ³n</span>"
    ]
  },
  {
    "objectID": "03-regresion.html",
    "href": "03-regresion.html",
    "title": "3Â  RegresiÃ³n",
    "section": "",
    "text": "3.1 Ejercicios Resueltos\nLos modelos de aprendizaje basados en regresiÃ³n son modelos bastante simples que pueden utilizarse para predecir variables cuantitativas (regresiÃ³n lineal) o cualitativas (regresiÃ³n logÃ­stica). Esta prÃ¡ctica contiene ejercicios que muestran como construir modelos de aprendizaje de regresiÃ³n lineal y regresiÃ³n logÃ­stica con Julia.\nPara la realizaciÃ³n de esta prÃ¡ctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>RegresiÃ³n</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#ejercicios-resueltos",
    "href": "03-regresion.html#ejercicios-resueltos",
    "title": "3Â  RegresiÃ³n",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de grÃ¡ficas.\nusing GLMakie  # Para obtener grÃ¡ficos interactivos.\n\nEjercicio 3.1 El conjunto de datos viviendas.csv contiene informaciÃ³n sobre el precio de venta de viviendas en una ciudad.\n\nCargar los datos del archivo viviendas.csv en un data frame.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/viviendas.csv\"), DataFrame)\nfirst(df, 5)\n\n5Ã—13 DataFrame\n\n\n\nRow\nprecio\narea\ndormitorios\nbaÃ±os\nhabitaciones\ncalleprincipal\nhuespedes\nsotano\ncalentador\nclimatizacion\ngaraje\ncentrico\namueblado\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nString3\nString3\nString3\nString3\nString3\nInt64\nString3\nString15\n\n\n\n\n1\n13300000\n7420\n4\n2\n3\nsi\nno\nno\nno\nsi\n2\nsi\namueblado\n\n\n2\n12250000\n8960\n4\n4\n4\nsi\nno\nno\nno\nsi\n3\nno\namueblado\n\n\n3\n12250000\n9960\n3\n2\n2\nsi\nno\nsi\nno\nno\n2\nsi\nsemi-amueblado\n\n\n4\n12215000\n7500\n4\n2\n2\nsi\nno\nsi\nno\nsi\n3\nsi\namueblado\n\n\n5\n11410000\n7420\n4\n1\n2\nsi\nsi\nsi\nno\nsi\n2\nno\namueblado\n\n\n\n\n\n\n\n\n\nDibujar un diagrama de dispersiÃ³n entre el precio y el area de las viviendas.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing Plots\nplt = scatter(df.area, df.precio, xlabel=\"Area\", ylabel=\"Precio\", title=\"Precio vs Area\", label = \"Ejemplos\", fmt=:png,)\n\n\n\n\n\n\n\nDefinir un modelo lineal que explique el precio en funciÃ³n del Ã¡rea de las viviendas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUn modelo lineal tiene ecuaciÃ³n \\(y = \\theta_1 + \\theta_2 x\\).\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nprecio(area, Î¸) = Î¸[1] .+ Î¸[2] * area\n\nprecio (generic function with 1 method)\n\n\nObserva que la funciÃ³n precio estÃ¡ vectorizada, lo que significa que puede recibir un vector de Ã¡reas y devolver un vector de precios.\n\n\n\nInicializar los parÃ¡metros del modelo lineal con valores nulos y dibujar el modelo sobre el diagrama de dispersiÃ³n.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nÎ¸ = [0.0, 0.0]\nplot!(df.area, precio(df.area, Î¸), label = \"Modelo 0\")\n\n\n\n\n\n\n\nDefinir una funciÃ³n de costo para el modelo lineal y evaluar el coste para el modelo lineal construido con los parÃ¡metros iniciales. A la vista del coste obtenido, Â¿cÃ³mo de bueno es el modelo?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLa funciÃ³n de coste para un modelo lineal es el error cuadrÃ¡tico medio.\n\\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]\ndonde \\(h_\\theta\\) es el modelo, \\(h_\\theta(x^{(i)})\\) es la predicciÃ³n del modelo para el ejemplo \\(i\\)-Ã©simo, \\(y^{(i)}\\) es el valor real observado para el ejemplo \\(i\\)-Ã©simo, y \\(m\\) es el nÃºmero de ejemplos.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nfunction coste(Î¸, X, Y)\n    m = length(Y)\n    return sum((precio(X, Î¸) .- Y).^2) / (2 * m)\nend\n\ncoste(Î¸, df.area, df.precio)\n\n1.3106916364659266e13\n\n\nLa funciÃ³n de coste nos da una medida de lo lejos que estÃ¡n las predicciones del modelo de los valores reales observados. En este caso, el coste es muy alto, lo que indica que el modelo no es bueno.\n\n\n\nÂ¿En quÃ© direcciÃ³n debemos modificar los parÃ¡metros del modelo para mejorar el modelo?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\nPara minimizar la funciÃ³n de coste, debemos modificar los parÃ¡metros del modelo en la direcciÃ³n opuesta al gradiente de la funciÃ³n de coste, ya que el gradiente de una funciÃ³n indica la direcciÃ³n de mayor crecimiento de la funciÃ³n.\n\n\n\nCrear una funciÃ³n para modificar los pesos del modelo lineal mediante el algoritmo del gradiente descendente, y aplicarla a los parÃ¡metros actuales tomando una tasa de aprendizaje de \\(10^{-8}\\). Â¿CÃ³mo han cambiado los parÃ¡metros del modelo? Dibujar el modelo actualizado sobre el diagrama de dispersiÃ³n. Â¿CÃ³mo ha cambiado el coste?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEl algoritmo del gradiente descendente actualiza los parÃ¡metros del modelo de acuerdo a la siguiente regla:\n\\[\n\\theta_j = \\theta_j - \\eta \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n\\]\ndonde \\(\\eta\\) es la tasa de aprendizaje y \\(\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\) es la derivada parcial de la funciÃ³n de coste con respecto al parÃ¡metro \\(\\theta_j\\).\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nfunction gradiente_descendente!(Î¸, X, Y, Î·)\n    # Calculamos el nÃºmero de ejemplos\n    m = length(Y)\n    # Actualizamos el tÃ©rmino independiente del modelo lineal.\n    Î¸[1] -= Î· * sum(precio(X, Î¸) - Y) / m\n    # Actualizamos la pendiente del modelo lineal.\n    Î¸[2] -= Î· * sum((precio(X, Î¸) - Y) .* X) / m\n    return Î¸\nend\n\ngradiente_descendente! (generic function with 1 method)\n\n\nAplicamos la funciÃ³n a los parÃ¡metros del modelo actual y mostramos los nuevos parÃ¡metros.\n\ngradiente_descendente!(Î¸, df.area, df.precio, 1e-8)\nÎ¸\n\n2-element Vector{Float64}:\n   0.04766729247706422\n 267.22919804579385\n\n\nDibujamos el nuevo modelo.\n\nplot!(df.area, precio(df.area, Î¸), label = \"Modelo 1\")\n\n\n\n\nSe observa que ahora la recta estÃ¡ mÃ¡s cerca de la nube de puntos, por lo que el modelo ha mejorado. Calculamos el coste del nuevo modelo.\n\ncoste(Î¸, df.area, df.precio)\n\n7.080823787113201e12\n\n\n\n\n\nRepetir el proceso de actualizaciÃ³n de los parÃ¡metros del modelo mediante el algoritmo del gradiente descendente durante 9 iteraciones mÃ¡s y dibujar los modelos actualizados.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\nfor i = 2:10\n    gradiente_descendente!(Î¸, df.area, df.precio, 1e-8)\n    plot!(df.area, precio(df.area, Î¸), label = \"Modelo $i\", legend = true)\nend\nplt\n\n\n\n\nDibujar un grÃ¡fico con la evoluciÃ³n del coste del modelo a lo largo de las iteraciones. Â¿CÃ³mo se comporta el coste a lo largo de las iteraciones?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ncostes = Float64[]\nfor i = 1:10\n    gradiente_descendente!(Î¸, df.area, df.precio, 1e-8)\n    push!(costes, coste(Î¸, df.area, df.precio))\nend\ncostes\n\n10-element Vector{Float64}:\n 4.230808760870044e12\n 2.882906194020343e12\n 2.2454213686913755e12\n 1.9439256128790886e12\n 1.8013344680594421e12\n 1.7338965877160208e12\n 1.7020021263374993e12\n 1.6869177748236997e12\n 1.6797836937723748e12\n 1.6764096595632322e12\n\n\nEl coste del modelo disminuye en cada iteraciÃ³n, lo que indica que el modelo estÃ¡ mejorando. Esto se debe a que el algoritmo del gradiente descendente modifica los parÃ¡metros del modelo en la direcciÃ³n que minimiza la funciÃ³n de coste.\n\n\n\nÂ¿Hasta quÃ© iteraciÃ³n habrÃ¡ que llegar para conseguir un reducciÃ³n del coste menor de un \\(0.0001\\%\\)?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nÎ¸ = [0.0, 0.0]\ncostes = [0, coste(Î¸, df.area, df.precio)]\ni = 1\nwhile abs(costes[end] - costes[end-1]) / costes[end-1] &gt; 0.000001\n    i += 1\n    gradiente_descendente!(Î¸, df.area, df.precio, 1e-8)\n    push!(costes, coste(Î¸, df.area, df.precio))\nend\ni\n\n23\n\n\nEn este caso, el algoritmo del gradiente descendente converge en 1000 iteraciones.\n\n\n\nÂ¿QuÃ© sucede si se utiliza una tasa de aprendizaje \\(\\eta = 0.0001\\)? Â¿CÃ³mo afecta al coste y a la convergencia del modelo?\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nÎ¸ = [0.0, 0.0]\ncostes = [coste(Î¸, df.area, df.precio)]\nfor i = 1:10\n    gradiente_descendente!(Î¸, df.area, df.precio, 0.0001)\n    push!(costes, coste(Î¸, df.area, df.precio))\nend\ncostes\n\n11-element Vector{Float64}:\n 1.3106916364659266e13\n 1.114133369099188e20\n 1.0856750832581238e27\n 1.05794371802143e34\n 1.0309206941949286e41\n 1.004587918634273e48\n 9.789277603492545e54\n 9.539230386975057e61\n 9.29557011881276e68\n 9.058133657380397e75\n 8.826762028174244e82\n\n\nSi la tasa de aprendizaje es demasiado grande, el algoritmo del gradiente descendente puede no converger y el coste puede oscilar en lugar de disminuir. En este caso, el coste aumenta en cada iteraciÃ³n, lo que indica que la tasa de aprendizaje es demasiado grande.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>RegresiÃ³n</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html",
    "href": "02-preprocesamiento.html",
    "title": "2Â  Preprocesamiento de datos",
    "section": "",
    "text": "2.1 Ejercicios Resueltos\nEsta prÃ¡ctica contiene ejercicios que muestran como preprocesar un conjunto de datos con Julia. El preprocesamiento de datos es una tarea fundamental en la construcciÃ³n de modelos de aprendizaje automÃ¡tico que consiste en la limpieza, transformaciÃ³n y preparaciÃ³n de los datos para que puedan alimentar el proceso de entrenamiento de los modelos, asÃ­ como para la evaluaciÃ³n de su rendimiento. El preprocesamiento de datos incluye tareas como\nPara la realizaciÃ³n de esta prÃ¡ctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-resueltos",
    "href": "02-preprocesamiento.html#ejercicios-resueltos",
    "title": "2Â  Preprocesamiento de datos",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de grÃ¡ficas.\nusing Makie  # Para obtener grÃ¡ficos interactivos.\nusing StatsBase  # Para la estandarizaciÃ³n de variables.\nusing Statistics  # Para el cÃ¡lculo de estadÃ­sticas.\n\nEjercicio 2.1 La siguiente tabla contiene los ingresos y gastos de una empresa durante el primer trimestre del aÃ±o.\n\nCrear un data frame con los datos de la tabla.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n DataFrame del paquete DataFrames para partir el rango de valores en intervalos y asociar a cada intervalo una categorÃ­a.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing DataFrames\ndf = DataFrame(\n    Mes = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"],\n    Ingresos = [45000, 41500, 51200, 49700],\n    Gastos = [33400, 35400, 35600, 36300],\n    Impuestos = [6450, 6300, 7100, 6850]\n    )\n\n4Ã—4 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\n\n\n\nString\nInt64\nInt64\nInt64\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n\n\n2\nFebrero\n41500\n35400\n6300\n\n\n3\nMarzo\n51200\n35600\n7100\n\n\n4\nAbril\n49700\n36300\n6850\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con los beneficios de cada mes (ingresos - gastos - impuestos).\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndf.Beneficios = df.Ingresos - df.Gastos - df.Impuestos\ndf\n\n4Ã—5 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\nBeneficios\n\n\n\nString\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n5150\n\n\n2\nFebrero\n41500\n35400\n6300\n-200\n\n\n3\nMarzo\n51200\n35600\n7100\n8500\n\n\n4\nAbril\n49700\n36300\n6850\n6550\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con la variable Balance con dos posibles categorÃ­as: positivo si ha habido beneficios y negativo si ha habido pÃ©rdidas.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndf.Balance = ifelse.(df.Beneficios .&gt; 0, \"positivo\", \"negativo\")\ndf\n\n4Ã—6 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\nBeneficios\nBalance\n\n\n\nString\nInt64\nInt64\nInt64\nInt64\nString\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n5150\npositivo\n\n\n2\nFebrero\n41500\n35400\n6300\n-200\nnegativo\n\n\n3\nMarzo\n51200\n35600\n7100\n8500\npositivo\n\n\n4\nAbril\n49700\n36300\n6850\n6550\npositivo\n\n\n\n\n\n\n\n\n\nFiltrar el conjunto de datos para quedarse con los nombres de los meses y los beneficios de los meses con balance positivo.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndf[df.Balance .== \"positivo\", [:Mes, :Beneficios]]\n\n3Ã—2 DataFrame\n\n\n\nRow\nMes\nBeneficios\n\n\n\nString\nInt64\n\n\n\n\n1\nEnero\n5150\n\n\n2\nMarzo\n8500\n\n\n3\nAbril\n6550\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.2 El fichero colesterol.csv contiene informaciÃ³n de una muestra de pacientes donde se han medido la edad, el sexo, el peso, la altura y el nivel de colesterol, ademÃ¡s de su nombre.\n\nCrear un data frame con los datos de todos los pacientes del estudio a partir del fichero colesterol.csv.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n CSV.read del paquete CSV para crear und data frame a partir de un fichero CSV. Si el fichero estÃ¡ en una url, utilizar la funciÃ³n download(url) para descargar el fichero y despuÃ©s leerlo con la funciÃ³n [CSV.read].\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CSV\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/colesterol.csv\"), DataFrame)\n\n14Ã—6 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n\n\n2\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n\n\n3\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\n\n\n4\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n\n\n5\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n\n\n7\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n\n\n8\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\nmissing\n\n\n9\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n\n\n11\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n\n\n12\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n\n\n13\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con el Ã­ndice de masa corporal, usando la siguiente fÃ³rmula\n\\[\n\\mbox{IMC} = \\frac{\\mbox{Peso (kg)}}{\\mbox{Altura (cm)}^2}\n\\]\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndf.imc = df.peso ./ (df.altura .^ 2)\ndf\n\n14Ã—7 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\n\n\n2\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\n\n\n3\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\nmissing\n\n\n4\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\n\n\n5\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\n\n\n7\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\n\n\n8\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\nmissing\n21.7738\n\n\n9\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\n\n\n11\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\n\n\n12\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\n\n\n13\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con la variable obesidad recodificando la columna imc en las siguientes categorÃ­as.\n\n\n\nRango IMC\nCategorÃ­a\n\n\n\n\nMenor de 18.5\nBajo peso\n\n\nDe 18.5 a 24.5\nSaludable\n\n\nDe 24.5 a 30\nSobrepeso\n\n\nMayor de 30\nObeso\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n cut del paquete CategoricalArrays para partir el rango de valores en intervalos y asociar a cada intervalo una categorÃ­a.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CategoricalArrays\ndf.obesidad = cut(df.imc, [0, 18.5, 24.5, 30, Inf],\n                labels=[\"Bajo peso\", \"Saludable\", \"Sobrepeso\", \"Obeso\"],\n                extend=true)\ndf\n\n14Ã—8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCatâ€¦?\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nSeleccionar las columnas nombre, sexo y edad.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndf[:, [:nombre, :sexo, :edad]]\n\n14Ã—3 DataFrame\n\n\n\nRow\nnombre\nsexo\nedad\n\n\n\nString\nString1\nInt64\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\nH\n18\n\n\n2\nRosa DÃ­az DÃ­az\nM\n32\n\n\n3\nJavier GarcÃ­a SÃ¡nchez\nH\n24\n\n\n4\nCarmen LÃ³pez PinzÃ³n\nM\n35\n\n\n5\nMarisa LÃ³pez Collado\nM\n46\n\n\n6\nAntonio Ruiz Cruz\nH\n68\n\n\n7\nAntonio FernÃ¡ndez OcaÃ±a\nH\n51\n\n\n8\nPilar MartÃ­n GonzÃ¡lez\nM\n22\n\n\n9\nPedro GÃ¡lvez Tenorio\nH\n35\n\n\n10\nSantiago Reillo Manzano\nH\n46\n\n\n11\nMacarena Ãlvarez Luna\nM\n53\n\n\n12\nJosÃ© MarÃ­a de la GuÃ­a Sanz\nH\n58\n\n\n13\nMiguel Angel Cuadrado GutiÃ©rrez\nH\n27\n\n\n14\nCarolina Rubio Moreno\nM\n20\n\n\n\n\n\n\n\n\n\nAnonimizar los datos eliminando la columna nombre.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n select del paquete DataFrames para seleccionar las columnas deseadas y eliminar las columnas no deseadas. Existe tambiÃ©n la funciÃ³n select! que modifica el data frame original eliminando las columnas no seleccionadas.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nselect(df, Not(:nombre))\n\n14Ã—7 DataFrame\n\n\n\nRow\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCatâ€¦?\n\n\n\n\n1\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nReordenar las columnas poniendo la columna sexo antes que la columna edad.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nselect(df, Cols(:sexo, :edad, Not(:sexo, :edad)))\n\n14Ã—8 DataFrame\n\n\n\nRow\nsexo\nedad\nnombre\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString1\nInt64\nString\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCatâ€¦?\n\n\n\n\n1\nH\n18\nJosÃ© Luis MartÃ­nez Izquierdo\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nM\n32\nRosa DÃ­az DÃ­az\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nH\n24\nJavier GarcÃ­a SÃ¡nchez\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nM\n35\nCarmen LÃ³pez PinzÃ³n\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nM\n46\nMarisa LÃ³pez Collado\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nH\n68\nAntonio Ruiz Cruz\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nH\n51\nAntonio FernÃ¡ndez OcaÃ±a\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nM\n22\nPilar MartÃ­n GonzÃ¡lez\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\nH\n35\nPedro GÃ¡lvez Tenorio\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nH\n46\nSantiago Reillo Manzano\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nM\n53\nMacarena Ãlvarez Luna\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nH\n58\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nH\n27\nMiguel Angel Cuadrado GutiÃ©rrez\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nM\n20\nCarolina Rubio Moreno\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las mujeres.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndf[df.sexo .== \"M\", :]\n\n6Ã—8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCatâ€¦?\n\n\n\n\n1\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n2\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n3\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n4\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n5\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n6\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con los hombres mayores de 30 aÃ±os.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndf[(df.sexo .== \"H\") .& (df.edad .&gt; 30), :]\n\n5Ã—8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCatâ€¦?\n\n\n\n\n1\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n2\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n3\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n4\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n5\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las filas sin valores perdidos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n dropmissing del paquete DataFrames para eliminar las filas con valores perdidos.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndropmissing(df)\n\n12Ã—8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64\nFloat64\nFloat64\nFloat64\nCatâ€¦\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n4\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n5\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n6\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n7\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n8\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n9\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n10\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n11\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n12\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para eliminar las filas con datos perdidos en la columna colesterol.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n dropmissing, col donde col es el nombre de la columna que contiene los valores perdidos.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndropmissing(df, :colesterol)    \n\n13Ã—8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCatâ€¦?\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n9\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n10\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n11\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n12\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n13\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nImputar los valores perdidos en la columna colesterol con la media de los valores no perdidos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n coalesce para reemplazar los valores perdidos por otros valores.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing Statistics\nmedia_colesterol = mean(skipmissing(df.colesterol))\ndf.colesterol = coalesce.(df.colesterol, media_colesterol)\ndf\n\n14Ã—8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCatâ€¦?\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n\n\n9\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con las puntuaciones tÃ­picas del colesterol, es decir, estandarizando la columna colesterol.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n Standardizer del paquete StatsBase para estandarizar una variable.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing StatsBase\ndf.colesterol_estandarizado = standardize(ZScoreTransform, df.colesterol)\ndf\n\n14Ã—9 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\ncolesterol_estandarizado\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCatâ€¦?\nFloat64\n\n\n\n\n1\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n-0.998592\n\n\n2\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n0.307414\n\n\n3\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n-0.763511\n\n\n4\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n-0.52843\n\n\n5\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n-1.88668\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n0.751456\n\n\n7\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n1.4567\n\n\n8\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n-7.42378e-16\n\n\n9\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n0.542495\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n1.56118\n\n\n11\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n1.09102\n\n\n12\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n-0.58067\n\n\n13\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n-0.267229\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n-0.685151\n\n\n\n\n\n\n\n\n\nOrdenar el data frame segÃºn la columna nombre.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n sort para ordenar las filas del data frame segÃºn los valores de una o varias columnas. Utilizar el parÃ¡metro rev para especificar mediante un vector de booleanos si el orden es ascendente o descendente.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nsort(df, :nombre)\n\n14Ã—9 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\ncolesterol_estandarizado\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCatâ€¦?\nFloat64\n\n\n\n\n1\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n1.4567\n\n\n2\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n0.751456\n\n\n3\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n-0.52843\n\n\n4\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n-0.685151\n\n\n5\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n-0.763511\n\n\n6\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n-0.998592\n\n\n7\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n-0.58067\n\n\n8\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n1.09102\n\n\n9\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n-1.88668\n\n\n10\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n-0.267229\n\n\n11\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n0.542495\n\n\n12\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n-7.42378e-16\n\n\n13\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n0.307414\n\n\n14\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n1.56118\n\n\n\n\n\n\n\n\n\nOrdenar el data frame ascendentemente por la columna sexo y descendentemente por la columna edad.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nsort(df, [:sexo, :edad], rev=[false, true])\n\n14Ã—9 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\ncolesterol_estandarizado\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCatâ€¦?\nFloat64\n\n\n\n\n1\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n0.751456\n\n\n2\nJosÃ© MarÃ­a de la GuÃ­a Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n-0.58067\n\n\n3\nAntonio FernÃ¡ndez OcaÃ±a\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n1.4567\n\n\n4\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n1.56118\n\n\n5\nPedro GÃ¡lvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n0.542495\n\n\n6\nMiguel Angel Cuadrado GutiÃ©rrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n-0.267229\n\n\n7\nJavier GarcÃ­a SÃ¡nchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n-0.763511\n\n\n8\nJosÃ© Luis MartÃ­nez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n-0.998592\n\n\n9\nMacarena Ãlvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n1.09102\n\n\n10\nMarisa LÃ³pez Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n-1.88668\n\n\n11\nCarmen LÃ³pez PinzÃ³n\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n-0.52843\n\n\n12\nRosa DÃ­az DÃ­az\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n0.307414\n\n\n13\nPilar MartÃ­n GonzÃ¡lez\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n-7.42378e-16\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n-0.685151\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.3 El fichero notas-curso2.csv contiene informaciÃ³n de las notas de los alumnos de un curso.\n\nCrear un data frame con los datos de los alumnos del curso a partir del fichero notas-curso2.csv.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/notas-curso2.csv\"), DataFrame; missingstring=\"NA\")\n\n120Ã—9 DataFrame95 rows omitted\n\n\n\nRow\nsexo\nturno\ngrupo\ntrabaja\nnotaA\nnotaB\nnotaC\nnotaD\nnotaE\n\n\n\nString7\nString7\nString1\nString1\nFloat64\nFloat64?\nFloat64?\nFloat64?\nFloat64?\n\n\n\n\n1\nMujer\nTarde\nC\nN\n5.2\n6.3\n3.4\n2.3\n2.0\n\n\n2\nHombre\nMaÃ±ana\nA\nN\n5.7\n5.7\n4.2\n3.5\n2.7\n\n\n3\nHombre\nMaÃ±ana\nB\nN\n8.3\n8.8\n8.8\n8.0\n5.5\n\n\n4\nHombre\nMaÃ±ana\nB\nN\n6.1\n6.8\n4.0\n3.5\n2.2\n\n\n5\nHombre\nMaÃ±ana\nA\nN\n6.2\n9.0\n5.0\n4.4\n3.7\n\n\n6\nHombre\nMaÃ±ana\nA\nS\n8.6\n8.9\n9.5\n8.4\n3.9\n\n\n7\nMujer\nMaÃ±ana\nA\nN\n6.7\n7.9\n5.6\n4.8\n4.2\n\n\n8\nMujer\nTarde\nC\nS\n4.1\n5.2\n1.7\n0.3\n1.0\n\n\n9\nHombre\nTarde\nC\nN\n5.0\n5.0\n3.3\n2.7\n6.0\n\n\n10\nHombre\nTarde\nC\nN\n5.3\n6.3\n4.8\n3.6\n2.3\n\n\n11\nMujer\nMaÃ±ana\nA\nN\n7.8\nmissing\n6.5\n6.7\n2.8\n\n\n12\nHombre\nMaÃ±ana\nA\nN\n6.5\n8.0\n5.0\n3.2\n3.3\n\n\n13\nHombre\nMaÃ±ana\nB\nN\n6.6\n7.6\n5.3\n4.0\n1.0\n\n\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\n\n\n109\nMujer\nTarde\nC\nN\n5.4\n7.3\n3.5\n2.5\n4.6\n\n\n110\nHombre\nMaÃ±ana\nB\nN\n7.4\n7.4\n6.2\n5.8\n1.1\n\n\n111\nMujer\nTarde\nC\nS\n5.1\n8.1\n5.2\n5.1\n4.5\n\n\n112\nHombre\nMaÃ±ana\nA\nN\n6.9\n7.8\n3.9\n2.8\nmissing\n\n\n113\nHombre\nTarde\nC\nN\n3.6\n4.8\n2.1\n0.5\n5.6\n\n\n114\nHombre\nTarde\nC\nS\n5.9\n6.2\n5.0\n3.9\n1.9\n\n\n115\nHombre\nMaÃ±ana\nB\nN\n6.8\n7.2\n4.9\n3.8\n2.8\n\n\n116\nHombre\nMaÃ±ana\nA\nN\n6.5\n6.1\n5.8\n4.9\n1.2\n\n\n117\nMujer\nMaÃ±ana\nB\nN\n6.2\n7.0\n5.6\n5.4\n1.7\n\n\n118\nMujer\nTarde\nC\nN\n5.0\n6.5\n4.0\n2.8\n3.6\n\n\n119\nHombre\nTarde\nC\nN\n4.7\n6.0\n1.3\n0.4\n2.2\n\n\n120\nHombre\nTarde\nC\nS\n4.5\n4.7\n6.0\n4.9\n1.8\n\n\n\n\n\n\n\n\n\nObtener el nÃºmero de datos perdidos en cada columna.\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\ndescribe(df)[:, [:variable, :nmissing]]\n\n9Ã—2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\nsexo\n0\n\n\n2\nturno\n0\n\n\n3\ngrupo\n0\n\n\n4\ntrabaja\n0\n\n\n5\nnotaA\n0\n\n\n6\nnotaB\n5\n\n\n7\nnotaC\n1\n\n\n8\nnotaD\n2\n\n\n9\nnotaE\n2\n\n\n\n\n\n\n\n\n\nRecodificar la variable grupo en una colecciÃ³n de columnas binarias.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la funciÃ³n onehotbatch del paquete OneHotArrays para recodificar una variable categÃ³rica en una colecciÃ³n de columnas binarias.\n\n\n\n\n\n\n\n\n\nSoluciÃ³n\n\n\n\n\n\n\nusing OneHotArrays\ncodificacion = permutedims(onehotbatch(df.grupo, unique(df.grupo)))\nhcat(df, DataFrame(codificacion, :auto))\n\n120Ã—12 DataFrame95 rows omitted\n\n\n\nRow\nsexo\nturno\ngrupo\ntrabaja\nnotaA\nnotaB\nnotaC\nnotaD\nnotaE\nx1\nx2\nx3\n\n\n\nString7\nString7\nString1\nString1\nFloat64\nFloat64?\nFloat64?\nFloat64?\nFloat64?\nBool\nBool\nBool\n\n\n\n\n1\nMujer\nTarde\nC\nN\n5.2\n6.3\n3.4\n2.3\n2.0\ntrue\nfalse\nfalse\n\n\n2\nHombre\nMaÃ±ana\nA\nN\n5.7\n5.7\n4.2\n3.5\n2.7\nfalse\ntrue\nfalse\n\n\n3\nHombre\nMaÃ±ana\nB\nN\n8.3\n8.8\n8.8\n8.0\n5.5\nfalse\nfalse\ntrue\n\n\n4\nHombre\nMaÃ±ana\nB\nN\n6.1\n6.8\n4.0\n3.5\n2.2\nfalse\nfalse\ntrue\n\n\n5\nHombre\nMaÃ±ana\nA\nN\n6.2\n9.0\n5.0\n4.4\n3.7\nfalse\ntrue\nfalse\n\n\n6\nHombre\nMaÃ±ana\nA\nS\n8.6\n8.9\n9.5\n8.4\n3.9\nfalse\ntrue\nfalse\n\n\n7\nMujer\nMaÃ±ana\nA\nN\n6.7\n7.9\n5.6\n4.8\n4.2\nfalse\ntrue\nfalse\n\n\n8\nMujer\nTarde\nC\nS\n4.1\n5.2\n1.7\n0.3\n1.0\ntrue\nfalse\nfalse\n\n\n9\nHombre\nTarde\nC\nN\n5.0\n5.0\n3.3\n2.7\n6.0\ntrue\nfalse\nfalse\n\n\n10\nHombre\nTarde\nC\nN\n5.3\n6.3\n4.8\n3.6\n2.3\ntrue\nfalse\nfalse\n\n\n11\nMujer\nMaÃ±ana\nA\nN\n7.8\nmissing\n6.5\n6.7\n2.8\nfalse\ntrue\nfalse\n\n\n12\nHombre\nMaÃ±ana\nA\nN\n6.5\n8.0\n5.0\n3.2\n3.3\nfalse\ntrue\nfalse\n\n\n13\nHombre\nMaÃ±ana\nB\nN\n6.6\n7.6\n5.3\n4.0\n1.0\nfalse\nfalse\ntrue\n\n\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\nâ‹®\n\n\n109\nMujer\nTarde\nC\nN\n5.4\n7.3\n3.5\n2.5\n4.6\ntrue\nfalse\nfalse\n\n\n110\nHombre\nMaÃ±ana\nB\nN\n7.4\n7.4\n6.2\n5.8\n1.1\nfalse\nfalse\ntrue\n\n\n111\nMujer\nTarde\nC\nS\n5.1\n8.1\n5.2\n5.1\n4.5\ntrue\nfalse\nfalse\n\n\n112\nHombre\nMaÃ±ana\nA\nN\n6.9\n7.8\n3.9\n2.8\nmissing\nfalse\ntrue\nfalse\n\n\n113\nHombre\nTarde\nC\nN\n3.6\n4.8\n2.1\n0.5\n5.6\ntrue\nfalse\nfalse\n\n\n114\nHombre\nTarde\nC\nS\n5.9\n6.2\n5.0\n3.9\n1.9\ntrue\nfalse\nfalse\n\n\n115\nHombre\nMaÃ±ana\nB\nN\n6.8\n7.2\n4.9\n3.8\n2.8\nfalse\nfalse\ntrue\n\n\n116\nHombre\nMaÃ±ana\nA\nN\n6.5\n6.1\n5.8\n4.9\n1.2\nfalse\ntrue\nfalse\n\n\n117\nMujer\nMaÃ±ana\nB\nN\n6.2\n7.0\n5.6\n5.4\n1.7\nfalse\nfalse\ntrue\n\n\n118\nMujer\nTarde\nC\nN\n5.0\n6.5\n4.0\n2.8\n3.6\ntrue\nfalse\nfalse\n\n\n119\nHombre\nTarde\nC\nN\n4.7\n6.0\n1.3\n0.4\n2.2\ntrue\nfalse\nfalse\n\n\n120\nHombre\nTarde\nC\nS\n4.5\n4.7\n6.0\n4.9\n1.8\ntrue\nfalse\nfalse",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-propuestos",
    "href": "02-preprocesamiento.html#ejercicios-propuestos",
    "title": "2Â  Preprocesamiento de datos",
    "section": "2.2 Ejercicios propuestos",
    "text": "2.2 Ejercicios propuestos\n\nEjercicio 2.4 El fichero vinos.csv contiene informaciÃ³n sobre las caracterÃ­sticas de una muestra de vinos portugueses de la denominaciÃ³n â€œVinho Verdeâ€. Las variables que contiene son:\n\n\n\n\n\n\n\n\nVariable\nDescripciÃ³n\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nCategÃ³rica (blanco, tinto)\n\n\nmeses.barrica\nMesesde envejecimiento en barrica\nNumÃ©rica(meses)\n\n\nacided.fija\nCantidadde Ã¡cidotartÃ¡rico\nNumÃ©rica(g/dm3)\n\n\nacided.volatil\nCantidad de Ã¡cido acÃ©tico\nNumÃ©rica(g/dm3)\n\n\nacido.citrico\nCantidad de Ã¡cidocÃ­trico\nNumÃ©rica(g/dm3)\n\n\nazucar.residual\nCantidad de azÃºcarremanente despuÃ©s de la fermentaciÃ³n\nNumÃ©rica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosÃ³dico\nNumÃ©rica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de diÃ³xido de azufreen formalibre\nNumÃ©rica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidadde diÃ³xido de azufretotal en forma libre o ligada\nNumÃ©rica(mg/dm3)\n\n\ndensidad\nDensidad\nNumÃ©rica(g/cm3)\n\n\nph\npH\nNumÃ©rica(0-14)\n\n\nsulfatos\nCantidadde sulfato de potasio\nNumÃ©rica(g/dm3)\n\n\nalcohol\nPorcentajede contenidode alcohol\nNumÃ©rica(0-100)\n\n\ncalidad\nCalificaciÃ³n otorgada porun panel de expertos\nNumÃ©rica(0-10)\n\n\n\n\nCrear un data frame con los datos de los vinos a partir del fichero vinos.csv.\nObtener el nÃºmero de valores perdidos en cada columna.\nImputar los valores perdidos del alcohol con la media de los valores no perdidos para cada tipo de vino.\nCrear la variable categÃ³rica Envejecimiento recodificando la variable meses.barrica en las siguientes categorÃ­as.\n\n\n\nRango en meses\nCategorÃ­a\n\n\n\n\nMenos de 3\nJoven\n\n\nEntre 3 y 12\nCrianza\n\n\nEntre 12 y 18\nReserva\n\n\nMÃ¡s de 18\nGran reserva\n\n\n\nCrear la variable categÃ³rica Dulzor recodificando la variable azucar.residual en las siguientes categorÃ­as.\n\n\n\nRango azÃºcar\nCategorÃ­a\n\n\n\n\nMenos de 4\nSeco\n\n\nMÃ¡s de 4 y menos de 12\nSemiseco\n\n\nMÃ¡s de 12 y menos de 45\nSemidulce\n\n\nMÃ¡s de 45\nDulce\n\n\n\nFiltrar el conjunto de datos para quedarse con los vinos Reserva o Gran Reserva con una calidad superior a 7 y ordenar el data frame por calidad de forma descendente.\nÂ¿CuÃ¡ntos vinos blancos con un contenido en alcohol superior al 12% y una calidad superior a 8 hay en el conjunto de datos?\nÂ¿CuÃ¡les son los 10 mejores vinos tintos crianza secos?",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1Â  IntroducciÃ³n",
    "section": "",
    "text": "1.1 El REPL de Julia\nLa gran potencia de cÃ¡lculo alcanzada por los ordenadores en las Ãºltimas dÃ©cadas ha convertido a los mismos en poderosas herramientas al servicio de todas aquellas disciplinas que, como las matemÃ¡ticas, requieren cÃ¡lculos largos y complejos.\nJulia es un lenguaje de programaciÃ³n especialmente orientado al cÃ¡lculo numÃ©rico y el anÃ¡lisis de datos. Julia permite ademÃ¡s realizar cÃ¡lculos simbÃ³licos y dispone de una gran biblioteca de paquetes con aplicaciones en muy diversas Ã¡reas de las MatemÃ¡ticas como CÃ¡lculo, Ãlgebra, GeometrÃ­a, MatemÃ¡tica Discreta o EstadÃ­stica.\nLa ventaja de Julia frente a otros programas habituales de cÃ¡lculo como Mathematica, MATLAB o Sage radica en su potencia de cÃ¡lculo y su velocidad (equiparable al lenguaje C), lo que lo hace ideal para manejar grandes volÃºmenes de datos o realizar tareas que requieran largos y complejos cÃ¡lculos. AdemÃ¡s, es software libre por lo que resulta ideal para introducirlo en el aula como soporte computacional para los modelos matemÃ¡ticos sin coste alguno.\nEn el siguiente enlace se explica el procedimiento de instalaciÃ³n de Julia.\nExisten tambiÃ©n varios entornos de desarrollo online que permiten ejecutar cÃ³digo en Julia sin necesidad de instalarlo en nuestro ordenador, como por ejemplo Replit, Cocalc o Codeanywhere.\nEl objetivo de esta prÃ¡ctica es introducir al alumno en la utilizaciÃ³n de este lenguaje, enseÃ±Ã¡ndole a realizar las operaciones bÃ¡sicas mÃ¡s habituales en CÃ¡lculo.\nPara arrancar el REPL^(REPL es el acrÃ³nimo de Read, Evaluate, Print and Loop, que describe el funcionamiento del compilador de Julia) de julia basta con abrir una terminal y teclear julia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#el-repl-de-julia",
    "href": "01-introduccion.html#el-repl-de-julia",
    "title": "1Â  IntroducciÃ³n",
    "section": "",
    "text": "prompt&gt; julia\n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.7.3 (2022-05-06)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia&gt;",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#el-gestor-de-paquetes-de-julia",
    "href": "01-introduccion.html#el-gestor-de-paquetes-de-julia",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.2 El gestor de paquetes de Julia",
    "text": "1.2 El gestor de paquetes de Julia\nJulia viene con varios paquetes bÃ¡sicos preinstalados, como por ejemplo el paquete LinearAlgebra que define funciones bÃ¡sicas del Ãlgebra Lineal, pero en estas prÃ¡cticas utilizaremos otros muchos paquetes que aÃ±aden mÃ¡s funcionalidades que no vienen instalados por defecto y tendremos que instalarlos aparte. Julia tiene un potente gestor de paquetes que facilita la bÃºsqueda, instalaciÃ³n, actualizaciÃ³n y eliminaciÃ³n de paquetes.\nPor defecto el gestor de paquetes utiliza el repositorio de paquetes oficial pero se pueden instalar paquetes de otros repositorios.\nPara entrar en el modo de gestiÃ³n de paquetes hay que teclear ]. Esto produce un cambio en el prompt del REPL de Julia.\nLos comandos mÃ¡s habituales son:\n\nadd p: Instala el paquete p en el entorno activo de Julia.\nupdate: Actualiza los paquetes del entorno activo de Julia.\nstatus: Muestra los paquetes instalados y sus versiones en el entorno activo de Julia.\nremove p: Elimina el paquete p del entorno activo de Julia.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara instalar el paquete SymPy para cÃ¡lculo simbÃ³lico basta con teclear add Sympy.\n(@v1.7) pkg&gt; add SymPy\n    Updating registry at `~/.julia/registries/General.toml`\n   Resolving package versions...\n    Updating `~/.julia/environments/v1.7/Project.toml`\n  [24249f21] + SymPy v1.1.6\n    Updating `~/.julia/environments/v1.7/Manifest.toml`\n  [3709ef60] + CommonEq v0.2.0\n  [38540f10] + CommonSolve v0.2.1\n  [438e738f] + PyCall v1.93.1\n  [24249f21] + SymPy v1.1.6",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-aritmÃ©ticos.",
    "href": "01-introduccion.html#operadores-aritmÃ©ticos.",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.3 Operadores aritmÃ©ticos.",
    "text": "1.3 Operadores aritmÃ©ticos.\nEl uso mÃ¡s simple de Julia es la realizaciÃ³n de operaciones aritmÃ©ticas como en una calculadora. En Julia se utilizan los siguientes operadores.\n\n\n\nOperador\nDescripciÃ³n\n\n\n\n\nx + y\nSuma\n\n\nx - y\nResta\n\n\nx * y\nProducto\n\n\nx / y\nDivisiÃ³n\n\n\nx Ã· y\nCociente divisiÃ³n entera\n\n\nx % y\nResto divisiÃ³n entera\n\n\nx ^ y\nPotencia",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-de-comparaciÃ³n",
    "href": "01-introduccion.html#operadores-de-comparaciÃ³n",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.4 Operadores de comparaciÃ³n",
    "text": "1.4 Operadores de comparaciÃ³n\n\n\n\nOperador\nDescripciÃ³n\n\n\n\n\n==\nIgualdad\n\n\n!=, â‰ \nDesigualdad\n\n\n&lt;\nMenor que\n\n\n&lt;=, â‰¤\nMenor o igual que\n\n\n&gt;\nMayor que\n\n\n&gt;=, â‰¥\nMayor o igual que",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-booleanos",
    "href": "01-introduccion.html#operadores-booleanos",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.5 Operadores booleanos",
    "text": "1.5 Operadores booleanos\n\n\n\nOperador\nDescripciÃ³n\n\n\n\n\n!x\nNegaciÃ³n\n\n\nx && y\nConjunciÃ³n (y)\n\n\nx || y\nDisyunciÃ³n (o)\n\n\n\nExisten tambiÃ©n un montÃ³n de funciones predefinidas habituales en CÃ¡lculo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-de-redondeo",
    "href": "01-introduccion.html#funciones-de-redondeo",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.6 Funciones de redondeo",
    "text": "1.6 Funciones de redondeo\n\n\n\n\n\n\n\nFunciÃ³n\nDescripciÃ³n\n\n\n\n\nround(x)\nDevuelve el entero mÃ¡s prÃ³ximo a x\n\n\nround(x, digits = n)\nDevuelve al valor mÃ¡s prÃ³ximo a x con n decimales\n\n\nfloor(x)\nRedondea x al prÃ³ximo entero menor\n\n\nceil(x)\nRedondea x al prÃ³ximo entero mayor\n\n\ntrunc(x)\nDevuelve la parte entera de x\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; round(2.7)\n3.0\n\njulia&gt; floor(2.7)\n2.0\n\njulia&gt; floor(-2.7)\n-3.0\n\njulia&gt; ceil(2.7)\n3.0\n\njulia&gt; ceil(-2.7)\n-2.0\n\njulia&gt; trunc(2.7)\n2.0\n\njulia&gt; trunc(-2.7)\n-2.0\n\njulia&gt; round(2.5)\n2.0\n\njulia&gt; round(2.786, digits = 2)\n2.79",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-de-divisiÃ³n",
    "href": "01-introduccion.html#funciones-de-divisiÃ³n",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.7 Funciones de divisiÃ³n",
    "text": "1.7 Funciones de divisiÃ³n\n\n\n\n\n\n\n\nFunciÃ³n\nDescripciÃ³n\n\n\n\n\ndiv(x,y), xÃ·y\nCociente de la divisiÃ³n entera\n\n\nfld(x,y)\nCociente de la divisiÃ³n entera redondeado hacia abajo\n\n\ncld(x,y)\nCociente de la divisiÃ³n entera redondeado hacia arriba\n\n\nrem(x,y), x%y\nResto de la divisiÃ³n entera. Se cumple x == div(x,y)*y + rem(x,y)\n\n\nmod(x,y)\nMÃ³dulo con respecto a y. Se cumple x == fld(x,y)*y + mod(x,y)\n\n\ngcd(x,y...)\nMÃ¡ximo comÃºn divisor positivo de x, y,â€¦\n\n\nlcm(x,y...)\nMÃ­nimo comÃºn mÃºltiplo positivo de x, y,â€¦\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; div(5,3)\n1\n\njulia&gt; cld(5,3)\n2\n\njulia&gt; 5%3\n2\n\njulia&gt; -5%3\n-2\n\njulia&gt; mod(5,3)\n2\n\njulia&gt; mod(-5,3)\n1\n\njulia&gt; gcd(12,18)\n6\n\njulia&gt; lcm(12,18)\n36",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-para-el-signo-y-el-valor-absoluto",
    "href": "01-introduccion.html#funciones-para-el-signo-y-el-valor-absoluto",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.8 Funciones para el signo y el valor absoluto",
    "text": "1.8 Funciones para el signo y el valor absoluto\n\n\n\n\n\n\n\nFunciÃ³n\nDescripciÃ³n\n\n\n\n\nabs(x)\nValor absoluto de x\n\n\nsign(x)\nDevuelve -1 si x es positivo, -1 si es negativo y 0 si es 0.\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; abs(2.5)\n2.5\n\njulia&gt; abs(-2.5)\n2.5\n\njulia&gt; sign(-2.5)\n-1.0\n\njulia&gt; sign(0)\n0\n\njulia&gt; sign(2.5)\n1.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#raÃ­ces-exponenciales-y-logaritmos",
    "href": "01-introduccion.html#raÃ­ces-exponenciales-y-logaritmos",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.9 RaÃ­ces, exponenciales y logaritmos",
    "text": "1.9 RaÃ­ces, exponenciales y logaritmos\n\n\n\n\n\n\n\nFunciÃ³n\nDescripciÃ³n\n\n\n\n\nsqrt(x), âˆšx\nRaÃ­z cuadrada de x\n\n\ncbrt(x), âˆ›x\nRaÃ­z cÃºbica de x\n\n\nexp(x)\nExponencial de x\n\n\nlog(x)\nLogaritmo neperiano de x\n\n\nlog(b,x)\nLogaritmo en base b de x\n\n\nlog2(x)\nLogaritmo en base 2 de x\n\n\nlog10(x)\nLogaritmo en base 10 de x\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; sqrt(4)\n2.0\n\njulia&gt; cbrt(27)\n3.0\n\njulia&gt; exp(1)\n2.718281828459045\n\njulia&gt; exp(-Inf)\n0.0\n\njulia&gt; log(1)\n0.0\n\njulia&gt; log(0)\n-Inf\n\njulia&gt; log(-1)\nERROR: DomainError with -1.0:\nlog will only return a complex result if called with a complex argument.\n...\n\njulia&gt; log(-1+0im)\n0.0 + 3.141592653589793im\n\njulia&gt; log2(2^3)\n3.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-trigonomÃ©tricas",
    "href": "01-introduccion.html#funciones-trigonomÃ©tricas",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.10 Funciones trigonomÃ©tricas",
    "text": "1.10 Funciones trigonomÃ©tricas\n\n\n\n\n\n\n\nFunciÃ³n\nDescripciÃ³n\n\n\n\n\nhypot(x,y)\nHipotenusa del triÃ¡ngulo rectÃ¡ngulo con catetos x e y\n\n\nsin(x)\nSeno del Ã¡ngulo x en radianes\n\n\nsind(x)\nSeno del Ã¡ngulo x en grados\n\n\ncos(x)\nCoseno del Ã¡ngulo x en radianes\n\n\ncosd(x)\nCoseno del Ã¡ngulo x en grados\n\n\ntan(x)\nTangente del Ã¡ngulo x en radianes\n\n\ntand(x)\nTangente del Ã¡ngulo x en grados\n\n\nsec(x)\nSecante del Ã¡ngulo x en radianes\n\n\ncsc(x)\nCosecante del Ã¡ngulo x en radianes\n\n\ncot(x)\nCotangente del Ã¡ngulo x en radianes\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; sin(Ï€/2)\n1.0\n\njulia&gt; cos(Ï€/2)\n6.123233995736766e-17\n\njulia&gt; cosd(90)\n0.0\n\njulia&gt; tan(Ï€/4)\n0.9999999999999999\n\njulia&gt; tand(45)\n1.0\n\njulia&gt; tan(Ï€/2)\n1.633123935319537e16\n\njulia&gt; tand(90)\nInf\n\njulia&gt; sin(Ï€/4)^2 + cos(Ï€/4)^2\n1.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-trigonomÃ©tricas-inversas",
    "href": "01-introduccion.html#funciones-trigonomÃ©tricas-inversas",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.11 Funciones trigonomÃ©tricas inversas",
    "text": "1.11 Funciones trigonomÃ©tricas inversas\n\n\n\n\n\n\n\nFunciÃ³n\nDescripciÃ³n\n\n\n\n\nasin(x)\nArcoseno (inversa del seno) de x en radianes\n\n\nasind(x)\nArcoseno (inversa del seno) de x en grados\n\n\nacos(x)\nArcocoseno (inversa del coseno) de x en radianes\n\n\nacosd(x)\nArcocoseno (inversa del coseno) de x en grados\n\n\natan(x)\nArcotangente (inversa de la tangente) de x en radianes\n\n\natand(x)\nArcotangente (inversa de la tangente) de x en grados\n\n\nasec(x)\nArcosecante (inversa de la secante) de x en radianes\n\n\nacsc(x)\nArcocosecante (inversa de la cosecante) de x en radianes\n\n\nacot(x)\nArcocotangente (inversa de la cotangente) de x en radianes\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; asin(1)\n1.5707963267948966\n\njulia&gt; asind(1)\n90.0\n\njulia&gt; acos(-1)\n3.141592653589793\n\njulia&gt; atan(1)\n0.7853981633974483\n\njulia&gt; atand(tan(Ï€/4))\n45.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#precedencia-de-operadores",
    "href": "01-introduccion.html#precedencia-de-operadores",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.12 Precedencia de operadores",
    "text": "1.12 Precedencia de operadores\nA la hora de evaluar una expresiÃ³n aritmÃ©tica, Julia evalÃºa los operadores segÃºn el siguiente orden de prioridad (de mayor a menor prioridad).\n\n\n\n\n\n\n\n\nCategorÃ­a\nOperadores\nAsociatividad\n\n\n\n\nFunciones\nexp, log, sin, etc.\n\n\n\nExponenciaciÃ³n\n^\nDerecha\n\n\nUnarios\n+ - âˆš\nDerecha\n\n\nFracciones\n//\nIzquierda\n\n\nMultiplicaciÃ³n\n* / % & \\ Ã·\nIzquierda\n\n\nAdiciÃ³n\n+ - |\nIzquierda\n\n\nComparaciones\n&gt; &lt; &gt;= &lt;= == != !==\n\n\n\nAsignaciones\n= += -= *= /= //= ^= Ã·= %= |= &=\nDerecha\n\n\n\nCuando se quiera evaluar un operador con menor prioridad antes que otro con mayor prioridad, hay que utilizar parÃ©ntesis.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; 1 + 4 ^ 2 / 2 - 3\n6.0\n\njulia&gt; (1 + 4 ^ 2) / 2 - 3\n5.5\n\njulia&gt; (1 + 4) ^ 2 / 2 - 3\n9.5\n\njulia&gt; 1 + 4 ^ 2 / (2 - 3)\n-15.0\n\njulia&gt; (1 + 4 ^ 2) / (2 - 3)\n-17.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#definiciÃ³n-de-variables",
    "href": "01-introduccion.html#definiciÃ³n-de-variables",
    "title": "1Â  IntroducciÃ³n",
    "section": "1.13 DefiniciÃ³n de variables",
    "text": "1.13 DefiniciÃ³n de variables\nPara definir variables se pueden utilizar cualquier carÃ¡cter Unicode. Los nombres de las variables pueden contener mÃ¡s de una letra y, en tal caso, pueden usarse tambiÃ©n nÃºmeros, pero siempre debe comenzar por una letra. AsÃ­, para Julia, la expresiÃ³n xy, no se interpreta como el producto de la variable \\(x\\) por la variable \\(y\\), sino como la variable \\(xy\\). AdemÃ¡s, se distingue entre mayÃºsculas y minÃºsculas, asÃ­ que no es lo mismo \\(xy\\) que \\(xY\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>IntroducciÃ³n</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "PÃ¡cticas de Aprendizaje AutomÃ¡tico con Julia",
    "section": "",
    "text": "Prefacio\nÂ¡Bienvenido a PrÃ¡cticas de Aprendizaje AutomÃ¡tico con Julia!\nEste libro presenta una recopilaciÃ³n de prÃ¡cticas de Aprendizaje AutomÃ¡tico (Machine Learning) con el lenguaje de programaciÃ³n Julia.\nNo es un libro para aprender a programar con Julia, ya que solo enseÃ±a el uso del lenguaje y de algunos de sus paquetes para implementar los algoritmos mÃ¡s comunes de Aprendizaje AutomÃ¡tico. Para quienes estÃ©n interesados en aprender a programar en este Julia, os recomiendo leer este manual de Julia.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "PÃ¡cticas de Aprendizaje AutomÃ¡tico con Julia",
    "section": "Licencia",
    "text": "Licencia\nEsta obra estÃ¡ bajo una licencia Reconocimiento â€“ No comercial â€“ Compartir bajo la misma licencia 3.0 EspaÃ±a de Creative Commons. Para ver una copia de esta licencia, visite https://creativecommons.org/licenses/by-nc-sa/3.0/es/.\nCon esta licencia eres libre de:\n\nCopiar, distribuir y mostrar este trabajo.\nRealizar modificaciones de este trabajo.\n\nBajo las siguientes condiciones:\n\nReconocimiento. Debe reconocer los crÃ©ditos de la obra de la manera especificada por el autor o el licenciador (pero no de una manera que sugiera que tiene su apoyo o apoyan el uso que hace de su obra).\nNo comercial. No puede utilizar esta obra para fines comerciales.\nCompartir bajo la misma licencia. Si altera o transforma esta obra, o genera una obra derivada, sÃ³lo puede distribuir la obra generada bajo una licencia idÃ©ntica a Ã©sta.\n\nAl reutilizar o distribuir la obra, tiene que dejar bien claro los tÃ©rminos de la licencia de esta obra.\nEstas condiciones pueden no aplicarse si se obtiene el permiso del titular de los derechos de autor.\nNada en esta licencia menoscaba o restringe los derechos morales del autor.",
    "crumbs": [
      "Prefacio"
    ]
  }
]