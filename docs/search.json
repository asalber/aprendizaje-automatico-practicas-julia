[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Pácticas de Aprendizaje Automático con Julia",
    "section": "",
    "text": "Prefacio\n¡Bienvenido a Prácticas de Aprendizaje Automático con Julia!\nEste libro presenta una recopilación de prácticas de Aprendizaje Automático (Machine Learning) con el lenguaje de programación Julia.\nNo es un libro para aprender a programar con Julia, ya que solo enseña el uso del lenguaje y de algunos de sus paquetes para implementar los algoritmos más comunes de Aprendizaje Automático. Para quienes estén interesados en aprender a programar en este Julia, os recomiendo leer este manual de Julia.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Pácticas de Aprendizaje Automático con Julia",
    "section": "Licencia",
    "text": "Licencia\nEsta obra está bajo una licencia Reconocimiento – No comercial – Compartir bajo la misma licencia 3.0 España de Creative Commons. Para ver una copia de esta licencia, visite https://creativecommons.org/licenses/by-nc-sa/3.0/es/.\nCon esta licencia eres libre de:\n\nCopiar, distribuir y mostrar este trabajo.\nRealizar modificaciones de este trabajo.\n\nBajo las siguientes condiciones:\n\nReconocimiento. Debe reconocer los créditos de la obra de la manera especificada por el autor o el licenciador (pero no de una manera que sugiera que tiene su apoyo o apoyan el uso que hace de su obra).\nNo comercial. No puede utilizar esta obra para fines comerciales.\nCompartir bajo la misma licencia. Si altera o transforma esta obra, o genera una obra derivada, sólo puede distribuir la obra generada bajo una licencia idéntica a ésta.\n\nAl reutilizar o distribuir la obra, tiene que dejar bien claro los términos de la licencia de esta obra.\nEstas condiciones pueden no aplicarse si se obtiene el permiso del titular de los derechos de autor.\nNada en esta licencia menoscaba o restringe los derechos morales del autor.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "01-introduccion.html",
    "href": "01-introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "1.1 El REPL de Julia\nLa gran potencia de cálculo alcanzada por los ordenadores en las últimas décadas ha convertido a los mismos en poderosas herramientas al servicio de todas aquellas disciplinas que, como las matemáticas, requieren cálculos largos y complejos.\nJulia es un lenguaje de programación especialmente orientado al cálculo numérico y el análisis de datos. Julia permite además realizar cálculos simbólicos y dispone de una gran biblioteca de paquetes con aplicaciones en muy diversas áreas de las Matemáticas como Cálculo, Álgebra, Geometría, Matemática Discreta o Estadística.\nLa ventaja de Julia frente a otros programas habituales de cálculo como Mathematica, MATLAB o Sage radica en su potencia de cálculo y su velocidad (equiparable al lenguaje C), lo que lo hace ideal para manejar grandes volúmenes de datos o realizar tareas que requieran largos y complejos cálculos. Además, es software libre por lo que resulta ideal para introducirlo en el aula como soporte computacional para los modelos matemáticos sin coste alguno.\nEn el siguiente enlace se explica el procedimiento de instalación de Julia.\nExisten también varios entornos de desarrollo online que permiten ejecutar código en Julia sin necesidad de instalarlo en nuestro ordenador, como por ejemplo Replit, Cocalc o Codeanywhere.\nEl objetivo de esta práctica es introducir al alumno en la utilización de este lenguaje, enseñándole a realizar las operaciones básicas más habituales en Cálculo.\nPara arrancar el REPL^(REPL es el acrónimo de Read, Evaluate, Print and Loop, que describe el funcionamiento del compilador de Julia) de julia basta con abrir una terminal y teclear julia.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#el-repl-de-julia",
    "href": "01-introduccion.html#el-repl-de-julia",
    "title": "1  Introducción",
    "section": "",
    "text": "prompt&gt; julia\n               _\n   _       _ _(_)_     |  Documentation: https://docs.julialang.org\n  (_)     | (_) (_)    |\n   _ _   _| |_  __ _   |  Type \"?\" for help, \"]?\" for Pkg help.\n  | | | | | | |/ _` |  |\n  | | |_| | | | (_| |  |  Version 1.7.3 (2022-05-06)\n _/ |\\__'_|_|_|\\__'_|  |  Official https://julialang.org/ release\n|__/                   |\n\njulia&gt;",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#el-gestor-de-paquetes-de-julia",
    "href": "01-introduccion.html#el-gestor-de-paquetes-de-julia",
    "title": "1  Introducción",
    "section": "1.2 El gestor de paquetes de Julia",
    "text": "1.2 El gestor de paquetes de Julia\nJulia viene con varios paquetes básicos preinstalados, como por ejemplo el paquete LinearAlgebra que define funciones básicas del Álgebra Lineal, pero en estas prácticas utilizaremos otros muchos paquetes que añaden más funcionalidades que no vienen instalados por defecto y tendremos que instalarlos aparte. Julia tiene un potente gestor de paquetes que facilita la búsqueda, instalación, actualización y eliminación de paquetes.\nPor defecto el gestor de paquetes utiliza el repositorio de paquetes oficial pero se pueden instalar paquetes de otros repositorios.\nPara entrar en el modo de gestión de paquetes hay que teclear ]. Esto produce un cambio en el prompt del REPL de Julia.\nLos comandos más habituales son:\n\nadd p: Instala el paquete p en el entorno activo de Julia.\nupdate: Actualiza los paquetes del entorno activo de Julia.\nstatus: Muestra los paquetes instalados y sus versiones en el entorno activo de Julia.\nremove p: Elimina el paquete p del entorno activo de Julia.\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\nPara instalar el paquete SymPy para cálculo simbólico basta con teclear add Sympy.\n(@v1.7) pkg&gt; add SymPy\n    Updating registry at `~/.julia/registries/General.toml`\n   Resolving package versions...\n    Updating `~/.julia/environments/v1.7/Project.toml`\n  [24249f21] + SymPy v1.1.6\n    Updating `~/.julia/environments/v1.7/Manifest.toml`\n  [3709ef60] + CommonEq v0.2.0\n  [38540f10] + CommonSolve v0.2.1\n  [438e738f] + PyCall v1.93.1\n  [24249f21] + SymPy v1.1.6",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-aritméticos.",
    "href": "01-introduccion.html#operadores-aritméticos.",
    "title": "1  Introducción",
    "section": "1.3 Operadores aritméticos.",
    "text": "1.3 Operadores aritméticos.\nEl uso más simple de Julia es la realización de operaciones aritméticas como en una calculadora. En Julia se utilizan los siguientes operadores.\n\n\n\nOperador\nDescripción\n\n\n\n\nx + y\nSuma\n\n\nx - y\nResta\n\n\nx * y\nProducto\n\n\nx / y\nDivisión\n\n\nx ÷ y\nCociente división entera\n\n\nx % y\nResto división entera\n\n\nx ^ y\nPotencia",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-de-comparación",
    "href": "01-introduccion.html#operadores-de-comparación",
    "title": "1  Introducción",
    "section": "1.4 Operadores de comparación",
    "text": "1.4 Operadores de comparación\n\n\n\nOperador\nDescripción\n\n\n\n\n==\nIgualdad\n\n\n!=, ≠\nDesigualdad\n\n\n&lt;\nMenor que\n\n\n&lt;=, ≤\nMenor o igual que\n\n\n&gt;\nMayor que\n\n\n&gt;=, ≥\nMayor o igual que",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#operadores-booleanos",
    "href": "01-introduccion.html#operadores-booleanos",
    "title": "1  Introducción",
    "section": "1.5 Operadores booleanos",
    "text": "1.5 Operadores booleanos\n\n\n\nOperador\nDescripción\n\n\n\n\n!x\nNegación\n\n\nx && y\nConjunción (y)\n\n\nx || y\nDisyunción (o)\n\n\n\nExisten también un montón de funciones predefinidas habituales en Cálculo.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-de-redondeo",
    "href": "01-introduccion.html#funciones-de-redondeo",
    "title": "1  Introducción",
    "section": "1.6 Funciones de redondeo",
    "text": "1.6 Funciones de redondeo\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nround(x)\nDevuelve el entero más próximo a x\n\n\nround(x, digits = n)\nDevuelve al valor más próximo a x con n decimales\n\n\nfloor(x)\nRedondea x al próximo entero menor\n\n\nceil(x)\nRedondea x al próximo entero mayor\n\n\ntrunc(x)\nDevuelve la parte entera de x\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; round(2.7)\n3.0\n\njulia&gt; floor(2.7)\n2.0\n\njulia&gt; floor(-2.7)\n-3.0\n\njulia&gt; ceil(2.7)\n3.0\n\njulia&gt; ceil(-2.7)\n-2.0\n\njulia&gt; trunc(2.7)\n2.0\n\njulia&gt; trunc(-2.7)\n-2.0\n\njulia&gt; round(2.5)\n2.0\n\njulia&gt; round(2.786, digits = 2)\n2.79",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-de-división",
    "href": "01-introduccion.html#funciones-de-división",
    "title": "1  Introducción",
    "section": "1.7 Funciones de división",
    "text": "1.7 Funciones de división\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\ndiv(x,y), x÷y\nCociente de la división entera\n\n\nfld(x,y)\nCociente de la división entera redondeado hacia abajo\n\n\ncld(x,y)\nCociente de la división entera redondeado hacia arriba\n\n\nrem(x,y), x%y\nResto de la división entera. Se cumple x == div(x,y)*y + rem(x,y)\n\n\nmod(x,y)\nMódulo con respecto a y. Se cumple x == fld(x,y)*y + mod(x,y)\n\n\ngcd(x,y...)\nMáximo común divisor positivo de x, y,…\n\n\nlcm(x,y...)\nMínimo común múltiplo positivo de x, y,…\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; div(5,3)\n1\n\njulia&gt; cld(5,3)\n2\n\njulia&gt; 5%3\n2\n\njulia&gt; -5%3\n-2\n\njulia&gt; mod(5,3)\n2\n\njulia&gt; mod(-5,3)\n1\n\njulia&gt; gcd(12,18)\n6\n\njulia&gt; lcm(12,18)\n36",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-para-el-signo-y-el-valor-absoluto",
    "href": "01-introduccion.html#funciones-para-el-signo-y-el-valor-absoluto",
    "title": "1  Introducción",
    "section": "1.8 Funciones para el signo y el valor absoluto",
    "text": "1.8 Funciones para el signo y el valor absoluto\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nabs(x)\nValor absoluto de x\n\n\nsign(x)\nDevuelve -1 si x es positivo, -1 si es negativo y 0 si es 0.\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; abs(2.5)\n2.5\n\njulia&gt; abs(-2.5)\n2.5\n\njulia&gt; sign(-2.5)\n-1.0\n\njulia&gt; sign(0)\n0\n\njulia&gt; sign(2.5)\n1.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#raíces-exponenciales-y-logaritmos",
    "href": "01-introduccion.html#raíces-exponenciales-y-logaritmos",
    "title": "1  Introducción",
    "section": "1.9 Raíces, exponenciales y logaritmos",
    "text": "1.9 Raíces, exponenciales y logaritmos\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nsqrt(x), √x\nRaíz cuadrada de x\n\n\ncbrt(x), ∛x\nRaíz cúbica de x\n\n\nexp(x)\nExponencial de x\n\n\nlog(x)\nLogaritmo neperiano de x\n\n\nlog(b,x)\nLogaritmo en base b de x\n\n\nlog2(x)\nLogaritmo en base 2 de x\n\n\nlog10(x)\nLogaritmo en base 10 de x\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; sqrt(4)\n2.0\n\njulia&gt; cbrt(27)\n3.0\n\njulia&gt; exp(1)\n2.718281828459045\n\njulia&gt; exp(-Inf)\n0.0\n\njulia&gt; log(1)\n0.0\n\njulia&gt; log(0)\n-Inf\n\njulia&gt; log(-1)\nERROR: DomainError with -1.0:\nlog will only return a complex result if called with a complex argument.\n...\n\njulia&gt; log(-1+0im)\n0.0 + 3.141592653589793im\n\njulia&gt; log2(2^3)\n3.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-trigonométricas",
    "href": "01-introduccion.html#funciones-trigonométricas",
    "title": "1  Introducción",
    "section": "1.10 Funciones trigonométricas",
    "text": "1.10 Funciones trigonométricas\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nhypot(x,y)\nHipotenusa del triángulo rectángulo con catetos x e y\n\n\nsin(x)\nSeno del ángulo x en radianes\n\n\nsind(x)\nSeno del ángulo x en grados\n\n\ncos(x)\nCoseno del ángulo x en radianes\n\n\ncosd(x)\nCoseno del ángulo x en grados\n\n\ntan(x)\nTangente del ángulo x en radianes\n\n\ntand(x)\nTangente del ángulo x en grados\n\n\nsec(x)\nSecante del ángulo x en radianes\n\n\ncsc(x)\nCosecante del ángulo x en radianes\n\n\ncot(x)\nCotangente del ángulo x en radianes\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; sin(π/2)\n1.0\n\njulia&gt; cos(π/2)\n6.123233995736766e-17\n\njulia&gt; cosd(90)\n0.0\n\njulia&gt; tan(π/4)\n0.9999999999999999\n\njulia&gt; tand(45)\n1.0\n\njulia&gt; tan(π/2)\n1.633123935319537e16\n\njulia&gt; tand(90)\nInf\n\njulia&gt; sin(π/4)^2 + cos(π/4)^2\n1.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#funciones-trigonométricas-inversas",
    "href": "01-introduccion.html#funciones-trigonométricas-inversas",
    "title": "1  Introducción",
    "section": "1.11 Funciones trigonométricas inversas",
    "text": "1.11 Funciones trigonométricas inversas\n\n\n\n\n\n\n\nFunción\nDescripción\n\n\n\n\nasin(x)\nArcoseno (inversa del seno) de x en radianes\n\n\nasind(x)\nArcoseno (inversa del seno) de x en grados\n\n\nacos(x)\nArcocoseno (inversa del coseno) de x en radianes\n\n\nacosd(x)\nArcocoseno (inversa del coseno) de x en grados\n\n\natan(x)\nArcotangente (inversa de la tangente) de x en radianes\n\n\natand(x)\nArcotangente (inversa de la tangente) de x en grados\n\n\nasec(x)\nArcosecante (inversa de la secante) de x en radianes\n\n\nacsc(x)\nArcocosecante (inversa de la cosecante) de x en radianes\n\n\nacot(x)\nArcocotangente (inversa de la cotangente) de x en radianes\n\n\n\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; asin(1)\n1.5707963267948966\n\njulia&gt; asind(1)\n90.0\n\njulia&gt; acos(-1)\n3.141592653589793\n\njulia&gt; atan(1)\n0.7853981633974483\n\njulia&gt; atand(tan(π/4))\n45.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#precedencia-de-operadores",
    "href": "01-introduccion.html#precedencia-de-operadores",
    "title": "1  Introducción",
    "section": "1.12 Precedencia de operadores",
    "text": "1.12 Precedencia de operadores\nA la hora de evaluar una expresión aritmética, Julia evalúa los operadores según el siguiente orden de prioridad (de mayor a menor prioridad).\n\n\n\n\n\n\n\n\nCategoría\nOperadores\nAsociatividad\n\n\n\n\nFunciones\nexp, log, sin, etc.\n\n\n\nExponenciación\n^\nDerecha\n\n\nUnarios\n+ - √\nDerecha\n\n\nFracciones\n//\nIzquierda\n\n\nMultiplicación\n* / % & \\ ÷\nIzquierda\n\n\nAdición\n+ - |\nIzquierda\n\n\nComparaciones\n&gt; &lt; &gt;= &lt;= == != !==\n\n\n\nAsignaciones\n= += -= *= /= //= ^= ÷= %= |= &=\nDerecha\n\n\n\nCuando se quiera evaluar un operador con menor prioridad antes que otro con mayor prioridad, hay que utilizar paréntesis.\n\n\n\n\n\n\nEjemplo\n\n\n\n\n\njulia&gt; 1 + 4 ^ 2 / 2 - 3\n6.0\n\njulia&gt; (1 + 4 ^ 2) / 2 - 3\n5.5\n\njulia&gt; (1 + 4) ^ 2 / 2 - 3\n9.5\n\njulia&gt; 1 + 4 ^ 2 / (2 - 3)\n-15.0\n\njulia&gt; (1 + 4 ^ 2) / (2 - 3)\n-17.0",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "01-introduccion.html#definición-de-variables",
    "href": "01-introduccion.html#definición-de-variables",
    "title": "1  Introducción",
    "section": "1.13 Definición de variables",
    "text": "1.13 Definición de variables\nPara definir variables se pueden utilizar cualquier carácter Unicode. Los nombres de las variables pueden contener más de una letra y, en tal caso, pueden usarse también números, pero siempre debe comenzar por una letra. Así, para Julia, la expresión xy, no se interpreta como el producto de la variable \\(x\\) por la variable \\(y\\), sino como la variable \\(xy\\). Además, se distingue entre mayúsculas y minúsculas, así que no es lo mismo \\(xy\\) que \\(xY\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html",
    "href": "02-preprocesamiento.html",
    "title": "2  Preprocesamiento de datos",
    "section": "",
    "text": "2.1 Ejercicios Resueltos\nEsta práctica contiene ejercicios que muestran como preprocesar un conjunto de datos con Julia. El preprocesamiento de datos es una tarea fundamental en la construcción de modelos de aprendizaje automático que consiste en la limpieza, transformación y preparación de los datos para que puedan alimentar el proceso de entrenamiento de los modelos, así como para la evaluación de su rendimiento. El preprocesamiento de datos incluye tareas como\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-resueltos",
    "href": "02-preprocesamiento.html#ejercicios-resueltos",
    "title": "2  Preprocesamiento de datos",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de gráficas.\nusing Makie  # Para obtener gráficos interactivos.\n\nEjercicio 2.1 La siguiente tabla contiene los ingresos y gastos de una empresa durante el primer trimestre del año.\n\nCrear un data frame con los datos de la tabla.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función DataFrame del paquete DataFrames para partir el rango de valores en intervalos y asociar a cada intervalo una categoría.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing DataFrames\ndf = DataFrame(\n    Mes = [\"Enero\", \"Febrero\", \"Marzo\", \"Abril\"],\n    Ingresos = [45000, 41500, 51200, 49700],\n    Gastos = [33400, 35400, 35600, 36300],\n    Impuestos = [6450, 6300, 7100, 6850]\n    )\n\n4×4 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\n\n\n\nString\nInt64\nInt64\nInt64\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n\n\n2\nFebrero\n41500\n35400\n6300\n\n\n3\nMarzo\n51200\n35600\n7100\n\n\n4\nAbril\n49700\n36300\n6850\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con los beneficios de cada mes (ingresos - gastos - impuestos).\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf.Beneficios = df.Ingresos - df.Gastos - df.Impuestos\ndf\n\n4×5 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\nBeneficios\n\n\n\nString\nInt64\nInt64\nInt64\nInt64\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n5150\n\n\n2\nFebrero\n41500\n35400\n6300\n-200\n\n\n3\nMarzo\n51200\n35600\n7100\n8500\n\n\n4\nAbril\n49700\n36300\n6850\n6550\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con la variable Balance con dos posibles categorías: positivo si ha habido beneficios y negativo si ha habido pérdidas.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf.Balance = ifelse.(df.Beneficios .&gt; 0, \"positivo\", \"negativo\")\ndf\n\n4×6 DataFrame\n\n\n\nRow\nMes\nIngresos\nGastos\nImpuestos\nBeneficios\nBalance\n\n\n\nString\nInt64\nInt64\nInt64\nInt64\nString\n\n\n\n\n1\nEnero\n45000\n33400\n6450\n5150\npositivo\n\n\n2\nFebrero\n41500\n35400\n6300\n-200\nnegativo\n\n\n3\nMarzo\n51200\n35600\n7100\n8500\npositivo\n\n\n4\nAbril\n49700\n36300\n6850\n6550\npositivo\n\n\n\n\n\n\n\n\n\nFiltrar el conjunto de datos para quedarse con los nombres de los meses y los beneficios de los meses con balance positivo.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf[df.Balance .== \"positivo\", [:Mes, :Beneficios]]\n\n3×2 DataFrame\n\n\n\nRow\nMes\nBeneficios\n\n\n\nString\nInt64\n\n\n\n\n1\nEnero\n5150\n\n\n2\nMarzo\n8500\n\n\n3\nAbril\n6550\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.2 El fichero colesterol.csv contiene información de una muestra de pacientes donde se han medido la edad, el sexo, el peso, la altura y el nivel de colesterol, además de su nombre.\n\nCrear un data frame con los datos de todos los pacientes del estudio a partir del fichero colesterol.csv.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función CSV.read del paquete CSV para crear und data frame a partir de un fichero CSV. Si el fichero está en una url, utilizar la función download(url) para descargar el fichero y después leerlo con la función [CSV.read].\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CSV\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/colesterol.csv\"), DataFrame)\n\n14×6 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con el índice de masa corporal, usando la siguiente fórmula\n\\[\n\\mbox{IMC} = \\frac{\\mbox{Peso (kg)}}{\\mbox{Altura (cm)}^2}\n\\]\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf.imc = df.peso ./ (df.altura .^ 2)\ndf\n\n14×7 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n21.7738\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\n\n\n\n\n\n\n\n\n\nCrear una nueva columna con la variable obesidad recodificando la columna imc en las siguientes categorías.\n\n\n\nRango IMC\nCategoría\n\n\n\n\nMenor de 18.5\nBajo peso\n\n\nDe 18.5 a 24.5\nSaludable\n\n\nDe 24.5 a 30\nSobrepeso\n\n\nMayor de 30\nObeso\n\n\n\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función cut del paquete CategoricalArrays para partir el rango de valores en intervalos y asociar a cada intervalo una categoría.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CategoricalArrays\ndf.obesidad = cut(df.imc, [0, 18.5, 24.5, 30, Inf],\n                labels=[\"Bajo peso\", \"Saludable\", \"Sobrepeso\", \"Obeso\"],\n                extend=true)\ndf\n\n14×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nSeleccionar las columnas nombre, sexo y edad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf[:, [:nombre, :sexo, :edad]]\n\n14×3 DataFrame\n\n\n\nRow\nnombre\nsexo\nedad\n\n\n\nString\nString1\nInt64\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\nH\n18\n\n\n2\nRosa Díaz Díaz\nM\n32\n\n\n3\nJavier García Sánchez\nH\n24\n\n\n4\nCarmen López Pinzón\nM\n35\n\n\n5\nMarisa López Collado\nM\n46\n\n\n6\nAntonio Ruiz Cruz\nH\n68\n\n\n7\nAntonio Fernández Ocaña\nH\n51\n\n\n8\nPilar Martín González\nM\n22\n\n\n9\nPedro Gálvez Tenorio\nH\n35\n\n\n10\nSantiago Reillo Manzano\nH\n46\n\n\n11\nMacarena Álvarez Luna\nM\n53\n\n\n12\nJosé María de la Guía Sanz\nH\n58\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\nH\n27\n\n\n14\nCarolina Rubio Moreno\nM\n20\n\n\n\n\n\n\n\n\n\nAnonimizar los datos eliminando la columna nombre.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función select del paquete DataFrames para seleccionar las columnas deseadas y eliminar las columnas no deseadas. Existe también la función select! que modifica el data frame original eliminando las columnas no seleccionadas.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nselect(df, Not(:nombre))\n\n14×7 DataFrame\n\n\n\nRow\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nReordenar las columnas poniendo la columna sexo antes que la columna edad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nselect(df, Cols(:sexo, :edad, Not(:sexo, :edad)))\n\n14×8 DataFrame\n\n\n\nRow\nsexo\nedad\nnombre\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString1\nInt64\nString\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nH\n18\nJosé Luis Martínez Izquierdo\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nM\n32\nRosa Díaz Díaz\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nH\n24\nJavier García Sánchez\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nM\n35\nCarmen López Pinzón\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nM\n46\nMarisa López Collado\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nH\n68\nAntonio Ruiz Cruz\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nH\n51\nAntonio Fernández Ocaña\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nM\n22\nPilar Martín González\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n9\nH\n35\nPedro Gálvez Tenorio\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nH\n46\nSantiago Reillo Manzano\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nM\n53\nMacarena Álvarez Luna\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nH\n58\nJosé María de la Guía Sanz\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nH\n27\nMiguel Angel Cuadrado Gutiérrez\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nM\n20\nCarolina Rubio Moreno\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las mujeres.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf[df.sexo .== \"M\", :]\n\n6×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n2\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n3\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n4\nPilar Martín González\n22\nM\n60.0\n1.66\nmissing\n21.7738\nSaludable\n\n\n5\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n6\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con los hombres mayores de 30 años.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndf[(df.sexo .== \"H\") .& (df.edad .&gt; 30), :]\n\n5×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64?\nFloat64?\nCat…?\n\n\n\n\n1\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n2\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n3\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n4\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n5\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para quedarse con las filas sin valores perdidos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función dropmissing del paquete DataFrames para eliminar las filas con valores perdidos.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndropmissing(df)\n\n12×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64\nFloat64\nFloat64\nFloat64\nCat…\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n4\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n5\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n6\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n7\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n8\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n9\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n10\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n11\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n12\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nFiltrar el data frame para eliminar las filas con datos perdidos en la columna colesterol.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función dropmissing, col donde col es el nombre de la columna que contiene los valores perdidos.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndropmissing(df, :colesterol)    \n\n13×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n9\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n10\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n11\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n12\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n13\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nImputar los valores perdidos en la columna colesterol con la media de los valores no perdidos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función coalesce para reemplazar los valores perdidos por otros valores.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing Statistics\nmedia_colesterol = mean(skipmissing(df.colesterol))\ndf.colesterol = coalesce.(df.colesterol, media_colesterol)\ndf\n\n14×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\n\n\n\n\n1\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n2\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n3\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n4\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n5\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n6\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n7\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n8\nPilar Martín González\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n\n\n9\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n10\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n11\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n12\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n13\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\nOrdenar el data frame según la columna nombre.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función sort para ordenar las filas del data frame según los valores de una o varias columnas. Utilizar el parámetro rev para especificar mediante un vector de booleanos si el orden es ascendente o descendente.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nsort(df, :nombre)\n\n14×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\n\n\n\n\n1\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n2\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n3\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n4\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n5\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n6\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n7\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n8\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n9\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n10\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n11\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n12\nPilar Martín González\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n\n\n13\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n14\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n\n\n\n\n\n\n\nOrdenar el data frame ascendentemente por la columna sexo y descendentemente por la columna edad.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nsort(df, [:sexo, :edad], rev=[false, true])\n\n14×8 DataFrame\n\n\n\nRow\nnombre\nedad\nsexo\npeso\naltura\ncolesterol\nimc\nobesidad\n\n\n\nString\nInt64\nString1\nFloat64?\nFloat64\nFloat64\nFloat64?\nCat…?\n\n\n\n\n1\nAntonio Ruiz Cruz\n68\nH\n66.0\n1.74\n249.0\n21.7994\nSaludable\n\n\n2\nJosé María de la Guía Sanz\n58\nH\n78.0\n1.87\n198.0\n22.3055\nSaludable\n\n\n3\nAntonio Fernández Ocaña\n51\nH\n62.0\n1.72\n276.0\n20.9573\nSaludable\n\n\n4\nSantiago Reillo Manzano\n46\nH\n75.0\n1.85\n280.0\n21.9138\nSaludable\n\n\n5\nPedro Gálvez Tenorio\n35\nH\n90.0\n1.94\n241.0\n23.9133\nSaludable\n\n\n6\nMiguel Angel Cuadrado Gutiérrez\n27\nH\n109.0\n1.98\n210.0\n27.8033\nSobrepeso\n\n\n7\nJavier García Sánchez\n24\nH\nmissing\n1.81\n191.0\nmissing\nmissing\n\n\n8\nJosé Luis Martínez Izquierdo\n18\nH\n85.0\n1.79\n182.0\n26.5285\nSobrepeso\n\n\n9\nMacarena Álvarez Luna\n53\nM\n55.0\n1.62\n262.0\n20.9572\nSaludable\n\n\n10\nMarisa López Collado\n46\nM\n51.0\n1.58\n148.0\n20.4294\nSaludable\n\n\n11\nCarmen López Pinzón\n35\nM\n65.0\n1.7\n200.0\n22.4913\nSaludable\n\n\n12\nRosa Díaz Díaz\n32\nM\n65.0\n1.73\n232.0\n21.7181\nSaludable\n\n\n13\nPilar Martín González\n22\nM\n60.0\n1.66\n220.231\n21.7738\nSaludable\n\n\n14\nCarolina Rubio Moreno\n20\nM\n61.0\n1.77\n194.0\n19.4708\nSaludable\n\n\n\n\n\n\n\n\n\n\n\n\nEjercicio 2.3 El fichero notas-curso2.csv contiene información de las notas de los alumnos de un curso.\n\nCrear un data frame con los datos de los alumnos del curso a partir del fichero notas-curso2.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/notas-curso2.csv\"), DataFrame; missingstring=\"NA\")\n\n120×9 DataFrame95 rows omitted\n\n\n\nRow\nsexo\nturno\ngrupo\ntrabaja\nnotaA\nnotaB\nnotaC\nnotaD\nnotaE\n\n\n\nString7\nString7\nString1\nString1\nFloat64\nFloat64?\nFloat64?\nFloat64?\nFloat64?\n\n\n\n\n1\nMujer\nTarde\nC\nN\n5.2\n6.3\n3.4\n2.3\n2.0\n\n\n2\nHombre\nMañana\nA\nN\n5.7\n5.7\n4.2\n3.5\n2.7\n\n\n3\nHombre\nMañana\nB\nN\n8.3\n8.8\n8.8\n8.0\n5.5\n\n\n4\nHombre\nMañana\nB\nN\n6.1\n6.8\n4.0\n3.5\n2.2\n\n\n5\nHombre\nMañana\nA\nN\n6.2\n9.0\n5.0\n4.4\n3.7\n\n\n6\nHombre\nMañana\nA\nS\n8.6\n8.9\n9.5\n8.4\n3.9\n\n\n7\nMujer\nMañana\nA\nN\n6.7\n7.9\n5.6\n4.8\n4.2\n\n\n8\nMujer\nTarde\nC\nS\n4.1\n5.2\n1.7\n0.3\n1.0\n\n\n9\nHombre\nTarde\nC\nN\n5.0\n5.0\n3.3\n2.7\n6.0\n\n\n10\nHombre\nTarde\nC\nN\n5.3\n6.3\n4.8\n3.6\n2.3\n\n\n11\nMujer\nMañana\nA\nN\n7.8\nmissing\n6.5\n6.7\n2.8\n\n\n12\nHombre\nMañana\nA\nN\n6.5\n8.0\n5.0\n3.2\n3.3\n\n\n13\nHombre\nMañana\nB\nN\n6.6\n7.6\n5.3\n4.0\n1.0\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nMujer\nTarde\nC\nN\n5.4\n7.3\n3.5\n2.5\n4.6\n\n\n110\nHombre\nMañana\nB\nN\n7.4\n7.4\n6.2\n5.8\n1.1\n\n\n111\nMujer\nTarde\nC\nS\n5.1\n8.1\n5.2\n5.1\n4.5\n\n\n112\nHombre\nMañana\nA\nN\n6.9\n7.8\n3.9\n2.8\nmissing\n\n\n113\nHombre\nTarde\nC\nN\n3.6\n4.8\n2.1\n0.5\n5.6\n\n\n114\nHombre\nTarde\nC\nS\n5.9\n6.2\n5.0\n3.9\n1.9\n\n\n115\nHombre\nMañana\nB\nN\n6.8\n7.2\n4.9\n3.8\n2.8\n\n\n116\nHombre\nMañana\nA\nN\n6.5\n6.1\n5.8\n4.9\n1.2\n\n\n117\nMujer\nMañana\nB\nN\n6.2\n7.0\n5.6\n5.4\n1.7\n\n\n118\nMujer\nTarde\nC\nN\n5.0\n6.5\n4.0\n2.8\n3.6\n\n\n119\nHombre\nTarde\nC\nN\n4.7\n6.0\n1.3\n0.4\n2.2\n\n\n120\nHombre\nTarde\nC\nS\n4.5\n4.7\n6.0\n4.9\n1.8\n\n\n\n\n\n\n\n\n\nObtener el número de datos perdidos en cada columna.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndescribe(df)[:, [:variable, :nmissing]]\n\n9×2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\nsexo\n0\n\n\n2\nturno\n0\n\n\n3\ngrupo\n0\n\n\n4\ntrabaja\n0\n\n\n5\nnotaA\n0\n\n\n6\nnotaB\n5\n\n\n7\nnotaC\n1\n\n\n8\nnotaD\n2\n\n\n9\nnotaE\n2\n\n\n\n\n\n\n\n\n\nRecodificar la variable grupo en una colección de columnas binarias.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función onehotbatch del paquete OneHotArrays para recodificar una variable categórica en una colección de columnas binarias.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing OneHotArrays\ncodificacion = permutedims(onehotbatch(df.grupo, unique(df.grupo)))\nhcat(df, DataFrame(codificacion, :auto))\n\n120×12 DataFrame95 rows omitted\n\n\n\nRow\nsexo\nturno\ngrupo\ntrabaja\nnotaA\nnotaB\nnotaC\nnotaD\nnotaE\nx1\nx2\nx3\n\n\n\nString7\nString7\nString1\nString1\nFloat64\nFloat64?\nFloat64?\nFloat64?\nFloat64?\nBool\nBool\nBool\n\n\n\n\n1\nMujer\nTarde\nC\nN\n5.2\n6.3\n3.4\n2.3\n2.0\ntrue\nfalse\nfalse\n\n\n2\nHombre\nMañana\nA\nN\n5.7\n5.7\n4.2\n3.5\n2.7\nfalse\ntrue\nfalse\n\n\n3\nHombre\nMañana\nB\nN\n8.3\n8.8\n8.8\n8.0\n5.5\nfalse\nfalse\ntrue\n\n\n4\nHombre\nMañana\nB\nN\n6.1\n6.8\n4.0\n3.5\n2.2\nfalse\nfalse\ntrue\n\n\n5\nHombre\nMañana\nA\nN\n6.2\n9.0\n5.0\n4.4\n3.7\nfalse\ntrue\nfalse\n\n\n6\nHombre\nMañana\nA\nS\n8.6\n8.9\n9.5\n8.4\n3.9\nfalse\ntrue\nfalse\n\n\n7\nMujer\nMañana\nA\nN\n6.7\n7.9\n5.6\n4.8\n4.2\nfalse\ntrue\nfalse\n\n\n8\nMujer\nTarde\nC\nS\n4.1\n5.2\n1.7\n0.3\n1.0\ntrue\nfalse\nfalse\n\n\n9\nHombre\nTarde\nC\nN\n5.0\n5.0\n3.3\n2.7\n6.0\ntrue\nfalse\nfalse\n\n\n10\nHombre\nTarde\nC\nN\n5.3\n6.3\n4.8\n3.6\n2.3\ntrue\nfalse\nfalse\n\n\n11\nMujer\nMañana\nA\nN\n7.8\nmissing\n6.5\n6.7\n2.8\nfalse\ntrue\nfalse\n\n\n12\nHombre\nMañana\nA\nN\n6.5\n8.0\n5.0\n3.2\n3.3\nfalse\ntrue\nfalse\n\n\n13\nHombre\nMañana\nB\nN\n6.6\n7.6\n5.3\n4.0\n1.0\nfalse\nfalse\ntrue\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n109\nMujer\nTarde\nC\nN\n5.4\n7.3\n3.5\n2.5\n4.6\ntrue\nfalse\nfalse\n\n\n110\nHombre\nMañana\nB\nN\n7.4\n7.4\n6.2\n5.8\n1.1\nfalse\nfalse\ntrue\n\n\n111\nMujer\nTarde\nC\nS\n5.1\n8.1\n5.2\n5.1\n4.5\ntrue\nfalse\nfalse\n\n\n112\nHombre\nMañana\nA\nN\n6.9\n7.8\n3.9\n2.8\nmissing\nfalse\ntrue\nfalse\n\n\n113\nHombre\nTarde\nC\nN\n3.6\n4.8\n2.1\n0.5\n5.6\ntrue\nfalse\nfalse\n\n\n114\nHombre\nTarde\nC\nS\n5.9\n6.2\n5.0\n3.9\n1.9\ntrue\nfalse\nfalse\n\n\n115\nHombre\nMañana\nB\nN\n6.8\n7.2\n4.9\n3.8\n2.8\nfalse\nfalse\ntrue\n\n\n116\nHombre\nMañana\nA\nN\n6.5\n6.1\n5.8\n4.9\n1.2\nfalse\ntrue\nfalse\n\n\n117\nMujer\nMañana\nB\nN\n6.2\n7.0\n5.6\n5.4\n1.7\nfalse\nfalse\ntrue\n\n\n118\nMujer\nTarde\nC\nN\n5.0\n6.5\n4.0\n2.8\n3.6\ntrue\nfalse\nfalse\n\n\n119\nHombre\nTarde\nC\nN\n4.7\n6.0\n1.3\n0.4\n2.2\ntrue\nfalse\nfalse\n\n\n120\nHombre\nTarde\nC\nS\n4.5\n4.7\n6.0\n4.9\n1.8\ntrue\nfalse\nfalse",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "02-preprocesamiento.html#ejercicios-propuestos",
    "href": "02-preprocesamiento.html#ejercicios-propuestos",
    "title": "2  Preprocesamiento de datos",
    "section": "2.2 Ejercicios propuestos",
    "text": "2.2 Ejercicios propuestos\n\nEjercicio 2.4 El fichero vinos.csv contiene información sobre las características de una muestra de vinos portugueses de la denominación “Vinho Verde”. Las variables que contiene son:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nCategórica (blanco, tinto)\n\n\nmeses.barrica\nMesesde envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidadde ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcarremanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufreen formalibre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidadde dióxido de azufretotal en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidadde sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentajede contenidode alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada porun panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un data frame con los datos de los vinos a partir del fichero vinos.csv.\nObtener el número de valores perdidos en cada columna.\nImputar los valores perdidos del alcohol con la media de los valores no perdidos para cada tipo de vino.\nCrear la variable categórica Envejecimiento recodificando la variable meses.barrica en las siguientes categorías.\n\n\n\nRango en meses\nCategoría\n\n\n\n\nMenos de 3\nJoven\n\n\nEntre 3 y 12\nCrianza\n\n\nEntre 12 y 18\nReserva\n\n\nMás de 18\nGran reserva\n\n\n\nCrear la variable categórica Dulzor recodificando la variable azucar.residual en las siguientes categorías.\n\n\n\nRango azúcar\nCategoría\n\n\n\n\nMenos de 4\nSeco\n\n\nMás de 4 y menos de 12\nSemiseco\n\n\nMás de 12 y menos de 45\nSemidulce\n\n\nMás de 45\nDulce\n\n\n\nFiltrar el conjunto de datos para quedarse con los vinos Reserva o Gran Reserva con una calidad superior a 7 y ordenar el data frame por calidad de forma descendente.\n¿Cuántos vinos blancos con un contenido en alcohol superior al 12% y una calidad superior a 8 hay en el conjunto de datos?\n¿Cuáles son los 10 mejores vinos tintos crianza secos?",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Preprocesamiento de datos</span>"
    ]
  },
  {
    "objectID": "03-regresion.html",
    "href": "03-regresion.html",
    "title": "3  Regresión",
    "section": "",
    "text": "3.1 Ejercicios Resueltos\nLos modelos de aprendizaje basados en regresión son modelos bastante simples que pueden utilizarse para predecir variables cuantitativas (regresión lineal) o cualitativas (regresión logística). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje de regresión lineal y regresión logística con Julia.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "03-regresion.html#ejercicios-resueltos",
    "href": "03-regresion.html#ejercicios-resueltos",
    "title": "3  Regresión",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de gráficas.\nusing GLMakie  # Para obtener gráficos interactivos.\n\nEjercicio 3.1 El conjunto de datos viviendas.csv contiene información sobre el precio de venta de viviendas en una ciudad.\n\nCargar los datos del archivo viviendas.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/viviendas.csv\"), DataFrame)\nfirst(df, 5)\n\n5×13 DataFrame\n\n\n\nRow\nprecio\narea\ndormitorios\nbaños\nhabitaciones\ncalleprincipal\nhuespedes\nsotano\ncalentador\nclimatizacion\ngaraje\ncentrico\namueblado\n\n\n\nInt64\nInt64\nInt64\nInt64\nInt64\nString3\nString3\nString3\nString3\nString3\nInt64\nString3\nString15\n\n\n\n\n1\n13300000\n7420\n4\n2\n3\nsi\nno\nno\nno\nsi\n2\nsi\namueblado\n\n\n2\n12250000\n8960\n4\n4\n4\nsi\nno\nno\nno\nsi\n3\nno\namueblado\n\n\n3\n12250000\n9960\n3\n2\n2\nsi\nno\nsi\nno\nno\n2\nsi\nsemi-amueblado\n\n\n4\n12215000\n7500\n4\n2\n2\nsi\nno\nsi\nno\nsi\n3\nsi\namueblado\n\n\n5\n11410000\n7420\n4\n1\n2\nsi\nsi\nsi\nno\nsi\n2\nno\namueblado\n\n\n\n\n\n\n\n\n\nDibujar un diagrama de dispersión entre el precio y el area de las viviendas.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing Plots\nplt = scatter(df.area, df.precio, xlabel=\"Area\", ylabel=\"Precio\", title=\"Precio vs Area\", label = \"Ejemplos\", fmt=:png,)\n\n\n\n\n\n\n\nDefinir un modelo lineal que explique el precio en función del área de las viviendas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUn modelo lineal tiene encuación \\(y = \\theta_1 + \\theta_2 x\\).\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nprecio(area, θ) = θ[1] .+ θ[2] * area\n\nprecio (generic function with 1 method)\n\n\nObserva que la función precio está vectorizada, lo que significa que puede recibir un vector de áreas y devolver un vector de precios.\n\n\n\nInicializar los parámetros del modelo lineal con valores nulos y dibujar el modelo sobre el diagrama de dispersión.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nθ = [0.0, 0.0]\nplot!(df.area, precio(df.area, θ), label = \"Modelo 0\")\n\n\n\n\n\n\n\nDefinir una función de costo para el modelo lineal y evaluar el coste para el modelo lineal construido con los parámetros iniciales. A la vista del coste obtenido, ¿cómo de bueno es el modelo?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLa función de coste para un modelo lineal es el error cuadrático medio.\n\\[ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2 \\]\ndonde \\(h_\\theta\\) es el modelo, \\(h_\\theta(x^{(i)})\\) es la predicción del modelo para el ejemplo \\(i\\)-ésimo, \\(y^{(i)}\\) es el valor real observado para el ejemplo \\(i\\)-ésimo, y \\(m\\) es el número de ejemplos.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nfunction coste(θ, X, Y)\n    m = length(Y)\n    return sum((precio(X, θ) .- Y).^2) / (2 * m)\nend\n\ncoste(θ, df.area, df.precio)\n\n1.3106916364659266e13\n\n\nLa función de coste nos da una medida de lo lejos que están las predicciones del modelo de los valores reales observados. En este caso, el coste es muy alto, lo que indica que el modelo no es bueno.\n\n\n\n¿En qué dirección debemos modificar los parámetros del modelo para mejorar el modelo?\n\n\n\n\n\n\nSolución\n\n\n\n\n\nPara minimizar la función de coste, debemos modificar los parámetros del modelo en la dirección opuesta al gradiente de la función de coste, ya que el gradiente de una función indica la dirección de mayor crecimiento de la función.\n\n\n\nCrear una función para modificar los pesos del modelo lineal mediante el algoritmo del gradiente descendente, y aplicarla a los parámetros actuales tomando una tasa de aprendizaje de \\(10^{-8}\\). ¿Cómo han cambiado los parámetros del modelo? Dibujar el modelo actualizado sobre el diagrama de dispersión. ¿Cómo ha cambiado el coste?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEl algoritmo del gradiente descendente actualiza los parámetros del modelo de acuerdo a la siguiente regla:\n\\[\n\\theta_j = \\theta_j - \\alpha \\frac{\\partial J(\\theta)}{\\partial \\theta_j}\n\\]\ndonde \\(\\alpha\\) es la tasa de aprendizaje y \\(\\frac{\\partial J(\\theta)}{\\partial \\theta_j}\\) es la derivada parcial de la función de coste con respecto al parámetro \\(\\theta_j\\).\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nfunction gradiente_descendente!(θ, X, Y, α)\n    # Calculamos el número de ejemplos\n    m = length(Y)\n    # Actualizamos el término independiente del modelo lineal.\n    θ[1] -= α * sum(precio(X, θ) - Y) / m\n    # Actualizamos la pendiente del modelo lineal.\n    θ[2] -= α * sum((precio(X, θ) - Y) .* X) / m\n    return θ\nend\n\ngradiente_descendente! (generic function with 1 method)\n\n\nAplicamos la función a los parámetros del modelo actual y mostramos los nuevos parámetros.\n\ngradiente_descendente!(θ, df.area, df.precio, 1e-8)\nθ\n\n2-element Vector{Float64}:\n   0.04766729247706422\n 267.22919804579385\n\n\nDibujamos el nuevo modelo.\n\nplot!(df.area, precio(df.area, θ), label = \"Modelo 1\")\n\n\n\n\nSe observa que ahora la recta está más cerca de la nube de puntos, por lo que el modelo ha mejorado. Calculamos el coste del nuevo modelo.\n\ncoste(θ, df.area, df.precio)\n\n7.080823787113201e12\n\n\n\n\n\nRepetir el proceso de actualización de los parámetros del modelo mediante el algoritmo del gradiente descendente durante 9 iteraciones más y dibujar los modelos actualizados.\n\n\n\n\n\n\nSolución\n\n\n\n\n\nfor i = 2:10\n    gradiente_descendente!(θ, df.area, df.precio, 1e-8)\n    plot!(df.area, precio(df.area, θ), label = \"Modelo $i\", legend = true)\nend\nplt\n\n\n\n\nDibujar un gráfico con la evolución del coste del modelo a lo largo de las iteraciones. ¿Cómo se comporta el coste a lo largo de las iteraciones?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ncostes = Float64[]\nfor i = 1:10\n    gradiente_descendente!(θ, df.area, df.precio, 1e-8)\n    push!(costes, coste(θ, df.area, df.precio))\nend\ncostes\n\n10-element Vector{Float64}:\n 4.230808760870044e12\n 2.882906194020343e12\n 2.2454213686913755e12\n 1.9439256128790886e12\n 1.8013344680594421e12\n 1.7338965877160208e12\n 1.7020021263374993e12\n 1.6869177748236997e12\n 1.6797836937723748e12\n 1.6764096595632322e12\n\n\nEl coste del modelo disminuye en cada iteración, lo que indica que el modelo está mejorando. Esto se debe a que el algoritmo del gradiente descendente modifica los parámetros del modelo en la dirección que minimiza la función de coste.\n\n\n\n¿Hasta qué iteración habrá que llegar para conseguir un reducción del coste menor de un \\(0.0001\\%\\)?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nθ = [0.0, 0.0]\ncostes = [0, coste(θ, df.area, df.precio)]\ni = 1\nwhile abs(costes[end] - costes[end-1]) / costes[end-1] &gt; 0.000001\n    i += 1\n    gradiente_descendente!(θ, df.area, df.precio, 1e-8)\n    push!(costes, coste(θ, df.area, df.precio))\nend\ni\n\n23\n\n\nEn este caso, el algoritmo del gradiente descendente converge en 1000 iteraciones.\n\n\n\n¿Qué sucede si se utiliza una tasa de aprendizaje \\(\\alpha = 0.0001\\)? ¿Cómo afecta al coste y a la convergencia del modelo?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nθ = [0.0, 0.0]\ncostes = [coste(θ, df.area, df.precio)]\nfor i = 1:10\n    gradiente_descendente!(θ, df.area, df.precio, 0.0001)\n    push!(costes, coste(θ, df.area, df.precio))\nend\ncostes\n\n11-element Vector{Float64}:\n 1.3106916364659266e13\n 1.114133369099188e20\n 1.0856750832581238e27\n 1.05794371802143e34\n 1.0309206941949286e41\n 1.004587918634273e48\n 9.789277603492545e54\n 9.539230386975057e61\n 9.29557011881276e68\n 9.058133657380397e75\n 8.826762028174244e82\n\n\nSi la tasa de aprendizaje es demasiado grande, el algoritmo del gradiente descendente puede no converger y el coste puede oscilar en lugar de disminuir. En este caso, el coste aumenta en cada iteración, lo que indica que la tasa de aprendizaje es demasiado grande.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regresión</span>"
    ]
  },
  {
    "objectID": "07-arboles-decision.html",
    "href": "07-arboles-decision.html",
    "title": "4  Árboles de decisión",
    "section": "",
    "text": "4.1 Ejercicios Resueltos\nLos árboles de decisión son modelos de aprendizaje simples e intuitivos que pueden utilizarse para tanto para predecir variables cuantitativas (regresión) como categóricas (clasificación). Esta práctica contiene ejercicios que muestran como construir modelos de aprendizaje basados en árboles de decisión con Julia.\nPara la realización de esta práctica se requieren los siguientes paquetes:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Árboles de decisión</span>"
    ]
  },
  {
    "objectID": "07-arboles-decision.html#ejercicios-resueltos",
    "href": "07-arboles-decision.html#ejercicios-resueltos",
    "title": "4  Árboles de decisión",
    "section": "",
    "text": "using CSV  # Para la lectura de archivos CSV.\nusing DataFrames  # Para el manejo de datos tabulares.\nusing Tidier # Para el preprocesamiento de datos.\nusing PrettyTables  # Para mostrar tablas formateadas.\nusing Plots  # Para el dibujo de gráficas.\nusing GLMakie  # Para obtener gráficos interactivos.\nusing AlgebraOfGraphics # Para generar gráficos mediante la gramática de gráficos.\nusing DecisionTree # Para construir árboles de decisión.\nusing GraphMakie # Para la visualización de árboles de decisión.\n\nEjercicio 4.1 El conjunto de datos tenis.csv contiene información sobre las condiciones meteorológicas de varios días y si se pudo jugar al tenis o no.\n\nCargar los datos del archivo tenis.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/tenis.csv\"), DataFrame)\n\n14×5 DataFrame\n\n\n\nRow\nCielo\nTemperatura\nHumedad\nViento\nTenis\n\n\n\nString15\nString15\nString7\nString7\nString3\n\n\n\n\n1\nSoleado\nCaluroso\nAlta\nSuave\nNo\n\n\n2\nSoleado\nCaluroso\nAlta\nFuerte\nNo\n\n\n3\nNublado\nCaluroso\nAlta\nSuave\nSí\n\n\n4\nLluvioso\nModerado\nAlta\nSuave\nSí\n\n\n5\nLluvioso\nFrío\nNormal\nSuave\nSí\n\n\n6\nLluvioso\nFrío\nNormal\nFuerte\nNo\n\n\n7\nNublado\nFrío\nNormal\nFuerte\nSí\n\n\n8\nSoleado\nModerado\nAlta\nSuave\nNo\n\n\n9\nSoleado\nFrío\nNormal\nSuave\nSí\n\n\n10\nLluvioso\nModerado\nNormal\nSuave\nSí\n\n\n11\nSoleado\nModerado\nNormal\nFuerte\nSí\n\n\n12\nNublado\nModerado\nAlta\nFuerte\nSí\n\n\n13\nNublado\nCaluroso\nNormal\nSuave\nSí\n\n\n14\nLluvioso\nModerado\nAlta\nFuerte\nNo\n\n\n\n\n\n\n\n\n\nCrear un diagrama de barras que muestre la distribución de frecuencias de cada variable meteorológica según si se pudo jugar al tenis o no. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing GLMakie, AlgebraOfGraphics\n\nfunction frecuencias(df::DataFrame, var::Symbol)\n    # Calculamos el número de días de cada clase que se juega al tenis.\n    frec = combine(groupby(df, [var, :Tenis]), nrow =&gt; :Días)\n    # Dibujamos el diagrama de barras.\n    plt = data(frec) * \n    mapping(var, :Días, stack = :Tenis, color = :Tenis, ) * \n    visual(BarPlot) \n    # Devolvemos el gráfico.\n    return plt\nend\n\nfig = Figure()\ndraw!(fig[1, 1], frecuencias(df, :Cielo))\ndraw!(fig[1, 2], frecuencias(df, :Temperatura))\ndraw!(fig[1, 3], frecuencias(df, :Humedad))\ndraw!(fig[1, 4], frecuencias(df, :Viento))\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nA la vista de las frecuencias de cada variable, las variable Cielo y Humedad parecen ser las que más influye en la decisión de jugar al tenis.\n\n\n\nCalcular la impureza del conjunto de datos utilizando el índice de Gini. ¿Qué variable meteorológica parece tener más influencia en la decisión de jugar al tenis?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nEl índice de Gini se calcula mediante la fórmula\n\\[ GI = 1 - \\sum_{i=1}^{n} p_i^2 \\]\ndonde \\(p_i\\) es la proporción de cada clase en el conjunto de datos y \\(n\\) es el número de clases.\nEl índice de Gini toma valores entre \\(0\\) y \\(1-\\frac{1}{n}\\) (\\(0.5\\) en el caso de clasificación binaria), donde \\(0\\) indica que todas las instancias pertenecen a una sola clase (mínima impureza) y \\(1-\\frac{1}{n}\\) indica que las instancias están distribuidas uniformemente entre todas las clases (máxima impureza).\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nfunction gini(df::DataFrame, var::Symbol)\n    # Calculamos el número de ejemplos.\n    n = nrow(df)\n    # Calculamos las frecuencias absolutas de cada clase.\n    frec = combine(groupby(df, var), nrow =&gt; :ni)\n    # Calculamos la proporción de cada clase.\n    frec.p = frec.ni ./ n\n    # Calculamos el índice de Gini.\n    gini = 1 - sum(frec.p .^ 2)\n    return gini\nend\n\ng0 = gini(df, :Tenis)\n\n0.4591836734693877\n\n\n\n\n\n¿Qué reducción del índice Gini se obtiene si dividimos el conjunto de ejemplos según la variable Humedad? ¿Y si dividimos el conjunto con respecto a la variable Viento?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLa reducción del índice de Gini se calcula como la diferencia entre el índice de Gini del conjunto original y el índice de Gini del conjunto dividido.\n\\[ \\Delta GI = GI_{original} - GI_{dividido} \\]\ndonde el índice de Gini del conjunto dividido es la media ponderada de los índices de Gini de los subconjuntos resultantes de la división.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\nCalculamos primero la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable Humedad.\n\nusing Tidier\n# Dividimos el conjunto de ejemplos según la variable Humedad.\ndf_humedad_alta = @filter(df, Humedad == \"Alta\")\ndf_humedad_normal = @filter(df, Humedad == \"Normal\")\n# Calculamos los tamaños de los subconjuntos de ejemplos.\nn = nrow(df_humedad_alta), nrow(df_humedad_normal)\n# Calculamos el índice de Gini de cada subconjunto.\ngis = gini(df_humedad_alta, :Tenis), gini(df_humedad_normal, :Tenis)\n# Calculamos media ponderada de los índices de Gini de los subconjuntos \ng_humedad = sum(gis .* n) / sum(n)\n# Calculamos la reducción del índice de Gini.\ng0 - g_humedad\n\n0.09183673469387743\n\n\nCalculamos ahora la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable Viento.\n\n# Dividimos el conjunto de ejemplos según la variable `Viento`\ndf_viento_fuerte = @filter(df, Viento == \"Fuerte\")\ndf_viento_suave = @filter(df, Viento == \"Suave\")\n# Calculamos los tamaños de los subconjuntos de ejemplos\nn = nrow(df_viento_fuerte), nrow(df_viento_suave)\n# Calculamos el índice de Gini de cada subconjunto\ngis = gini(df_viento_fuerte, :Tenis), gini(df_viento_suave, :Tenis)\n# Calculamos media ponderada de los índices de Gini de los subconjuntos\ng_viento = sum(gis .* n) / sum(n)\n# Calculamos la reducción del índice de Gini\ng0 - g_viento\n\n0.030612244897959162\n\n\nComo se puede observar, la reducción del índice de Gini al dividir el conjunto de ejemplos según la variable Humedad es mayor que la reducción del índice de Gini al dividir el conjunto con respecto a la variable Viento. Por lo tanto, la variable Humedad parece tener más influencia en la decisión de jugar al tenis y sería la variable que se debería elegir para dividir el conjunto de ejemplos.\n\n\n\nConstruir un árbol de decisión que explique si se puede jugar al tenis en función de las variables meteorológicas.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función DecisionTreeClassifier del paquete DecisionTree.jl.\nLos parámetros más importantes de esta función son:\n\nmax_depth: Profundidad máxima del árbol. Si no se indica, el árbol crecerá hasta que todas las hojas sean puras o hasta que todas las hojas contengan menos de min_samples_split ejemplos.\nmin_samples_leaf: Número mínimo de ejemplos en una hoja (1 por defecto).\nmin_samples_split: Número mínimo de ejemplos para dividir un nodo (2 por defecto).\nmin_impurity_decrease: Reducción mínima de la impureza para dividir un nodo (0 por defecto).\npost-prune: Si se indica true, se poda el árbol después de que se ha construido. La poda reduce el tamaño del árbol eliminando nodos que no aportan información útil.\nmerge_purity_threshold: Umbral de pureza para fusionar nodos. Si se indica, se fusionan los nodos que tienen una pureza menor que este umbral.\nfeature_importance: Indica la medida para calcular la importancia de las variables a la hora de dividir el conjunto de datos. Puede ser :impurity o :split. Si no se indica, se utiliza la impureza de Gini.\nrng: Indica la semilla para la generación de números aleatorios. Si no se indica, se utiliza el generador de números aleatorios por defecto.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing DecisionTree, CategoricalArrays\n# Variables predictoras.\nX = Matrix(select(df, Not(:Tenis)))\n# Variable objetivo.\ny = df.Tenis\n# Convertir las variables categóricas a enteros.\nX = hcat([levelcode.(categorical(X[:, j])) for j in 1:size(X, 2)]...)\n# Convertir la variable objetivo a enteros.\ny = levelcode.(categorical(y))\ntree = DecisionTreeClassifier(max_depth=3)\nfit!(tree, X, y)\n\nDecisionTreeClassifier\nmax_depth:                3\nmin_samples_leaf:         1\nmin_samples_split:        2\nmin_purity_increase:      0.0\npruning_purity_threshold: 1.0\nn_subfeatures:            0\nclasses:                  [1, 2]\nroot:                     Decision Tree\nLeaves: 6\nDepth:  3\n\n\n\n\n\nVisualizar el árbol de decisión construido.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función plot_tree del paquete DecisionTree.jl.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nprint_tree(tree, feature_names=names(df)[1:end-1])\n\nFeature 3: \"Humedad\" &lt; 2.0 ?\n├─ Feature 1: \"Cielo\" &lt; 3.0 ?\n    ├─ Feature 4: \"Viento\" &lt; 2.0 ?\n        ├─ 2 : 1/2\n        └─ 2 : 2/2\n    └─ 1 : 3/3\n└─ Feature 4: \"Viento\" &lt; 2.0 ?\n    ├─ Feature 1: \"Cielo\" &lt; 2.0 ?\n        ├─ 1 : 1/1\n        └─ 2 : 2/2\n    └─ 2 : 4/4\n\n\n\n\n\n\n\n\nEjercicio 4.2 El conjunto de datos pingüinos.csv contiene un conjunto de datos sobre tres eEspecie de pingüinos con las siguientes variables:\n\nEspecie: Especie de pingüino, comúnmente Adelie, Chinstrap o Gentoo.\nIsla: Isla del archipiélago Palmer donde se realizó la observación.\nLongitud_pico: Longitud del pico en mm.\nProfundidad_pico: Profundidad del pico en mm\nLongitud_ala: Longitud de la aleta en mm.\nPeso: Masa corporal en gramos.\nSexo: Sexo\n\n\nCargar los datos del archivo pinguïnos.csv en un data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/pingüinos.csv\"), DataFrame, missingstring=\"NA\")\n\n344×7 DataFrame319 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64?\nFloat64?\nInt64?\nInt64?\nString7?\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmacho\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nhembra\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nhembra\n\n\n4\nAdelie\nTorgersen\nmissing\nmissing\nmissing\nmissing\nmissing\n\n\n5\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nhembra\n\n\n6\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmacho\n\n\n7\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nhembra\n\n\n8\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmacho\n\n\n9\nAdelie\nTorgersen\n34.1\n18.1\n193\n3475\nmissing\n\n\n10\nAdelie\nTorgersen\n42.0\n20.2\n190\n4250\nmissing\n\n\n11\nAdelie\nTorgersen\n37.8\n17.1\n186\n3300\nmissing\n\n\n12\nAdelie\nTorgersen\n37.8\n17.3\n180\n3700\nmissing\n\n\n13\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nhembra\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n333\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nhembra\n\n\n334\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmacho\n\n\n335\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmacho\n\n\n336\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nhembra\n\n\n337\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmacho\n\n\n338\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nhembra\n\n\n339\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nhembra\n\n\n340\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n341\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n342\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmacho\n\n\n343\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmacho\n\n\n344\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nhembra\n\n\n\n\n\n\n\n\n\nHacer un análisis de los datos perdidos en el data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndescribe(df, :nmissing)\n\n7×2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\nEspecie\n0\n\n\n2\nIsla\n0\n\n\n3\nLongitud_pico\n2\n\n\n4\nProfundidad_pico\n2\n\n\n5\nLongitud_ala\n2\n\n\n6\nPeso\n2\n\n\n7\nSexo\n11\n\n\n\n\n\n\n\n\n\nEliminar del data frame los casos con valores perdidos.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndropmissing!(df)\n\n333×7 DataFrame308 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64\nFloat64\nInt64\nInt64\nString7\n\n\n\n\n1\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmacho\n\n\n2\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nhembra\n\n\n3\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nhembra\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nhembra\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmacho\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181\n3625\nhembra\n\n\n7\nAdelie\nTorgersen\n39.2\n19.6\n195\n4675\nmacho\n\n\n8\nAdelie\nTorgersen\n41.1\n17.6\n182\n3200\nhembra\n\n\n9\nAdelie\nTorgersen\n38.6\n21.2\n191\n3800\nmacho\n\n\n10\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmacho\n\n\n11\nAdelie\nTorgersen\n36.6\n17.8\n185\n3700\nhembra\n\n\n12\nAdelie\nTorgersen\n38.7\n19.0\n195\n3450\nhembra\n\n\n13\nAdelie\nTorgersen\n42.5\n20.7\n197\n4500\nmacho\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n322\nChinstrap\nDream\n45.2\n16.6\n191\n3250\nhembra\n\n\n323\nChinstrap\nDream\n49.3\n19.9\n203\n4050\nmacho\n\n\n324\nChinstrap\nDream\n50.2\n18.8\n202\n3800\nmacho\n\n\n325\nChinstrap\nDream\n45.6\n19.4\n194\n3525\nhembra\n\n\n326\nChinstrap\nDream\n51.9\n19.5\n206\n3950\nmacho\n\n\n327\nChinstrap\nDream\n46.8\n16.5\n189\n3650\nhembra\n\n\n328\nChinstrap\nDream\n45.7\n17.0\n195\n3650\nhembra\n\n\n329\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n330\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n331\nChinstrap\nDream\n49.6\n18.2\n193\n3775\nmacho\n\n\n332\nChinstrap\nDream\n50.8\n19.0\n210\n4100\nmacho\n\n\n333\nChinstrap\nDream\n50.2\n18.7\n198\n3775\nhembra\n\n\n\n\n\n\n\n\n\nCrear diagramas que muestren la distribución de frecuencias de cada variable según la especie de pingüino. ¿Qué variable parece tener más influencia en la especie de pingüino?\n\n\n\n\n\n\nSolución\n\n\n\n\n\nPara las variables cualitativas dibujamos diagramas de barras.\n\nusing GLMakie, AlgebraOfGraphics\n\nfrec_isla = combine(groupby(df, [:Isla, :Especie]), nrow =&gt; :Frecuencia)\ndata(frec_isla) * \n    mapping(:Isla, :Frecuencia, stack = :Especie, color =:Especie) *\n    visual(BarPlot) |&gt; draw\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\nfrec_sexo = combine(groupby(df, [:Sexo, :Especie]), nrow =&gt; :Frecuencia)\ndata(frec_sexo) * \n    mapping(:Sexo, :Frecuencia, stack = :Especie, color =:Especie) *\n    visual(BarPlot) |&gt; draw\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\nPara las variables cuantitativas dibujamos diagramas de cajas.\n\nfunction cajas(df, var, clase)\n    data(df) *\n        mapping(clase, var, color = clase) *\n        visual(BoxPlot) |&gt; \n        draw\nend\n\ncajas(df, :Longitud_pico, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Profundidad_pico, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Longitud_ala, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\ncajas(df, :Peso, :Especie)\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cuál es la reducción de la impureza del conjunto de datos si dividimos el conjunto de datos en dos conjuntos según si la longitud del pico es mayor o menor que 44 mm?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing Tidier\nfunction gini(df::DataFrame, var::Symbol)\n    n = nrow(df)\n    frec = combine(groupby(df, var), nrow =&gt; :ni)\n    frec.p = frec.ni ./ n\n    gini = 1 - sum(frec.p .^ 2)\n    return gini\nend\n\nfunction reduccion_impureza(df::DataFrame, var::Symbol, val::Number)\n    # Dividimos el conjunto de ejemplos según la longitud del pico es menor de 44.\n    df_menor = @eval @filter($df, $var &lt;= $val)\n    df_mayor = @eval @filter($df, $var &gt; $val)\n    # Calculamos los tamaños de los subconjuntos de ejemplos.\n    n = nrow(df_menor), nrow(df_mayor)\n    # Calculamos el índice de Gini de cada subconjunto.\n    gis = gini(df_menor, :Especie), gini(df_mayor, :Especie)\n    # Calculamos media ponderada de los índices de Gini de los subconjuntos.\n    g1 = sum(gis .* n) / sum(n)\n    # Calculamos la reducción del índice de Gini.\n    gini(df, :Especie) - g1\nend\n\nreduccion_impureza(df, :Longitud_pico, 44)\n\n0.26577182779353914\n\n\n\n\n\nDeterminar el valor óptimo de división del conjunto de datos según la longitud del pico. Para ello, calcular la reducción de la impureza para cada valor de longitud del pico y dibujar el resultado.\n\n\n\n\n\n\nSolución\n\n\n\n\n\nDibujamos la reducción de la impureza en función de la longitud del pico.\n\nusing Plots\n# Valores únicos de longitud del pico.\nvalores = unique(df.Longitud_pico)\n# Reducción de la impureza para cada valor.\nreducciones = [reduccion_impureza(df, :Longitud_pico, val) for val in valores]\n# Graficamos el resultado.\nPlots.scatter(valores, reducciones, xlabel = \"Longitud del pico\", ylabel = \"Reducción de la impureza\", legend = false)\n\n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nY ahora obtenemos el valor óptimo de división del conjunto de datos según la longitud del pico.\n\nval_optimo = valores[argmax(reducciones)]\n\n42.3\n\n\n\n\n\nDividir aleatoriamente el dataframe en un conjunto de entrenamiento y un conjunto de test con proporciones \\(3/4\\) y \\(1/4\\) respectivamente.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función shuffle del paquete Random para barajar el dataframe y luego dividirlo en dos subconjuntos.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing Random\n# Establecemos la semilla para la reproducibilidad.\nRandom.seed!(1234)\n# Barajamos el dataframe.\ndf = shuffle(df)\n# Dividimos el dataframe en un conjunto de entrenamiento y un conjunto de test.\nn = nrow(df)\ndf_test = df[1:div(n, 4), :]\ndf_train = df[div(n, 4)+1:end, :]\n\n250×7 DataFrame225 rows omitted\n\n\n\nRow\nEspecie\nIsla\nLongitud_pico\nProfundidad_pico\nLongitud_ala\nPeso\nSexo\n\n\n\nString15\nString15\nFloat64\nFloat64\nInt64\nInt64\nString7\n\n\n\n\n1\nAdelie\nDream\n39.0\n18.7\n185\n3650\nmacho\n\n\n2\nChinstrap\nDream\n52.8\n20.0\n205\n4550\nmacho\n\n\n3\nChinstrap\nDream\n55.8\n19.8\n207\n4000\nmacho\n\n\n4\nAdelie\nTorgersen\n35.1\n19.4\n193\n4200\nmacho\n\n\n5\nAdelie\nTorgersen\n34.6\n21.1\n198\n4400\nmacho\n\n\n6\nGentoo\nBiscoe\n50.0\n15.2\n218\n5700\nmacho\n\n\n7\nChinstrap\nDream\n50.6\n19.4\n193\n3800\nmacho\n\n\n8\nChinstrap\nDream\n43.5\n18.1\n202\n3400\nhembra\n\n\n9\nAdelie\nDream\n36.9\n18.6\n189\n3500\nhembra\n\n\n10\nAdelie\nDream\n36.6\n18.4\n184\n3475\nhembra\n\n\n11\nChinstrap\nDream\n46.6\n17.8\n193\n3800\nhembra\n\n\n12\nGentoo\nBiscoe\n50.8\n17.3\n228\n5600\nmacho\n\n\n13\nChinstrap\nDream\n52.2\n18.8\n197\n3450\nmacho\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n239\nAdelie\nDream\n39.8\n19.1\n184\n4650\nmacho\n\n\n240\nAdelie\nTorgersen\n43.1\n19.2\n197\n3500\nmacho\n\n\n241\nChinstrap\nDream\n49.8\n17.3\n198\n3675\nhembra\n\n\n242\nGentoo\nBiscoe\n49.8\n15.9\n229\n5950\nmacho\n\n\n243\nChinstrap\nDream\n50.8\n18.5\n201\n4450\nmacho\n\n\n244\nGentoo\nBiscoe\n50.7\n15.0\n223\n5550\nmacho\n\n\n245\nGentoo\nBiscoe\n46.2\n14.1\n217\n4375\nhembra\n\n\n246\nAdelie\nTorgersen\n35.5\n17.5\n190\n3700\nhembra\n\n\n247\nAdelie\nBiscoe\n39.7\n18.9\n184\n3550\nmacho\n\n\n248\nGentoo\nBiscoe\n47.7\n15.0\n216\n4750\nhembra\n\n\n249\nAdelie\nTorgersen\n42.9\n17.6\n196\n4700\nmacho\n\n\n250\nAdelie\nDream\n40.8\n18.9\n208\n4300\nmacho\n\n\n\n\n\n\n\n\n\nConstruir un árbol de decisión con el conjunto de entrenamiento sin tener en cuenta la variable Isla y visualizarlo.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing DecisionTree, CategoricalArrays\n# Variables predictivas.\nX_train = Matrix(select(df_train, Not(:Isla, :Especie)))\n# Variable objetivo.\ny_train = df_train.Especie\n# Convertir las variables categóricas a enteros.\nX_train = hcat([levelcode.(categorical(X_train[:, j])) for j in 1:size(X_train, 2)]...)\n# Convertir la variable objetivo a enteros\ny_train = levelcode.(categorical(y_train))\n\n# Construimos el árbol de decisión con profundidad máxima 3.\ntree = DecisionTreeClassifier(max_depth = 3)\nfit!(tree, X_train, y_train)\nprint_tree(tree, feature_names=names(df)[3:end])\n\nFeature 3: \"Longitud_ala\" &lt; 29.0 ?\n├─ Feature 1: \"Longitud_pico\" &lt; 62.0 ?\n    ├─ 1 : 96/96\n    └─ Feature 1: \"Longitud_pico\" &lt; 87.0 ?\n        ├─ 2 : 10/20\n        └─ 2 : 37/38\n└─ Feature 2: \"Profundidad_pico\" &lt; 46.0 ?\n    ├─ 3 : 90/90\n    └─ Feature 1: \"Longitud_pico\" &lt; 109.0 ?\n        ├─ 1 : 2/2\n        └─ 2 : 4/4\n\n\n\n\n\nPredecir la especie de los pingüinos del conjunto de test y calcular la matriz de confusión de las predicciones.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUtilizar la función confmat del paquete StatisticalMeaures para barajar el dataframe y luego dividirlo en dos subconjuntos.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing StatisticalMeasures\n# Variables predictivas\nX_test = Matrix(select(df_test, Not(:Isla, :Especie)))\n# Variable objetivo\ny_test = df_test.Especie\n# Convertir las variables categóricas a enteros\nX_test = hcat([levelcode.(categorical(X_test[:, j])) for j in 1:size(X_test, 2)]...)\n# Convertir la variable objetivo a enteros\ny_test = levelcode.(categorical(y_test))\n# Predecimos la especie de pingüino del conjunto de test\ny_pred = predict(tree, X_test)\n# Calculamos la precisión del modelo\nconfmat(y_pred, y_test)\n\n          ┌──────────────┐\n          │ Ground Truth │\n┌─────────┼────┬────┬────┤\n│Predicted│ 1  │ 2  │ 3  │\n├─────────┼────┼────┼────┤\n│    1    │ 38 │ 11 │ 9  │\n├─────────┼────┼────┼────┤\n│    2    │ 0  │ 6  │ 0  │\n├─────────┼────┼────┼────┤\n│    3    │ 0  │ 0  │ 19 │\n└─────────┴────┴────┴────┘\n\n\n\n\n\nCalcular la precisión del modelo.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nLa precisión es la proporción de predicciones correctas sobre el total de predicciones.\nUtilizar la función accuracy del paquete StatisticalMeaures para calcular la precisión del modelo.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Calculamos la precisión del modelo\naccuracy(y_pred, y_test)\n\n0.7590361445783133\n\n\n\n\n\n\n\n\nEjercicio 4.3 El fichero vinos.csv contiene información sobre las características de una muestra de vinos portugueses de la denominación “Vinho Verde”. Las variables que contiene son:\n\n\n\n\n\n\n\n\nVariable\nDescripción\nTipo (unidades)\n\n\n\n\ntipo\nTipo de vino\nCategórica (blanco, tinto)\n\n\nmeses.barrica\nMesesde envejecimiento en barrica\nNumérica(meses)\n\n\nacided.fija\nCantidadde ácidotartárico\nNumérica(g/dm3)\n\n\nacided.volatil\nCantidad de ácido acético\nNumérica(g/dm3)\n\n\nacido.citrico\nCantidad de ácidocítrico\nNumérica(g/dm3)\n\n\nazucar.residual\nCantidad de azúcarremanente después de la fermentación\nNumérica(g/dm3)\n\n\ncloruro.sodico\nCantidad de clorurosódico\nNumérica(g/dm3)\n\n\ndioxido.azufre.libre\nCantidad de dióxido de azufreen formalibre\nNumérica(mg/dm3)\n\n\ndioxido.azufre.total\nCantidadde dióxido de azufretotal en forma libre o ligada\nNumérica(mg/dm3)\n\n\ndensidad\nDensidad\nNumérica(g/cm3)\n\n\nph\npH\nNumérica(0-14)\n\n\nsulfatos\nCantidadde sulfato de potasio\nNumérica(g/dm3)\n\n\nalcohol\nPorcentajede contenidode alcohol\nNumérica(0-100)\n\n\ncalidad\nCalificación otorgada porun panel de expertos\nNumérica(0-10)\n\n\n\n\nCrear un data frame con los datos de los vinos a partir del fichero vinos.csv.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CSV, DataFrames\ndf = CSV.read(download(\"https://aprendeconalf.es/aprendizaje-automatico-practicas-julia/datos/vinos.csv\"), DataFrame)\n\n5320×14 DataFrame5295 rows omitted\n\n\n\nRow\ntipo\nmeses_barrica\nacided_fija\nacided_volatil\nacido_citrico\nazucar_residual\ncloruro_sodico\ndioxido_azufre_libre\ndioxido_azufre_total\ndensidad\nph\nsulfatos\nalcohol\ncalidad\n\n\n\nString7\nInt64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nFloat64\nInt64\n\n\n\n\n1\nblanco\n0\n7.0\n0.27\n0.36\n20.7\n0.045\n45.0\n170.0\n1.001\n3.0\n0.45\n8.8\n6\n\n\n2\nblanco\n0\n6.3\n0.3\n0.34\n1.6\n0.049\n14.0\n132.0\n0.994\n3.3\n0.49\n9.5\n6\n\n\n3\nblanco\n0\n8.1\n0.28\n0.4\n6.9\n0.05\n30.0\n97.0\n0.9951\n3.26\n0.44\n10.1\n6\n\n\n4\nblanco\n0\n7.2\n0.23\n0.32\n8.5\n0.058\n47.0\n186.0\n0.9956\n3.19\n0.4\n9.9\n6\n\n\n5\nblanco\n0\n6.2\n0.32\n0.16\n7.0\n0.045\n30.0\n136.0\n0.9949\n3.18\n0.47\n9.6\n6\n\n\n6\nblanco\n0\n8.1\n0.22\n0.43\n1.5\n0.044\n28.0\n129.0\n0.9938\n3.22\n0.45\n11.0\n6\n\n\n7\nblanco\n0\n8.1\n0.27\n0.41\n1.45\n0.033\n11.0\n63.0\n0.9908\n2.99\n0.56\n12.0\n5\n\n\n8\nblanco\n0\n8.6\n0.23\n0.4\n4.2\n0.035\n17.0\n109.0\n0.9947\n3.14\n0.53\n9.7\n5\n\n\n9\nblanco\n0\n7.9\n0.18\n0.37\n1.2\n0.04\n16.0\n75.0\n0.992\n3.18\n0.63\n10.8\n5\n\n\n10\nblanco\n0\n6.6\n0.16\n0.4\n1.5\n0.044\n48.0\n143.0\n0.9912\n3.54\n0.52\n12.4\n7\n\n\n11\nblanco\n0\n8.3\n0.42\n0.62\n19.25\n0.04\n41.0\n172.0\n1.0002\n2.98\n0.67\n9.7\n5\n\n\n12\nblanco\n0\n6.6\n0.17\n0.38\n1.5\n0.032\n28.0\n112.0\n0.9914\n3.25\n0.55\n11.4\n7\n\n\n13\nblanco\n0\n6.3\n0.48\n0.04\n1.1\n0.046\n30.0\n99.0\n0.9928\n3.24\n0.36\n9.6\n6\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n5309\ntinto\n7\n7.5\n0.31\n0.41\n2.4\n0.065\n34.0\n60.0\n0.99492\n3.34\n0.85\n11.4\n6\n\n\n5310\ntinto\n7\n5.8\n0.61\n0.11\n1.8\n0.066\n18.0\n28.0\n0.99483\n3.55\n0.66\n10.9\n6\n\n\n5311\ntinto\n10\n7.2\n0.66\n0.33\n2.5\n0.068\n34.0\n102.0\n0.99414\n3.27\n0.78\n12.8\n6\n\n\n5312\ntinto\n3\n6.6\n0.725\n0.2\n7.8\n0.073\n29.0\n79.0\n0.9977\n3.29\n0.54\n9.2\n5\n\n\n5313\ntinto\n7\n6.3\n0.55\n0.15\n1.8\n0.077\n26.0\n35.0\n0.99314\n3.32\n0.82\n11.6\n6\n\n\n5314\ntinto\n9\n5.4\n0.74\n0.09\n1.7\n0.089\n16.0\n26.0\n0.99402\n3.67\n0.56\n11.6\n6\n\n\n5315\ntinto\n3\n6.3\n0.51\n0.13\n2.3\n0.076\n29.0\n40.0\n0.99574\n3.42\n0.75\n11.0\n6\n\n\n5316\ntinto\n3\n6.8\n0.62\n0.08\n1.9\n0.068\n28.0\n38.0\n0.99651\n3.42\n0.82\n9.5\n6\n\n\n5317\ntinto\n5\n6.2\n0.6\n0.08\n2.0\n0.09\n32.0\n44.0\n0.9949\n3.45\n0.58\n10.5\n5\n\n\n5318\ntinto\n10\n5.9\n0.55\n0.1\n2.2\n0.062\n39.0\n51.0\n0.99512\n3.52\n0.76\n11.2\n6\n\n\n5319\ntinto\n6\n5.9\n0.645\n0.12\n2.0\n0.075\n32.0\n44.0\n0.99547\n3.57\n0.71\n10.2\n5\n\n\n5320\ntinto\n3\n6.0\n0.31\n0.47\n3.6\n0.067\n18.0\n42.0\n0.99549\n3.39\n0.66\n11.0\n6\n\n\n\n\n\n\n\n\n\nMostrar los tipos de cada variable del data frame.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función schema del paquete MLJ.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing MLJ\nschema(df)\n\n\n┌──────────────────────┬────────────┬─────────┐\n│ names                │ scitypes   │ types   │\n├──────────────────────┼────────────┼─────────┤\n│ tipo                 │ Textual    │ String7 │\n│ meses_barrica        │ Count      │ Int64   │\n│ acided_fija          │ Continuous │ Float64 │\n│ acided_volatil       │ Continuous │ Float64 │\n│ acido_citrico        │ Continuous │ Float64 │\n│ azucar_residual      │ Continuous │ Float64 │\n│ cloruro_sodico       │ Continuous │ Float64 │\n│ dioxido_azufre_libre │ Continuous │ Float64 │\n│ dioxido_azufre_total │ Continuous │ Float64 │\n│ densidad             │ Continuous │ Float64 │\n│ ph                   │ Continuous │ Float64 │\n│ sulfatos             │ Continuous │ Float64 │\n│ alcohol              │ Continuous │ Float64 │\n│ calidad              │ Count      │ Int64   │\n└──────────────────────┴────────────┴─────────┘\n\n\n\n\n\n\n\nHacer un análisis de los datos perdidos en el data frame.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ndescribe(df, :nmissing)\n\n14×2 DataFrame\n\n\n\nRow\nvariable\nnmissing\n\n\n\nSymbol\nInt64\n\n\n\n\n1\ntipo\n0\n\n\n2\nmeses_barrica\n0\n\n\n3\nacided_fija\n0\n\n\n4\nacided_volatil\n0\n\n\n5\nacido_citrico\n0\n\n\n6\nazucar_residual\n0\n\n\n7\ncloruro_sodico\n0\n\n\n8\ndioxido_azufre_libre\n0\n\n\n9\ndioxido_azufre_total\n0\n\n\n10\ndensidad\n0\n\n\n11\nph\n0\n\n\n12\nsulfatos\n0\n\n\n13\nalcohol\n0\n\n\n14\ncalidad\n0\n\n\n\n\n\n\n\n\n\nMostrar la distribución de frecuencias de las variables cuantitativas del data frame mediante histogramas.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing GLMakie\nfig = Figure() \nax = [Axis(fig[trunc(Int, i / 3), i % 3], title = names(df)[i+2]) for i in 0:12]\nfor i in 1:13\n    hist!(ax[i], df[!, i+1], strokewidth = 0.5, strokecolor = (:white, 0.5))\nend\nfig\n\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\n\nSe considera que un vino es bueno si tiene una puntuación de calidad mayor que \\(6.5\\). Recodificar la variable calidad en una variable categórica que tome el valor 1 si la calidad es mayor que \\(6.5\\) y 0 en caso contrario.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nusing CategoricalArrays\n# Recodificamos la variable calidad.\ndf.calidad = cut(df.calidad, [0, 6.5, 10], labels = [\" ☹️ \", \" 😊 \"])\n\n5320-element CategoricalArray{String,1,UInt32}:\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" 😊 \"\n \" ☹️ \"\n \" 😊 \"\n \" ☹️ \"\n ⋮\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n\n\n\n\n\nDescomponer el data frame en un data frame con las variables predictivas y un vector con la variable objetivo bueno.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\ny, X = unpack(df, ==(:calidad), rng = 123)\n\n\n(CategoricalValue{String, UInt32}[\" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" 😊 \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \"  …  \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \", \" ☹️ \"], 5320×13 DataFrame\n  Row │ tipo     meses_barrica  acided_fija  acided_volatil  acido_citrico  az ⋯\n      │ String7  Int64          Float64      Float64         Float64        Fl ⋯\n──────┼─────────────────────────────────────────────────────────────────────────\n    1 │ blanco               0          6.7           0.5             0.36     ⋯\n    2 │ blanco               0          6.3           0.2             0.3\n    3 │ blanco               0          6.2           0.35            0.03\n    4 │ tinto                3          8.0           0.39            0.3\n    5 │ blanco               0          7.9           0.255           0.26     ⋯\n    6 │ blanco               0          6.1           0.31            0.37\n    7 │ blanco               0          6.8           0.28            0.36\n    8 │ blanco               0          8.2           0.34            0.49\n    9 │ tinto                0          6.7           0.48            0.02     ⋯\n   10 │ blanco               0          7.4           0.35            0.2\n   11 │ tinto                5          7.5           0.53            0.06\n  ⋮   │    ⋮           ⋮             ⋮             ⋮               ⋮           ⋱\n 5311 │ blanco               0          7.2           0.14            0.35\n 5312 │ tinto                3          7.6           0.41            0.24     ⋯\n 5313 │ tinto                0          7.3           0.4             0.3\n 5314 │ tinto                4          7.1           0.48            0.28\n 5315 │ blanco               0          6.4           0.29            0.2\n 5316 │ blanco               0          9.4           0.24            0.29     ⋯\n 5317 │ blanco               0          6.3           0.25            0.27\n 5318 │ blanco               0          5.5           0.16            0.26\n 5319 │ blanco               0          7.4           0.36            0.32\n 5320 │ blanco               0          7.6           0.51            0.24     ⋯\n                                                 8 columns and 5299 rows omitted)\n\n\n\n\n\n\nPara poder entrenar un modelo de un arbol de decisión, las variables predictivas deben ser cuantitativas. Transmformar las variables categóricas en variables numéricas.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Convertir las variables categóricas a enteros.\ncoerce!(X, :tipo =&gt; OrderedFactor, :meses_barrica =&gt; Continuous)\nschema(X)\n\n\n┌──────────────────────┬──────────────────┬───────────────────────────────────┐\n│ names                │ scitypes         │ types                             │\n├──────────────────────┼──────────────────┼───────────────────────────────────┤\n│ tipo                 │ OrderedFactor{2} │ CategoricalValue{String7, UInt32} │\n│ meses_barrica        │ Continuous       │ Float64                           │\n│ acided_fija          │ Continuous       │ Float64                           │\n│ acided_volatil       │ Continuous       │ Float64                           │\n│ acido_citrico        │ Continuous       │ Float64                           │\n│ azucar_residual      │ Continuous       │ Float64                           │\n│ cloruro_sodico       │ Continuous       │ Float64                           │\n│ dioxido_azufre_libre │ Continuous       │ Float64                           │\n│ dioxido_azufre_total │ Continuous       │ Float64                           │\n│ densidad             │ Continuous       │ Float64                           │\n│ ph                   │ Continuous       │ Float64                           │\n│ sulfatos             │ Continuous       │ Float64                           │\n│ alcohol              │ Continuous       │ Float64                           │\n└──────────────────────┴──────────────────┴───────────────────────────────────┘\n\n\n\n\n\n\n\nDefinir un modelo de árbol de decisión con profundidad máxima 3.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nCargar el modelo DecisionTreeClassifier del paquete DecisionTree con la macros @iload.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Cargamos el tipo de modelo.\nTree = @iload DecisionTreeClassifier pkg = \"DecisionTree\"\n# Instanciamos el modelo con sus parámetros.\narbol = Tree(max_depth =3, rng = 123)\n\nimport MLJDecisionTreeInterface ✔\n\n\n[ Info: For silent loading, specify `verbosity=0`. \n\n\nDecisionTreeClassifier(\n  max_depth = 3, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = 123)\n\n\n\n\n\nEvaluar el modelo tomando un 70% de ejemplos en el conjunto de entrenamiento y un 30% en el conjunto de test. Utilizar como métrica la precisión.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función evaluate del paquete MLJ para evaluar el modelo. Los parámetros más importantes de esta función son:\n\nresampling: Indica el método de muestreo para definir los conjuntos de entrenamiento y test. Los métodos más habituales son:\n\nHoldout(fraction_train = p): Divide el conjunto de datos tomando una proporción de \\(p\\) ejemplos en el conjunto de entrenamiento y \\(1-p\\) en el conjunto de test.\nCV(nfolds = n, shuffle = true|false): Utiliza validación cruzada con n iteraciones. Si se indica shuffle = true, se utiliza validación cruzada aleatoria.\nStratifiedCV(nfolds = n, shuffle = true|false): Utiliza validación cruzada estratificada con n iteraciones. Si se indica shuffle = true, se utiliza validación cruzada estratificada aleatoria.\nInSample(): Utiliza el conjunto de entrenamiento como conjunto de test.\n\nmeasures: Indica las métricas a utilizar para evaluar el modelo. Las métricas más habituales son:\n\ncross_entropy: Pérdida de entropía cruzada.\nconfusion_matrix: Matriz de confusión.\ntrue_positive_rate: Tasa de verdaderos positivos.\ntrue_negative_rate: Tasa de verdaderos negativos.\nppv: Valor predictivo positivo.\nnpv: Valor predictivo negativo.\naccuracy: Precisión.\n\nSe puede indicar más de una en un vector.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nevaluate(arbol, X, y, resampling = Holdout(fraction_train = 0.7, rng = 123), measures = accuracy)\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌────────────┬──────────────┬─────────────┐\n│ measure    │ operation    │ measurement │\n├────────────┼──────────────┼─────────────┤\n│ Accuracy() │ predict_mode │ 0.843       │\n└────────────┴──────────────┴─────────────┘\n\n\n\n\n\n\n\nEvaluar el modelo mediante validación cruzada estratificada usando las métricas de la pérdida de entropía cruzada, la matriz de confusión, la tasa de verdaderos positivos, la tasa de verdaderos negativos, el valor predictivo positivo, el valor predictivo negativo y la precisión. ¿Es un buen modelo?\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\nevaluate(arbol, X, y, resampling = StratifiedCV(rng = 123), measures = [cross_entropy, confusion_matrix, true_positive_rate, true_negative_rate, ppv, npv, accuracy])\n\nEvaluating over 6 folds:  83%[====================&gt;    ]  ETA: 0:00:00Evaluating over 6 folds: 100%[=========================] Time: 0:00:00\n\n\n\nPerformanceEvaluation object with these fields:\n  model, measure, operation,\n  measurement, per_fold, per_observation,\n  fitted_params_per_fold, report_per_fold,\n  train_test_rows, resampling, repeats\nExtract:\n┌───┬──────────────────────────┬──────────────┬─────────────────────────────────\n│   │ measure                  │ operation    │ measurement                    ⋯\n├───┼──────────────────────────┼──────────────┼─────────────────────────────────\n│ A │ LogLoss(                 │ predict      │ 0.375                          ⋯\n│   │   tol = 2.22045e-16)     │              │                                ⋯\n│ B │ ConfusionMatrix(         │ predict_mode │ ConfusionMatrix{2}([3821534 78 ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   perm = nothing,        │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ C │ TruePositiveRate(        │ predict_mode │ 0.128                          ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ D │ TrueNegativeRate(        │ predict_mode │ 1.0                            ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ E │ PositivePredictiveValue( │ predict_mode │ 0.994                          ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│   │   checks = true)         │              │                                ⋯\n│ F │ NegativePredictiveValue( │ predict_mode │ 0.83                           ⋯\n│   │   levels = nothing,      │              │                                ⋯\n│   │   rev = nothing,         │              │                                ⋯\n│ ⋮ │            ⋮             │      ⋮       │                        ⋮       ⋱\n└───┴──────────────────────────┴──────────────┴─────────────────────────────────\n                                                     1 column and 2 rows omitted\n┌───┬───────────────────────────────────────────────────────────────────────────\n│   │ per_fold                                                                 ⋯\n├───┼───────────────────────────────────────────────────────────────────────────\n│ A │ [0.391, 0.394, 0.35, 0.358, 0.365, 0.391]                                ⋯\n│ B │ ConfusionMatrix{2, true, CategoricalValue{String, UInt32}}[ConfusionMatr ⋯\n│ C │ [0.125, 0.167, 0.155, 0.112, 0.113, 0.0952]                              ⋯\n│ D │ [1.0, 0.999, 1.0, 1.0, 1.0, 1.0]                                         ⋯\n│ E │ [1.0, 0.966, 1.0, 1.0, 1.0, 1.0]                                         ⋯\n│ F │ [0.83, 0.837, 0.835, 0.827, 0.828, 0.825]                                ⋯\n│ G │ [0.834, 0.841, 0.84, 0.831, 0.832, 0.828]                                ⋯\n└───┴───────────────────────────────────────────────────────────────────────────\n                                                               2 columns omitted\n\n\n\n\nLa precisión del modelo es de \\(0.834\\) que no está mal, pero si consdieramos la tasa de verdadero positivos, que es \\(0.13\\) y la tasa de verdaderos negativos, que es prácticamente 1, el modelo tiene un buen rendimiento en la clasificación de los vinos malos, pero un mal rendimiento en la clasificación de los vinos buenos. Por lo tanto, no podemos decir que sea un buen modelo.\n\n\n\nConstruir árboles de decisión con profundidades máximas de 2 a 10 y evaluar el modelo con validación cruzada estratificada. ¿Cuál es la profundidad máxima que da mejor resultado?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función TunedModel del paquete MLJ para ajustar los parámetros del modelo.\nLos parámetros más importantes de esta función son:\n\nmodel: Indica el modelo a ajustar.\nresampling: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.\ntuning: Indica el método de ajuste de los parámetros del modelo. Los métodos más habituales son:\n\nGrid(resolution = n): Ajusta los parámetros del modelo utilizando una cuadrícula de búsqueda con n valores.\nRandomSearch(resolution = n): Ajusta los parámetros del modelo utilizando una búsqueda aleatoria con n valores.\n\nrange: Indica el rango de valores a utilizar para ajustar los parámetros del modelo. Se puede indicar un rango de valores o un vector de valores.\nmeasure: Indica la métrica a utilizar para evaluar el modelo.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Instanciamos el modelo de árbol de decisión.\narbol = Tree()\n# Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.\nr = range(arbol, :max_depth, lower=2, upper=10)\n# Ajustamos los parámetros del modelo utilizando una cuadrícula de búsqueda con 9 valores.\narbol_parametrizado = TunedModel(\n    model = arbol,\n    resampling = StratifiedCV(rng = 123),\n    tuning = Grid(resolution = 9),\n    range = r,\n    measure = accuracy)\n# Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol_parametrizado, X, y)\n# Ajustamos los parámetros del modelo.\nMLJ.fit!(mach)\n# Mostramos los parámetros del mejor modelo.\nfitted_params(mach).best_model\n\n[ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).\n[ Info: Attempting to evaluate 9 models.\nEvaluating over 9 metamodels:   0%[&gt;                        ]  ETA: N/AEvaluating over 9 metamodels:  11%[==&gt;                      ]  ETA: 0:00:01Evaluating over 9 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:01Evaluating over 9 metamodels:  33%[========&gt;                ]  ETA: 0:00:01Evaluating over 9 metamodels:  44%[===========&gt;             ]  ETA: 0:00:01Evaluating over 9 metamodels:  56%[=============&gt;           ]  ETA: 0:00:01Evaluating over 9 metamodels:  67%[================&gt;        ]  ETA: 0:00:01Evaluating over 9 metamodels:  78%[===================&gt;     ]  ETA: 0:00:01Evaluating over 9 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 9 metamodels: 100%[=========================] Time: 0:00:02\n\n\nDecisionTreeClassifier(\n  max_depth = 5, \n  min_samples_leaf = 1, \n  min_samples_split = 2, \n  min_purity_increase = 0.0, \n  n_subfeatures = 0, \n  post_prune = false, \n  merge_purity_threshold = 1.0, \n  display_depth = 5, \n  feature_importance = :impurity, \n  rng = TaskLocalRNG())\n\n\n\n\n\nDibujar la curva de aprendizaje del modelo en función de la profundidad del árbol de decisión.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función learning_curve del paquete MLJ para dibujar la curva de aprendizaje. Los parámetros más importantes de esta función son:\n\nmach: Indica la máquina de aprendizaje a utilizar.\nrange: Indica el rango de valores a utilizar para ajustar los parámetros del modelo.\nresampling: Indica el método de muestreo para definir los conjuntos de entrenamiento y test.\nmeasure: Indica la métrica a utilizar para evaluar el modelo.\nrngs: Indica la semilla para la generación de números aleatorios. Se pueden indicar varias semillas en un vector y se genera una curva de aprendizaje para cada semilla.\n\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Instanciamos el modelo de árbol de decisión.\narbol = Tree()\n# Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol, X, y)\n# Definimos el rango de valores a utilizar para ajustar los parámetros del modelo.\nr = range(arbol, :max_depth, lower=2, upper=10)\n# Dibujamos la curva de aprendizaje.\ncurva = learning_curve(mach, range = r, resampling = StratifiedCV(rng = 123), measure = accuracy)\n# Dibujamos la curva de aprendizaje.\nfig = Figure()\nax = Axis(fig[1, 1], title = \"Curva de aprendizaje\", xlabel = \"Profundidad del árbol\", ylabel = \"Precisión\")\nMakie.scatter!(ax, curva.parameter_values, curva.measurements)\nfig\n\n[ Info: Training machine(ProbabilisticTunedModel(model = DecisionTreeClassifier(max_depth = -1, …), …), …).\n[ Info: Attempting to evaluate 9 models.\nEvaluating over 9 metamodels:  22%[=====&gt;                   ]  ETA: 0:00:01Evaluating over 9 metamodels:  33%[========&gt;                ]  ETA: 0:00:01Evaluating over 9 metamodels:  44%[===========&gt;             ]  ETA: 0:00:01Evaluating over 9 metamodels:  56%[=============&gt;           ]  ETA: 0:00:01Evaluating over 9 metamodels:  67%[================&gt;        ]  ETA: 0:00:00Evaluating over 9 metamodels:  78%[===================&gt;     ]  ETA: 0:00:00Evaluating over 9 metamodels:  89%[======================&gt;  ]  ETA: 0:00:00Evaluating over 9 metamodels: 100%[=========================] Time: 0:00:01\n┌ Warning: Found `resolution` in the theme when creating a `Scene`. The `resolution` keyword for `Scene`s and `Figure`s has been deprecated. Use `Figure(; size = ...` or `Scene(; size = ...)` instead, which better reflects that this is a unitless size and not a pixel resolution. The key could also come from `set_theme!` calls or related theming functions.\n└ @ Makie ~/.julia/packages/Makie/ux0Te/src/scenes.jl:238\n\n\n\n\n\n\n\n\n\n\n\n\nConstruir un árbol de decisión con la profundidad máxima que da mejor resultado y visualizarlo.\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Instanciamos el modelo de árbol de decisión.\narbol = Tree(max_depth = 4)\n# Definimos una máquina de aprendizaje con el modelo, las variables predictivas y la variable objetivo.\nmach = machine(arbol, X, y)\n# Ajustamos los parámetros del modelo.\nMLJ.fit!(mach)\n# Visualizamos el árbol de decisión.\nfitted_params(mach).tree\n\n[ Info: Training machine(DecisionTreeClassifier(max_depth = 4, …), …).\n\n\nalcohol &lt; 10.62\n├─ meses_barrica &lt; 8.5\n│  ├─ acided_volatil &lt; 0.3125\n│  │  ├─ acided_volatil &lt; 0.2025\n│  │  │  ├─  ☹️  (408/496)\n│  │  │  └─  ☹️  (1095/1172)\n│  │  └─ meses_barrica &lt; 5.5\n│  │     ├─  ☹️  (1334/1345)\n│  │     └─  ☹️  (51/58)\n│  └─  😊  (25/25)\n└─ meses_barrica &lt; 12.5\n   ├─ cloruro_sodico &lt; 0.0455\n   │  ├─ alcohol &lt; 12.55\n   │  │  ├─  ☹️  (751/1160)\n   │  │  └─  😊  (185/286)\n   │  └─ meses_barrica &lt; 10.5\n   │     ├─  ☹️  (552/629)\n   │     └─  😊  (25/43)\n   └─ alcohol &lt; 14.45\n      ├─  😊  (105/105)\n      └─  ☹️  (1/1)\n\n\n\n\n\n¿Cuál es la importancia de cada variable en el modelo?\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función feature_importances del paquete DecisionTree para calcular la importancia de cada variable.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\n\n# Calculamos la importancia de cada variable.\nfeature_importances(mach)\n\n13-element Vector{Pair{Symbol, Float64}}:\n              :alcohol =&gt; 0.5303315899204789\n        :meses_barrica =&gt; 0.26854115615561525\n       :acided_volatil =&gt; 0.1040970236546446\n       :cloruro_sodico =&gt; 0.09703023026926123\n                 :tipo =&gt; 0.0\n          :acided_fija =&gt; 0.0\n        :acido_citrico =&gt; 0.0\n      :azucar_residual =&gt; 0.0\n :dioxido_azufre_libre =&gt; 0.0\n :dioxido_azufre_total =&gt; 0.0\n             :densidad =&gt; 0.0\n                   :ph =&gt; 0.0\n             :sulfatos =&gt; 0.0\n\n\n\n\n\nPredecir la calidad de los 10 primeros vinos del conjunto de ejemplos.\n\n\n\n\n\n\nAyuda\n\n\n\n\n\nUsar la función predict del paquete DecisionTree para predecir las probabilidades de pertenecer a cada clase un ejemplo o conjunto de ejemplos.\nUsar la función predict_mode del paquete DecisionTree para predecir la clase de un ejemplo o conjunto de ejemplos.\n\n\n\n\n\n\n\n\n\nSolución\n\n\n\n\n\nPrimero calculamos las probabilidades de cada clase.\n\nMLJ.predict(mach, X[1:10, :])\n\n10-element CategoricalDistributions.UnivariateFiniteVector{OrderedFactor{2}, String, UInt32, Float64}:\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.823,  😊 =&gt;0.177)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.647,  😊 =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.647,  😊 =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.647,  😊 =&gt;0.353)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.878,  😊 =&gt;0.122)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n UnivariateFinite{OrderedFactor{2}}( ☹️ =&gt;0.992,  😊 =&gt;0.00818)\n\n\nY ahora predecimos la clase.\n\npredict_mode(mach, X[1:10, :])\n\n10-element CategoricalArray{String,1,UInt32}:\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"\n \" ☹️ \"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Árboles de decisión</span>"
    ]
  }
]